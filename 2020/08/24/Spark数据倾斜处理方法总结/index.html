<!DOCTYPE html>


<html lang="zh-CN">


<head>
  <meta charset="utf-8" />
   
  <meta name="keywords" content="生活,代码" />
   
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Spark数据倾斜处理方法总结 |  你当像鸟飞往你的山
  </title>
  <meta name="generator" content="hexo-theme-ayer">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/dist/main.css">

  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
  
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

  
  

  

<link rel="alternate" href="/atom.xml" title="你当像鸟飞往你的山" type="application/atom+xml">
</head>

</html>

<body>
  <div id="app">
    
      <canvas class="fireworks"></canvas>
      <style>
        .fireworks {
          position: fixed;
          left: 0;
          top: 0;
          z-index: 99999;
          pointer-events: none;
        }
      </style>
      
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-Spark数据倾斜处理方法总结"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Spark数据倾斜处理方法总结
</h1>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" class="article-date">
  <time datetime="2020-08-24T07:53:57.000Z" itemprop="datePublished">2020-08-24</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">7.3k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">29 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="Spark性能优化之道——解决Spark数据倾斜（Data-Skew）的N种姿势"><a href="#Spark性能优化之道——解决Spark数据倾斜（Data-Skew）的N种姿势" class="headerlink" title="Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势"></a>Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势</h2><p>本文结合实例详细阐明了Spark数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义Partitioner，使用Map侧Join代替Reduce侧Join，给倾斜Key加上随机前缀等。</p>
<blockquote>
<p>  本文转发自技术世界，原文链接<a href="http://www.jasongj.com/spark/skew/" target="_blank" rel="noopener">http://www.jasongj.com/spark/skew/</a></p>
</blockquote>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文结合实例详细阐明了Spark数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义Partitioner，使用Map侧Join代替Reduce侧Join，给倾斜Key加上随机前缀等。</p>
<h1 id="为何要处理数据倾斜（Data-Skew）"><a href="#为何要处理数据倾斜（Data-Skew）" class="headerlink" title="为何要处理数据倾斜（Data Skew）"></a>为何要处理数据倾斜（Data Skew）</h1><h2 id="什么是数据倾斜"><a href="#什么是数据倾斜" class="headerlink" title="什么是数据倾斜"></a>什么是数据倾斜</h2><p>对Spark/Hadoop这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。</p>
<p>何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p>
<p>对于分布式系统而言，理想情况下，随着系统规模（节点数量）的增加，应用整体耗时线性下降。如果一台机器处理一批大量数据需要120分钟，当机器数量增加到三时，理想的耗时为120 / 3 = 40分钟，如下图所示</p>
<p>但是，上述情况只是理想情况，实际上将单机任务转换成分布式任务后，会有overhead，使得总的任务量较之单机时有所增加，所以每台机器的执行时间加起来比单台机器时更大。这里暂不考虑这些overhead，假设单机任务转换成分布式任务后，总任务量不变。<br>　　<br>但即使如此，想做到分布式情况下每台机器执行时间是单机时的，就必须保证每台机器的任务量相等。不幸的是，很多时候，任务的分配是不均匀的，甚至不均匀到大部分任务被分配到个别机器上，其它大部分机器所分配的任务量只占总得的小部分。比如一台机器负责处理80%的任务，另外两台机器各处理10%的任务，如下图所示<br><a href="http://www.jasongj.com/img/spark/spark1_skew/skew_time.png" target="_blank" rel="noopener"><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/skew_time.png" alt="unideal scale out"></a></p>
<a id="more"></a>　　
<p>在上图中，机器数据增加为三倍，但执行时间只降为原来的80%，远低于理想值。 　　</p>
<h2 id="数据倾斜的危害"><a href="#数据倾斜的危害" class="headerlink" title="数据倾斜的危害"></a>数据倾斜的危害</h2><p>从上图可见，当出现数据倾斜时，小量任务耗时远高于其它任务，从而使得整体耗时过大，未能充分发挥分布式系统的并行计算优势。<br>　　<br>另外，当发生数据倾斜时，部分任务处理的数据量过大，可能造成内存不足使得任务失败，并进而引进整个应用失败。 　　</p>
<h2 id="数据倾斜是如何造成的"><a href="#数据倾斜是如何造成的" class="headerlink" title="数据倾斜是如何造成的"></a>数据倾斜是如何造成的</h2><p>在Spark中，同一个Stage的不同Partition可以并行处理，而具有依赖关系的不同Stage之间是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。</p>
<p>由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。</p>
<p>Stage的数据来源主要分为如下两类</p>
<ul>
<li>从数据源直接读取。如读取HDFS，Kafka</li>
<li>读取上一个Stage的Shuffle数据</li>
</ul>
<h1 id="如何缓解-消除数据倾斜"><a href="#如何缓解-消除数据倾斜" class="headerlink" title="如何缓解/消除数据倾斜"></a>如何缓解/消除数据倾斜</h1><h2 id="避免数据源的数据倾斜-—读Kafka"><a href="#避免数据源的数据倾斜-—读Kafka" class="headerlink" title="避免数据源的数据倾斜 —读Kafka"></a>避免数据源的数据倾斜 —读Kafka</h2><p>以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。</p>
<p>如《<a href="http://www.jasongj.com/2015/03/10/KafkaColumn1/#Producer消息路由" target="_blank" rel="noopener">Kafka设计解析（一）- Kafka背景及架构介绍</a>》一文所述，Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。</p>
<p>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。</p>
<h2 id="避免数据源的数据倾斜-—读文件"><a href="#避免数据源的数据倾斜-—读文件" class="headerlink" title="避免数据源的数据倾斜 —读文件"></a>避免数据源的数据倾斜 —读文件</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>Spark以通过<code>textFile(path, minPartitions)</code>方法读取文件时，使用TextFileFormat。</p>
<p>对于不可切分的文件，每个文件对应一个Split从而对应一个Partition。此时各文件大小是否一致，很大程度上决定了是否存在数据源侧的数据倾斜。另外，对于不可切分的压缩文件，即使压缩后的文件大小一致，它所包含的实际数据量也可能差别很多，因为源文件数据重复度越高，压缩比越高。反过来，即使压缩文件大小接近，但由于压缩比可能差距很大，所需处理的数据量差距也可能很大。</p>
<p>此时可通过在数据生成端将不可切分文件存储为可切分文件，或者保证各文件包含数据量相同的方式避免数据倾斜。</p>
<p>对于可切分的文件，每个Split大小由如下算法决定。其中goalSize等于所有文件总大小除以minPartitions。而blockSize，如果是HDFS文件，由文件本身的block大小决定；如果是Linux本地文件，且使用本地模式，由<code>fs.local.block.size</code>决定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>默认情况下各Split的大小不会太大，一般相当于一个Block大小（在Hadoop 2中，默认值为128MB），所以数据倾斜问题不明显。如果出现了严重的数据倾斜，可通过上述参数调整。</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>现通过脚本生成一些文本文件，并通过如下代码进行简单的单词计数。为避免Shuffle，只计单词总个数，不须对单词进行分组计数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SparkConf sparkConf &#x3D; new SparkConf()</span><br><span class="line">    .setAppName(&quot;ReadFileSkewDemo&quot;);</span><br><span class="line">JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line">long count &#x3D; javaSparkContext.textFile(inputFile, minPartitions)</span><br><span class="line">    .flatMap((String line) -&gt; Arrays.asList(line.split(&quot; &quot;)).iterator()).count();</span><br><span class="line">System.out.printf(&quot;total words : %s&quot;, count);</span><br><span class="line">javaSparkContext.stop();</span><br></pre></td></tr></table></figure>



<p>总共生成如下11个csv文件，其中10个大小均为271.9MB，另外一个大小为8.5GB。<br><a href="http://www.jasongj.com/img/spark/spark1_skew/uncompressedfiles.png" target="_blank" rel="noopener"><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/uncompressedfiles.png" alt="uncompressed files"></a></p>
<p>之后将8.5GB大小的文件使用gzip压缩，压缩后大小仅为25.3MB。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/compressedfiles.png" alt="compressed files">]</p>
<p>使用如上代码对未压缩文件夹进行单词计数操作。Split大小为 max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9 <em>10+8.5</em> 1024) / 1 MB, 128 MB) = 128MB。无明显数据倾斜。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/splitable_unskewed.png" alt="splitable_unskewed">]</p>
<p>使用同样代码对包含压缩文件的文件夹进行同样的单词计数操作。未压缩文件的Split大小仍然为128MB，而压缩文件（gzip压缩）由于不可切分，且大小仅为25.3MB，因此该文件作为一个单独的Split/Partition。虽然该文件相对较小，但是它由8.5GB文件压缩而来，包含数据量是其它未压缩文件的32倍，因此处理该Split/Partition/文件的Task耗时为4.4分钟，远高于其它Task的10秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/compressedfileskew.png" alt="compressed file skew">]</p>
<p>由于上述gzip压缩文件大小为25.3MB，小于128MB的Split大小，不能证明gzip压缩文件不可切分。现将minPartitions从默认的1设置为229，从而目标Split大小为max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9 * 10+25.3) / 229 MB, 128 MB) = 12 MB。如果gzip压缩文件可切分，则所有Split/Partition大小都不会远大于12。反之，如果仍然存在25.3MB的Partition，则说明gzip压缩文件确实不可切分，在生成不可切分文件时需要如上文所述保证各文件数量大大致相同。</p>
<p>如下图所示，gzip压缩文件对应的Split/Partition大小为25.3MB，其它Split大小均为12MB左右。而该Task耗时4.7分钟，远大于其它Task的4秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/unsplitable_skew.png" alt="compressed unsplitable file skew">]</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>数据源侧存在不可切分文件，且文件内包含的数据量相差较大。</p>
<p><strong><em>解决方案\</em></strong><br>尽量使用可切分的格式代替不可切分的格式，或者保证各文件实际包含数据量大致相同。</p>
<p><strong><em>优势\</em></strong><br>可撤底消除数据源侧数据倾斜，效果显著。</p>
<p><strong><em>劣势\</em></strong><br>数据源一般来源于外部系统，需要外部系统的支持。</p>
<h2 id="调整并行度分散同一个Task的不同Key"><a href="#调整并行度分散同一个Task的不同Key" class="headerlink" title="调整并行度分散同一个Task的不同Key"></a>调整并行度分散同一个Task的不同Key</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。</p>
<p>如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/changeparallelism.png" alt="spark change parallelism">]</p>
<h3 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h3><p>现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5亿条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CASE WHEN id &lt; 940000000 THEN (9500000  + (CAST (RAND() * 8 AS INTEGER)) * 12 )</span><br><span class="line">       ELSE CAST(id&#x2F;100 AS INTEGER)</span><br><span class="line">       END,</span><br><span class="line">       name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br></pre></td></tr></table></figure>

<p>通过上述处理，一份可能造成后续数据倾斜的测试数据即以准备好。接下来，使用Spark读取该测试数据，并通过<code>groupByKey(12)</code>对id分组处理，且Shuffle并行度为12。代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkSession sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">      .appName(&quot;SparkDataSkewTunning&quot;)</span><br><span class="line">      .config(&quot;hive.metastore.uris&quot;, &quot;thrift:&#x2F;&#x2F;hadoop1:9083&quot;)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate();</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; dataframe &#x3D; sparkSession.sql( &quot;select * from test&quot;);</span><br><span class="line">    dataframe.toJavaRDD()</span><br><span class="line">      .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1)))</span><br><span class="line">      .groupByKey(12)</span><br><span class="line">      .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123;</span><br><span class="line">        int id &#x3D; tuple._1();</span><br><span class="line">        AtomicInteger atomicInteger &#x3D; new AtomicInteger(0);</span><br><span class="line">        tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get());</span><br><span class="line">      &#125;).count();</span><br><span class="line"></span><br><span class="line">      sparkSession.stop();</span><br><span class="line">      sparkSession.close();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本次实验所使用集群节点数为4，每个节点可被Yarn使用的CPU核数为16，内存为16GB。使用如下方式提交上述应用，将启动4个Executor，每个Executor可使用核数为12（该配置并非生产环境下的最优配置，仅用于本文实验），可用内存为12GB。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar</span><br></pre></td></tr></table></figure>



<p>GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为4500万，远大于（9倍于）其它11个Task处理的500万记录。而Task 8所耗费的时间为38秒，远高于其它11个Task的平均时间（16秒）。整个Stage的时间也为38秒，该时间主要由最慢的Task 8决定。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew12.png" alt="data skew">]</p>
<p>在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。</p>
<p>通过<code>groupByKey(48)</code>将Shuffle并行度调整为48，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew48.png" alt="add parallelism">]</p>
<p>从上图可知，记录数最多的Task 20处理的记录数约为1125万，相比于并行度为12时Task 8的4500万，降低了75%左右，而其耗时从原来Task 8的38秒降到了24秒。</p>
<p>在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过<code>groupByKey(11)</code>将Shuffle并行度调整为11，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew11.png" alt="reduce parallelism">]</p>
<p>从上图可见，处理记录数最多的Task 6所处理的记录数约为1045万，耗时为23秒。处理记录数最少的Task 1处理的记录数约为545万，耗时12秒。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong><em>解决方案\</em></strong><br>调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。</p>
<p><strong><em>优势\</em></strong><br>实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用<code>spark.default.parallelism</code>设置。如果是Spark SQL，还可通过<code>SET spark.sql.shuffle.partitions=[num_tasks]</code>设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p>
<p><strong><em>劣势\</em></strong><br>适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p>
<h2 id="自定义Partitioner"><a href="#自定义Partitioner" class="headerlink" title="自定义Partitioner"></a>自定义Partitioner</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。</p>
<h3 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h3><p>以上述数据集为例，继续将并发度设置为12，但是在<code>groupByKey</code>算子上，使用自定义的<code>Partitioner</code>（实现如下）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">.groupByKey(new Partitioner() &#123;</span><br><span class="line">  @Override</span><br><span class="line">  public int numPartitions() &#123;</span><br><span class="line">    return 12;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  public int getPartition(Object key) &#123;</span><br><span class="line">    int id &#x3D; Integer.parseInt(key.toString());</span><br><span class="line">    if(id &gt;&#x3D; 9500000 &amp;&amp; id &lt;&#x3D; 9500084 &amp;&amp; ((id - 9500000) % 12) &#x3D;&#x3D; 0) &#123;</span><br><span class="line">      return (id - 9500000) &#x2F; 12;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return id % 12;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>



<p>由下图可见，使用自定义Partition后，耗时最长的Task 6处理约1000万条数据，用时15秒。并且各Task所处理的数据集大小相当。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/customizedpartition.png" alt="customizec partitioner">]</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong><em>解决方案\</em></strong><br>使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p>
<p><strong><em>优势\</em></strong><br>不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p>
<p><strong><em>劣势\</em></strong><br>适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p>
<h2 id="将Reduce-side-Join转变为Map-side-Join"><a href="#将Reduce-side-Join转变为Map-side-Join" class="headerlink" title="将Reduce side Join转变为Map side Join"></a>将Reduce side Join转变为Map side Join</h2><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoin.png" alt="spark map join">]</p>
<h3 id="案例-3"><a href="#案例-3" class="headerlink" title="案例"></a>案例</h3><p>通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000  + (CAST (RAND() * 4 AS INT) + 1) * 48 )</span><br><span class="line">       ELSE CAST(id&#x2F;10 AS INT) END AS STRING),</span><br><span class="line">       name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br></pre></td></tr></table></figure>



<p>使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test_new</span><br><span class="line">SELECT CAST(CAST(id&#x2F;10 AS INT) AS STRING),</span><br><span class="line">       name</span><br><span class="line">FROM student_delta_external</span><br><span class="line">WHERE id BETWEEN 950000000 AND 950500000;</span><br></pre></td></tr></table></figure>



<p>直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>该SQL对应的DAG如下图所示。从该图可见，该执行过程总共分为三个Stage，前两个用于从Hive中读取数据，同时二者进行Shuffle，通过最后一个Stage进行Join并将结果写入表test_join中。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoindag.png" alt="reduce join DAG">]</p>
<p>从下图可见，Join Stage各Task处理的数据倾斜严重，处理数据量最大的Task耗时7.1分钟，远高于其它无数据倾斜的Task约2秒的耗时。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoinlaststage.png" alt="reduce join DAG">]</p>
<p>接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过<code>CACHE TABLE test_new</code>将小表test_new进行cache。现通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CACHE TABLE test_new;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>通过如下DAG图可见，该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoincachedag.png" alt="reduce join DAG">]</p>
<p>并且数据倾斜仍然存在。如下图所示，最慢的Task耗时为7.1分钟，远高于其它Task的约2秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoincachelaststage.png" alt="reduce join DAG">]</p>
<p>正确的使用Broadcast实现Map侧Join的方式是，通过<code>SET spark.sql.autoBroadcastJoinThreshold=104857600;</code>将Broadcast的阈值设置得足够大。</p>
<p>再次通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SET spark.sql.autoBroadcastJoinThreshold&#x3D;104857600;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>通过如下DAG图可见，该方案只包含一个Stage。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoindag.png" alt="reduce join DAG">]</p>
<p>并且从下图可见，各Task耗时相当，无明显数据倾斜现象。并且总耗时为1.5分钟，远低于Reduce侧Join的7.3分钟。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoinlaststage.png" alt="reduce join DAG">]</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。</p>
<p><strong><em>解决方案\</em></strong><br>在Java/Scala代码中将小数据集数据拉取到Driver，然后通过Broadcast方案将小数据集的数据广播到各Executor。或者在使用SQL前，将Broadcast的阈值调整得足够大，从而使用Broadcast生效。进而将Reduce侧Join替换为Map侧Join。</p>
<p><strong><em>优势\</em></strong><br>避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p>
<p><strong><em>劣势\</em></strong><br>要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。</p>
<h2 id="为skew的key增加随机前-后缀"><a href="#为skew的key增加随机前-后缀" class="headerlink" title="为skew的key增加随机前/后缀"></a>为skew的key增加随机前/后缀</h2><h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/randomprefix.png" alt="spark random prefix">]</p>
<h3 id="案例-4"><a href="#案例-4" class="headerlink" title="案例"></a>案例</h3><p>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。</p>
<p>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000  + (CAST (RAND() * 2 AS INT) + 1) * 48 )</span><br><span class="line">  ELSE CAST(id&#x2F;100 AS INT) END AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE TABLE test_new</span><br><span class="line">SELECT CAST(CAST(id&#x2F;100 AS INT) AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_delta_external</span><br><span class="line">WHERE id BETWEEN 950000000 AND 950500000;</span><br></pre></td></tr></table></figure>

<p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">    sparkConf.setAppName(&quot;DemoSparkDataFrameWithSkewedBigTableDirect&quot;);</span><br><span class="line">    sparkConf.set(&quot;spark.default.parallelism&quot;, String.valueOf(parallelism));</span><br><span class="line">    JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">          return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    leftRDD.join(rightRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()))</span><br><span class="line">      .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">        AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinallstage.png" alt="few skewed key join">]</p>
<p>通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinlaststage.png" alt="few skewed key join">]</p>
<p>现通过如下操作，实现倾斜Key的分散处理</p>
<ul>
<li>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD</li>
<li>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD</li>
<li>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD</li>
<li>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD</li>
<li>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD</li>
<li>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集</li>
</ul>
<p>具体实现代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">      int parallelism &#x3D; 48;</span><br><span class="line">      SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">      sparkConf.setAppName(&quot;SolveDataSkewWithRandomPrefix&quot;);</span><br><span class="line">      sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);</span><br><span class="line">      JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">      JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">        .mapToPair((String row) -&gt; &#123;</span><br><span class="line">          String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">            return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">          .mapToPair((String row) -&gt; &#123;</span><br><span class="line">            String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">              return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">          &#125;);</span><br><span class="line"></span><br><span class="line">        String[] skewedKeyArray &#x3D; new String[]&#123;&quot;9500048&quot;, &quot;9500096&quot;&#125;;</span><br><span class="line">        Set&lt;String&gt; skewedKeySet &#x3D; new HashSet&lt;String&gt;();</span><br><span class="line">        List&lt;String&gt; addList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">        for(int i &#x3D; 1; i &lt;&#x3D;24; i++) &#123;</span><br><span class="line">            addList.add(i + &quot;&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        for(String key : skewedKeyArray) &#123;</span><br><span class="line">            skewedKeySet.add(key);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys &#x3D; javaSparkContext.broadcast(skewedKeySet);</span><br><span class="line">        Broadcast&lt;List&lt;String&gt;&gt; addListKeys &#x3D; javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftSkewRDD &#x3D; leftRDD</span><br><span class="line">          .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + &quot;,&quot; + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightSkewRDD &#x3D; rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">          .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))</span><br><span class="line">          .collect(Collectors.toList())</span><br><span class="line">          .iterator()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; skewedJoinRDD &#x3D; leftSkewRDD</span><br><span class="line">          .join(rightSkewRDD, parallelism)</span><br><span class="line">          .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftUnSkewRDD &#x3D; leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));</span><br><span class="line">        JavaPairRDD&lt;String, String&gt; unskewedJoinRDD &#x3D; leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">          AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.stop();</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyrandomjoinallstage.png" alt="few skewed key join">]</p>
<p>通过分析Join Stage的所有Task可知</p>
<ul>
<li>由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96</li>
<li>由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始）</li>
<li>由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况</li>
</ul>
<p>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinrandomlaststage.png" alt="few skewed key join">]</p>
<p>实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。</p>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p>
<p><strong><em>解决方案\</em></strong><br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p>
<p><strong><em>优势\</em></strong><br>相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p>
<p><strong><em>劣势\</em></strong><br>如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p>
<h2 id="大表随机添加N种随机前缀，小表扩大N倍"><a href="#大表随机添加N种随机前缀，小表扩大N倍" class="headerlink" title="大表随机添加N种随机前缀，小表扩大N倍"></a>大表随机添加N种随机前缀，小表扩大N倍</h2><h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p>如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。<br><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/randomprefixandenlargesmalltable.png" alt="spark random prefix"></p>
<h3 id="案例-5"><a href="#案例-5" class="headerlink" title="案例"></a>案例</h3><p>这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">    sparkConf.setAppName(&quot;ResolveDataSkewWithNAndRandom&quot;);</span><br><span class="line">    sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);</span><br><span class="line">    JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; addList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">    for(int i &#x3D; 1; i &lt;&#x3D;48; i++) &#123;</span><br><span class="line">      addList.add(i + &quot;&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Broadcast&lt;List&lt;String&gt;&gt; addListKeys &#x3D; javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRandomRDD &#x3D; leftRDD.mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(new Random().nextInt(48) + &quot;,&quot; + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightNewRDD &#x3D; rightRDD</span><br><span class="line">      .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">      .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))</span><br><span class="line">      .collect(Collectors.toList())</span><br><span class="line">      .iterator()</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; joinRDD &#x3D; leftRandomRDD</span><br><span class="line">      .join(rightNewRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">    joinRDD.foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">      AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">      iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。</p>
<p><strong><em>优势\</em></strong><br>对大部分场景都适用，效果不错。</p>
<p><strong><em>劣势\</em></strong><br>需要将一个数据集整体扩大N倍，会增加资源消耗。</p>
<h1 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h1><p>对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。</p>
 
      <!-- reward -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" rel="tag">数据倾斜</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2020/09/01/Hive-SQl/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Hive SQl
          
        </div>
      </a>
    
    
      <a href="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Docker介绍</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "Dfy9sbO4s9px2dbCyrGKbdLm-gzGzoHsz",
    app_key: "UL9nHVEnBrVAIePJrdmz9qYr",
    path: window.location.pathname,
    avatar: "monsterid",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2020
        <i class="ri-heart-fill heart_icon"></i> Xuefeng Tao
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="你当像鸟飞往你的山"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="https://xuefeng-vip.lofter.com/" target="_blank" rel="noopener">相册</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->


<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: 'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto'
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script>

<script src="/js/clickBoom1.js"></script>


<!-- ClickBoom2 -->

<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="86"
        src="//music.163.com/outchain/player?type=2&id=1454478388&auto=0&height=66"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"live2d-widget-model-wanko"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7},"log":false});</script></body>

</html>