<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hive2的启动</title>
    <url>/2020/06/26/hive2%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<ol>
<li>启动hive的服务<br>hive ‐‐service hiveserver2 &amp;</li>
<li>使用beeline，去连接thrift的服务<br>beeline -u jdbc:hive2://CentOS-01:10000</li>
</ol>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>服务</tag>
        <tag>beeline</tag>
      </tags>
  </entry>
  <entry>
    <title>数据质量检查</title>
    <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E6%A3%80%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-数据质量检查"><a href="#1-数据质量检查" class="headerlink" title="1.数据质量检查"></a>1.数据质量检查</h1><p>是在完成宽表数据开发后进行的，主要包括四个方面：</p>
<ol>
<li><p>重复值检查</p>
</li>
<li><p>缺失值检查</p>
</li>
<li><p>数据倾斜问题</p>
</li>
<li><p>异常值检查</p>
<a id="more"></a>

</li>
</ol>
<h2 id="1-重复值检查"><a href="#1-重复值检查" class="headerlink" title="1. 重复值检查"></a>1. 重复值检查</h2><h3 id="1-1-什么是重复值"><a href="#1-1-什么是重复值" class="headerlink" title="1.1 什么是重复值"></a>1.1 什么是重复值</h3><p>重复值的检查首先要明确一点，即重复值的定义。对于一份二维表形式的数据集来说，什么是重复值？主要有两个层次：<br>① 关键字段出现重复记录，比如主索引字段出现重复；<br>② 所有字段出现重复记录。<br>第一个层次是否是重复，必须从这份数据的业务含义进行确定。比如一张表，从业务上讲，一个用户应该只会有一条记录，那么如果某个用户出现了超过一条的记录，那么这就是重复值。第二个层次，就一定是重复值了。</p>
<h3 id="1-2-重复值产生的原因"><a href="#1-2-重复值产生的原因" class="headerlink" title="1.2 重复值产生的原因"></a>1.2 重复值产生的原因</h3><p>重复值的产生主要有两个原因，一是上游源数据造成的，二是数据准备脚本中的数据关联造成的。从数据准备角度来看，首先检查数据准备的脚本，判断使用的源表是否有重复记录，同时检查关联语句的正确性和严谨性，比如关联条件是否合理、是否有限定数据周期等等。<br>比如：检查源表数据是否重复的SQL：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> MON_ID,<span class="keyword">COUNT</span>(*),<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> USER_ID)</span><br><span class="line"><span class="keyword">FROM</span> TABLE_NAME</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> MON_ID;</span><br></pre></td></tr></table></figure>

<p>如果是上游源数据出现重复，那么应该及时反映给上游进行修正；如果是脚本关联造成的，修改脚本，重新生成数据即可。<br>还有一份情况，这份数据集是一份单独的数据集，并不是在数据仓库中开发得到的数据，既没有上游源数据，也不存在生成数据的脚本，比如公开数据集，那么如何处理其中的重复值？一般的处理方式就是直接删除重复值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">dataset = pd.read_excel(<span class="string">"/labcenter/python/dataset.xlsx"</span>)</span><br><span class="line"><span class="comment">#判断重复数据</span></span><br><span class="line">dataset.duplicated()    <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.duplicated([<span class="string">'col2'</span>])    <span class="comment">#col2字段重复是重复数据</span></span><br><span class="line"><span class="comment">#删除重复数据</span></span><br><span class="line">dataset.drop_duplicates()     <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.drop_duplicates([<span class="string">'col2'</span>])   <span class="comment">#col2字段重复是重复数据</span></span><br></pre></td></tr></table></figure>

<h2 id="2-缺失值检查"><a href="#2-缺失值检查" class="headerlink" title="2. 缺失值检查"></a>2. 缺失值检查</h2><p>缺失值主要是指数据集中部分记录存在部分字段的信息缺失。</p>
<h3 id="2-1-缺失值出现的原因"><a href="#2-1-缺失值出现的原因" class="headerlink" title="2.1 缺失值出现的原因"></a>2.1 缺失值出现的原因</h3><p>出现趋势值主要有三种原因：<br>① 上游源系统因为技术或者成本原因无法完全获取到这一信息，比如对用户手机APP上网记录的解析；<br>② 从业务上讲，这一信息本来就不存在，比如一个学生的收入，一个未婚者的配偶姓名；<br>③ 数据准备脚本开发中的错误造成的。<br>第一种原因，短期内无法解决；第二种原因，数据的缺失并不是错误，无法避免；第三种原因，则只需通过查证修改脚本即可。<br>缺失值的存在既代表了某一部分信息的丢失，也影响了挖掘分析结论的可靠性与稳定性，因此，必须对缺失值进行处理。<br>如果缺失值记录数超过了全部记录数的50%，则应该从数据集中直接剔除掉该字段，尝试从业务上寻找替代字段；<br>如果缺失值记录数没有超过50%，则应该首先看这个字段在业务上是否有替代字段，如果有，则直接剔除掉该字段，如果没有，则必须对其进行处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看哪些字段有缺失值   </span></span><br><span class="line">dataset.isnull().any()    <span class="comment">#获取含有NaN的字段</span></span><br><span class="line"><span class="comment">#统计各字段的缺失值个数</span></span><br><span class="line">dataset.isnull().apply(pd.value_counts)</span><br><span class="line"><span class="comment">#删除含有缺失值的字段</span></span><br><span class="line">nan_col = dataset.isnull().any()</span><br><span class="line">dataset.drop(nan_col[nan_col].index,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-缺失值的处理"><a href="#2-2-缺失值的处理" class="headerlink" title="2.2 缺失值的处理"></a>2.2 缺失值的处理</h3><p>缺失值的处理主要有两种方式：过滤和填充。</p>
<h4 id="（1）缺失值的过滤"><a href="#（1）缺失值的过滤" class="headerlink" title="（1）缺失值的过滤"></a>（1）缺失值的过滤</h4><p>直接删除含有缺失值的记录，总体上会影响样本个数，如果删除样本过多或者数据集本来就是小数据集时，这种方式并不建议采用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除含有缺失值的记录</span></span><br><span class="line">dataset.dropna()</span><br></pre></td></tr></table></figure>

<h4 id="2）缺失值的填充"><a href="#2）缺失值的填充" class="headerlink" title="2）缺失值的填充"></a>2）缺失值的填充</h4><p>缺失值的填充主要三种方法：<br>① 方法一：使用特定值填充<br>使用缺失值字段的平均值、中位数、众数等统计量填充。<br>优点：简单、快速<br>缺点：容易产生数据倾斜<br>② 方法二：使用算法预测填充<br>将缺失值字段作为因变量，将没有缺失值字段作为自变量，使用决策树、随机森林、KNN、回归等预测算法进行缺失值的预测，用预测结果进行填充。<br>优点：相对精确<br>缺点：效率低，如果缺失值字段与其他字段相关性不大，预测效果差<br>③ 方法三：将缺失值单独作为一个分组，指定值进行填充<br>从业务上选择一个单独的值进行填充，使缺失值区别于其他值而作为一个分组，从而不影响算法计算。<br>优点：简单，实用<br>缺点：效率低</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用Pandas进行特定值填充</span></span><br><span class="line">dataset.fillna(<span class="number">0</span>)   <span class="comment">#不同字段的缺失值都用0填充</span></span><br><span class="line">dataset.fillna(&#123;<span class="string">'col2'</span>:<span class="number">20</span>,<span class="string">'col5'</span>:<span class="number">0</span>&#125;)    <span class="comment">#不同字段使用不同的填充值</span></span><br><span class="line">dataset.fillna(dataset.mean())   <span class="comment">#分别使用各字段的平均值填充</span></span><br><span class="line">dataset.fillna(dataset.median())     <span class="comment">#分别使用个字段的中位数填充</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用sklearn中的预处理方法进行缺失值填充(只适用于连续型字段)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">dataset2 = dataset.drop([<span class="string">'col4'</span>],axis=<span class="number">1</span>)</span><br><span class="line">colsets = dataset2.columns</span><br><span class="line">nan_rule1 = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)    <span class="comment">#创建填充规则(平均值填充)</span></span><br><span class="line">pd.DataFrame(nan_rule1.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule2 = Imputer(missing_values=<span class="string">'median'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>) <span class="comment">#创建填充规则(中位数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule2.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule3 = Imputer(missing_values=<span class="string">'most_frequent'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)  <span class="comment">#创建填充规则(众数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule3.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br></pre></td></tr></table></figure>

<h2 id="3-数据倾斜问题"><a href="#3-数据倾斜问题" class="headerlink" title="3. 数据倾斜问题"></a>3. 数据倾斜问题</h2><p>数据倾斜是指字段的取值分布主要集中在某个特定类别或者特定区间。</p>
<h3 id="3-1-数据倾斜问题的原因"><a href="#3-1-数据倾斜问题的原因" class="headerlink" title="3.1 数据倾斜问题的原因"></a>3.1 数据倾斜问题的原因</h3><p>出现这一问题的原因主要有三种：<br>① 上游源数据存在问题；<br>② 数据准备脚本的问题；<br>③ 数据本身的分布就是如此。<br>如果某个字段出现数据倾斜问题，必须首先排查上述第一、二种原因，如果都没有问题或者无法检查（如：单独的数据集），那么就要考虑这个字段对后续的分析建模是否有价值。一般来说，有严重的数据倾斜的字段对目标变量的区分能力很弱，对分析建模的价值不大，应该直接剔除掉。</p>
<h3 id="3-2-如何衡量数据的倾斜程度"><a href="#3-2-如何衡量数据的倾斜程度" class="headerlink" title="3.2 如何衡量数据的倾斜程度"></a>3.2 如何衡量数据的倾斜程度</h3><p>衡量数据的倾斜程度，主要采用频数分析方法，但因数据类别的不同而有所差异：<br>① 针对连续型字段，需要首先采用等宽分箱方式进行离散化，然后计算各分箱的记录数分布；<br>② 针对离散型字段，直接计算各类别的记录数分布。<br>一般来说，如果某个字段90%以上的记录数，主要集中在某个特定类别或者特定区间，那么这个字段就存在严重的数据倾斜问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对于连续型变量进行等宽分箱</span></span><br><span class="line">pd.value_counts(pd.cut(dataset[<span class="string">'col3'</span>],<span class="number">5</span>))  <span class="comment">#分成5箱</span></span><br><span class="line"><span class="comment">#对于离散型变量进行频数统计</span></span><br><span class="line">pd.value_counts(dataset[<span class="string">'col4'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="4-异常值检查"><a href="#4-异常值检查" class="headerlink" title="4. 异常值检查"></a>4. 异常值检查</h2><p>异常值是指数据中出现了处于特定分布、范围或者趋势之外的数据，这些数据一般会被成为异常值、离群点、噪音等。</p>
<h3 id="4-1-异常值产生的原因"><a href="#4-1-异常值产生的原因" class="headerlink" title="4.1 异常值产生的原因"></a>4.1 异常值产生的原因</h3><p>异常值的产生主要有两类原因：<br>① 数据采集、生成或者传递过程中发生的错误；<br>② 业务运营过程出现的一些特殊情况。<br>将第一种原因产生的异常值称为统计上的异常，这是错误带来的数据问题，需要解决；将第二种原因产生的异常值称为业务上的异常，反映了业务运营过程的某种特殊结果，它不是错误，但需要深究，在数据挖掘中的一种典型应用就是异常检测模型，比如信用卡欺诈，网络入侵检测、客户异动行为识别等等。</p>
<h3 id="4-2-异常值的识别方法"><a href="#4-2-异常值的识别方法" class="headerlink" title="4.2 异常值的识别方法"></a>4.2 异常值的识别方法</h3><p>异常值的识别方法主要有以下几种：</p>
<h4 id="（1）极值检查"><a href="#（1）极值检查" class="headerlink" title="（1）极值检查"></a>（1）极值检查</h4><p>主要检查字段的取值是否超出了合理的值域范围。<br>① 方法一：最大值最小值<br>使用最大值、最小值进行判断。比如客户年龄的最大值为199岁，客户账单的最小费用为-20，这些都明显存在异常。<br>② 方法二：3σ原则<br>如果数据服从正态分布，在3σ原则下，异常值被定义为与平均值的偏差超过了3倍标准差的值。这是因为，在正态分布的假设下，具体平均值3倍标准差之外的值出现的概率低于0.003，属于极个别的小概率事件。<br>③ 方法三：箱线图分析<br>箱线图提供了识别异常的标准：异常值被定义为小于下四分位-1.5倍的四分位间距，或者大于上四分位+1.5倍的四分位间距的值。<br>箱线图分析不要求数据服从任何分布，因此对异常值的识别比较客观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#计算相关统计指标</span></span><br><span class="line">statDF = dataset2.describe()  <span class="comment">#获取描述性统计量</span></span><br><span class="line">statDF.loc[<span class="string">'mean+3std'</span>] = statDF.loc[<span class="string">'mean'</span>] + <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值+3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'mean-3std'</span>] = statDF.loc[<span class="string">'mean'</span>] - <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值-3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'75%+1.5dist'</span>] = statDF.loc[<span class="string">'75%'</span>] + <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算上四分位+1.5倍的四分位间距</span></span><br><span class="line">statDF.loc[<span class="string">'25%-1.5dist'</span>] = statDF.loc[<span class="string">'25%'</span>] - <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算下四分位-1.5倍的四分位间距</span></span><br><span class="line"><span class="comment">#获取各字段最大值、最小值</span></span><br><span class="line">statDF.loc[[<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line"><span class="comment">#判断取值是否大于平均值+3倍标准差</span></span><br><span class="line">dataset3 = dataset2 - statDF.loc[<span class="string">'mean+3std'</span>]</span><br><span class="line">dataset3[dataset3&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于平均值-3倍标准差</span></span><br><span class="line">dataset4 = dataset2 - statDF.loc[<span class="string">'mean-3std'</span>]</span><br><span class="line">dataset4[dataset4&lt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否大于上四分位+1.5倍的四分位间距</span></span><br><span class="line">dataset5 = dataset2 - statDF.loc[<span class="string">'75%+1.5dist'</span>]</span><br><span class="line">dataset5[dataset5&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于下四分位-1.5倍的四分位间距</span></span><br><span class="line">dataset6 = dataset2 - statDF.loc[<span class="string">'25%-1.5dist'</span>]</span><br><span class="line">dataset6[dataset6&lt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="（2）记录数分布检查"><a href="#（2）记录数分布检查" class="headerlink" title="（2）记录数分布检查"></a>（2）记录数分布检查</h4><p>主要检查字段的记录数分布是否超出合理的分布范围，包括三个指标：零值记录数、正值记录数、负值记录数。</p>
<h4 id="（3）波动检查"><a href="#（3）波动检查" class="headerlink" title="（3）波动检查"></a>（3）波动检查</h4><p>波动检查主要适用于有监督的数据，用于检查随着自变量的变化，因变量是否发生明显的波动情况。<br>以上异常值的识别方法主要针对连续型的字段，而对于离散型的字段的异常识别主要通过检查类别出现是否出现了合理阈值外的数据，比如苹果终端型号字段，出现了“P20”的取值。</p>
<h3 id="4-3-异常值的处理"><a href="#4-3-异常值的处理" class="headerlink" title="4.3 异常值的处理"></a>4.3 异常值的处理</h3><p>对于统计上的异常值的处理，主要采取两种方式：剔除或者替换。剔除是指直接将被标记为异常值的记录从数据集中删除掉，而替换是指将异常值用一个非异常值进行替换，比如边界值，或者有监督情况下的目标变量表征相似的某个值。<br>对于业务上的异常值的处理，原则就是进行深入探索分析，查找出现这一特殊情况的根本原因。</p>
<h1 id="2-直接影响数据质量的问题"><a href="#2-直接影响数据质量的问题" class="headerlink" title="2.直接影响数据质量的问题"></a>2.直接影响数据质量的问题</h1><p><strong>孤立的数据。</strong> 又称“数据筒仓”，这些独立的数据组要么属于特定的业务单元，要么包含在特定的软件中。隔离数据的问题是，组织的其他部分无法访问它，因为该软件可能与任何其他内容不兼容，或者业务单元严格控制用户权限。虽然这些数据可能提供有用的，甚至是非常有价值的洞察力，因为它不容易被访问，但是业务不能对它形成一个完整的图景，更不用说从中受益了。</p>
<p><strong>过时的数据。</strong> 企业结构庞大而复杂，有多个团队和部门。因此，跨组织收集数据通常是一个缓慢而费力的过程。到收集所有数据时，其中一些-如果不是大多数-在相关性方面已经落后，因此大大降低了其对组织的价值。</p>
<p><strong>复杂的数据。</strong> 数据来自许多不同的来源和不同的形式。数据来自智能手机、笔记本电脑、网站、客户服务交互、销售和营销、数据库等。它可以是结构化的，也可以是非结构化的。理解输入的数据量和数据种类，并使其标准化供每个人使用是一个资源密集型的过程，许多组织没有足够的带宽或专门知识来跟上</p>
<h1 id="3-如何提高数据质量"><a href="#3-如何提高数据质量" class="headerlink" title="3.如何提高数据质量"></a>3.如何提高数据质量</h1><p>和任何有价值的商业活动一样，提高数据的质量和效用是一个多步骤、多方法的过程。以下是如何：</p>
<ol>
<li><p><strong>方法1：</strong> <a href="https://www.deployinc.com/software-development/choosing-a-scripting-language-for-big-data-processing/" target="_blank" rel="noopener"> 大数据脚本 </a> 获取大量数据，并使用脚本语言与其他现有语言进行通信和组合，以清理和处理数据以进行分析。虽然工程师欣赏脚本的灵活性，但它确实需要对需要合成的数据类型和数据存在的特定上下文有一个重要的理解，以便知道要使用哪种脚本语言。判断和执行中的错误会打乱整个过程。</p>
</li>
<li><p><strong>方法2：</strong> 传统的ETL(提取、加载、转换)工具集成了来自不同来源的数据，并将其加载到数据仓库中，然后准备进行分析。但是，通常需要一组技术熟练的内部数据科学家首先手动清除数据，以解决与源和目的地之间存在的模式和格式不兼容的问题。更不方便的是，这些工具通常是批量处理，而不是实时处理。传统的ETL需要基础设施的类型、现场的专业知识以及很少有组织愿意投资的时间承诺。</p>
</li>
<li><p><strong>方法3：</strong> 开放源码工具提供数据质量服务，如解除欺骗、标准化、充实和实时清理，以及快速注册和比其他解决方案更低的成本。然而，大多数开源工具在实现任何真正的好处之前仍然需要一定程度的定制。对于服务的启动和运行，支持可能是有限的，这意味着组织必须再次依靠他们现有的IT团队来使其工作。</p>
</li>
<li><p><strong>方法4：</strong> <a href="https://www.alooma.com/blog/what-is-data-integration" target="_blank" rel="noopener"> 现代数据集成 </a> 通过自动集成、清理和转换数据，然后将数据存储在数据仓库或数据湖中，从而消除了传统ETL工具的手工操作。组织定义数据类型和目的地，并可以根据需要使用更新的客户详细信息、IP地理定位数据或其他信息丰富数据流。转换过程将来自所有源和各种格式的数据标准化，使其可供组织中的任何人使用。而且，由于它实时处理数据，用户可以检查数据流并纠正正在发生的任何错误。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>质量检查</tag>
        <tag>数仓</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop从mysql导入数据到hive</title>
    <url>/2020/07/22/sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。</p>
<pre><code>把MySQL、Oracle等数据库中的数据导入到HDFS、Hive、HBase中。
把HDFS、Hive、HBase中的数据导出到MySQL、Oracle等数据库中。
1.4 为sqoop1, 1.9 为sqoop2 ，sqoop1与sqoop2是不兼容的。</code></pre><a id="more"></a>

<h3 id="实现需要"><a href="#实现需要" class="headerlink" title="实现需要"></a>实现需要</h3><p>数据库:</p>
<ul>
<li>driver</li>
<li>URL、username、password</li>
<li>database、table</li>
</ul>
<p>hadoop:</p>
<ul>
<li>type (hdfp、hive、hbase)</li>
<li>path 存储到哪里？</li>
<li>数据分隔符</li>
<li>mappers 数量，也就是使用多少线程。</li>
</ul>
<h2 id="二、命令"><a href="#二、命令" class="headerlink" title="二、命令"></a>二、命令</h2><h3 id="查看-sqoop-支持的命令"><a href="#查看-sqoop-支持的命令" class="headerlink" title="查看 sqoop 支持的命令"></a>查看 sqoop 支持的命令</h3><p>  sqoop help</p>
<h3 id="显示所有库名"><a href="#显示所有库名" class="headerlink" title="显示所有库名"></a>显示所有库名</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306 \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="显示某个数据库里所有表"><a href="#显示某个数据库里所有表" class="headerlink" title="显示某个数据库里所有表"></a>显示某个数据库里所有表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="MYSQL-导入数据到-HIVE"><a href="#MYSQL-导入数据到-HIVE" class="headerlink" title="MYSQL 导入数据到 HIVE"></a>MYSQL 导入数据到 HIVE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234 \</span></span><br><span class="line"><span class="comment">--table people</span></span><br><span class="line"><span class="comment">--hive-import </span></span><br><span class="line"><span class="comment">--create-hive-table </span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"</span></span><br><span class="line">-m 5</span><br></pre></td></tr></table></figure>

<h3 id="hive-参数"><a href="#hive-参数" class="headerlink" title="hive 参数"></a>hive 参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–hive-import 必须参数，指定导入hive</span><br><span class="line">–hive-database default hive库名</span><br><span class="line">–hive-table people hive表名</span><br><span class="line">–fields-terminated-by hive的分隔符</span><br><span class="line">–hive-overwrite 重写重复字段</span><br><span class="line">–create-hive-table 帮创建好 hive 表，但是表存在会出错。不建议使用这个参数，因为到导入的时候，会与我们的字段类型有出入。</span><br><span class="line">–hive-partition-key “dt” 指定分区表的字段</span><br><span class="line">–hive-partition-value “2018-08-08” 指定分区表的值</span><br></pre></td></tr></table></figure>

<h3 id="导出没有主键的表"><a href="#导出没有主键的表" class="headerlink" title="导出没有主键的表"></a>导出没有主键的表</h3><p>可以使用两种方式：</p>
<ul>
<li>–split-by 指定切分的字段</li>
<li>-m 1 : 设置只使用一个map进行数据迁移</li>
</ul>
<h3 id="过滤条件"><a href="#过滤条件" class="headerlink" title="过滤条件"></a>过滤条件</h3><p>–where “age&gt;18” 匹配条件<br>       –columns “name,age” 选择要导入的指定列<br>       –query ‘select * from people where age&gt;18 and $CONDITIONS’: sql语句查询的结果集<br>      不能 –table 一起使用<br>      需要指定 –target-dir 路径 </p>
<h3 id="当数据库中字符为空时的处理"><a href="#当数据库中字符为空时的处理" class="headerlink" title="当数据库中字符为空时的处理"></a>当数据库中字符为空时的处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–null-non-string ‘0’ 当不是字符串的数据为空的时候，用 0 替换</span><br><span class="line">–null-string ‘string’ 当字符串为空的时候，使用string 字符替换</span><br></pre></td></tr></table></figure>

<h3 id="提高传输速度"><a href="#提高传输速度" class="headerlink" title="提高传输速度"></a>提高传输速度</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–direct 提高数据库到hadoop的传输速度</span><br></pre></td></tr></table></figure>

<p>支持的数据库类型与版本：</p>
<ul>
<li>myslq 5.0 以上</li>
<li>oracle 10.2.0 以上</li>
</ul>
<h3 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h3><p>增量导入对应，首先需要知监控那一列，这列要从哪个值开始增量</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">* check-column id 用来指定一些列</span><br><span class="line">* 这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的,常用的是指定主键id.</span><br><span class="line">* –check-column 可以去指定多个列</span><br></pre></td></tr></table></figure>

<ul>
<li>last-value 10 从哪个值开始增量</li>
<li>incremental 增量的模式<ul>
<li>append id 是获取大于某一列的某个值。</li>
<li>lastmodified “2016-12-15 15:47:30” 获取某个时间后修改的所有数据<ul>
<li>–append 附加模式</li>
<li>–merge-key id 合并模式</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意：增量导入不能与 –delete-target-dir 一起使用，还有必须指定增量的模式</p>
]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Python时间操作</title>
    <url>/2020/07/22/python%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">print</span> time.time()	<span class="comment">#输出的结果是:1279578704.6725271</span></span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>但是这样是一连串的数字不是我们想要的结果，我们可以利用time模块的格式化时间的方法来处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.localtime(time.time())</span><br></pre></td></tr></table></figure>

<p>用time.localtime()方法，作用是格式化时间戳为本地的时间。<br>输出的结果是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.struct_time(tm_year=<span class="number">2010</span>, tm_mon=<span class="number">7</span>, tm_mday=<span class="number">19</span>, tm_hour=<span class="number">22</span>, tm_min=<span class="number">33</span>, tm_sec=<span class="number">39</span>, tm_wday=<span class="number">0</span>, tm_yday=<span class="number">200</span>, tm_isdst=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>现在看起来更有希望格式成我们想要的时间了。<br>time.strftime(‘%Y-%m-%d’,time.localtime(time.time()))</p>
<p>最后用time.strftime()方法，把刚才的一大串信息格式化成我们想要的东西，现在的结果是：<br>2010-07-19</p>
<p>time.strftime里面有很多参数，可以让你能够更随意的输出自己想要的东西：<br>下面是time.strftime的参数：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">strftime(format[, tuple]) -&gt; string</span><br></pre></td></tr></table></figure>

<p>将指定的struct_time(默认为当前时间)，根据指定的格式化字符串输出<br>python中时间日期格式化符号：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%y 两位数的年份表示（<span class="number">00</span><span class="number">-99</span>）</span><br><span class="line">%Y 四位数的年份表示（<span class="number">000</span><span class="number">-9999</span>）</span><br><span class="line">%m 月份（<span class="number">01</span><span class="number">-12</span>）</span><br><span class="line">%d 月内中的一天（<span class="number">0</span><span class="number">-31</span>）</span><br><span class="line">%H <span class="number">24</span>小时制小时数（<span class="number">0</span><span class="number">-23</span>）</span><br><span class="line">%I <span class="number">12</span>小时制小时数（<span class="number">01</span><span class="number">-12</span>） </span><br><span class="line">%M 分钟数（<span class="number">00</span>=<span class="number">59</span>）</span><br><span class="line">%S 秒（<span class="number">00</span><span class="number">-59</span>）</span><br><span class="line">%a 本地简化星期名称</span><br><span class="line">%A 本地完整星期名称</span><br><span class="line">%b 本地简化的月份名称</span><br><span class="line">%B 本地完整的月份名称</span><br><span class="line">%c 本地相应的日期表示和时间表示</span><br><span class="line">%j 年内的一天（<span class="number">001</span><span class="number">-366</span>）</span><br><span class="line">%p 本地A.M.或P.M.的等价符</span><br><span class="line">%U 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期天为星期的开始</span><br><span class="line">%w 星期（<span class="number">0</span><span class="number">-6</span>），星期天为星期的开始</span><br><span class="line">%W 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期一为星期的开始</span><br><span class="line">%x 本地相应的日期表示</span><br><span class="line">%X 本地相应的时间表示</span><br><span class="line">%Z 当前时区的名称</span><br><span class="line">%% %号本身</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive中Parquet格式的使用</title>
    <url>/2020/07/22/Hive%E4%B8%ADParquet%E6%A0%BC%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Hive中Parquet格式的使用"><a href="#Hive中Parquet格式的使用" class="headerlink" title="Hive中Parquet格式的使用"></a>Hive中Parquet格式的使用</h1><p><strong>#Hive建外部External表（外部表external table）：</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`table_name`</span>(</span><br><span class="line">  <span class="string">`column1`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column2`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column3`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`proc_date`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">'hdfs://hdfscluster/...'</span></span><br><span class="line">TBLPROPERTIES ( <span class="string">'orc.compress'</span>=<span class="string">'snappy'</span>);</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#添加分区并加载分区数据：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'...'</span>（不改变源数据存储位置）</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> exsit <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'hdfs://hdfscluster/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'...'</span> <span class="keyword">into</span> <span class="keyword">table</span> table_name <span class="keyword">partition</span>(proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>);（会将源数据切到hive表指定的路径下）</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除分区：alter table table_name drop if exists partition(proc_date='$&#123;hivevar:pdate&#125;');</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>Parquet列式存储格式</title>
    <url>/2020/07/22/parquet%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
    <content><![CDATA[<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目，最新的版本是1.8.0。</p>
<a id="more"></a>

<h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>列式存储和行式存储相比有哪些优势呢？</p>
<ol>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
</ol>
]]></content>
      <categories>
        <category>Parquet</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑主键,业务主键和复合主键</title>
    <url>/2020/07/21/%E9%80%BB%E8%BE%91%E4%B8%BB%E9%94%AE,%E4%B8%9A%E5%8A%A1%E4%B8%BB%E9%94%AE%E5%92%8C%E5%A4%8D%E5%90%88%E4%B8%BB%E9%94%AE/</url>
    <content><![CDATA[<p><strong>1.概念定义</strong></p>
<ul>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms191236(v=SQL.100).aspx" target="_blank" rel="noopener">主键(PRIMARY KEY)</a>：表通常具有包含唯一标识表中每一行的值的一列或一组列。这样的一列或多列称为表的主键 (PK)，用于强制表的实体完整性。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms175464(v=SQL.100).aspx" target="_blank" rel="noopener">外键(FOREIGN KEY)</a>：外键 (FK) 是用于建立和加强两个表数据之间的链接的一列或多列。在外键引用中，当一个表的列被引用作为另一个表的主键值的列时，就在两表之间创建了链接。这个列就成为第二个表的外键。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms190639(v=SQL.100).aspx" target="_blank" rel="noopener">聚集索引</a>：聚集索引基于数据行的键值在表内排序和存储这些数据行。每个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179325(v=SQL.100).aspx" target="_blank" rel="noopener">非聚集索引</a>：非聚集索引包含索引键值和指向表数据存储位置的行定位器。可以对表或索引视图创建多个非聚集索引。通常，设计非聚集索引是为改善经常使用的、没有建立聚集索引的查询的性能。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179413(v=SQL.100).aspx" target="_blank" rel="noopener">自动编号列和标识符列</a>：对于每个表，均可创建一个包含系统生成的序号值的标识符列，该序号值以唯一方式标识表中的每一行。</p>
</li>
<li><p>业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。</p>
</li>
<li><p>逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。</p>
</li>
<li><p>复合主键（联合主键）：通过两个或者多个字段的组合作为主键。</p>
<a id="more"></a>

<p><strong>2.原理分析</strong></p>
<p>​    使用逻辑主键的主要原因是，业务主键一旦改变则系统中关联该主键的部分的修改将会是不可避免的，并且引用越多改动越大。而使用逻辑主键则只需要修改相应的业务主键相关的业务逻辑即可，减少了因为业务主键相关改变对系统的影响范围。业务逻辑的改变是不可避免的，因为“永远不变的是变化”，没有任何一个公司是一成不变的，没有任何一个业务是永远不变的。最典型的例子就是身份证升位和驾驶执照号换用身份证号的业务变更。而且现实中也确实出现了<a href="http://zhidao.baidu.com/question/22152449" target="_blank" rel="noopener">身份证号码重复</a>的情况，这样如果用身份证号码作为主键也带来了难以处理的情况。当然应对改变，可以有很多解决方案，方案之一是做一新系统与时俱进，这对软件公司来说确实是件好事。</p>
<p>​    使用逻辑主键的另外一个原因是，业务主键过大，不利于传输、处理和存储。我认为一般如果业务主键超过8字节就应该考虑使用逻辑主键了，因为int是4字节的，bigint是8字节的，而业务主键一般是字符串，同样是 8 字节的 bigint 和 8 字节的字符串在传输和处理上自然是 bigint 效率更高一些。想象一下 code == “12345678” 和 id == 12345678 的汇编码的不同就知道了。当然逻辑主键不一定是 int 或者 bigint  ，而业务主键也不一定是字符串也可以是 int 或 datetime  等类型，同时传输的也不一定就是主键，这个就要具体分析了，但是原理类似，这里只是讨论通常情况。同时如果其他表需要引用该主键的话，也需要存储该主键，那么这个存储空间的开销也是不一样的。而且这些表的这个引用字段通常就是外键，或者通常也会建索引方便查找，这样也会造成存储空间的开销的不同，这也是需要具体分析的。</p>
<p>​    使用逻辑主键的再一个原因是，使用 int 或者 bigint 作为外键进行联接查询，性能会比以字符串作为外键进行联接查询快。原理和上面的类似，这里不再重复。</p>
<p>​    使用逻辑主键的再一个原因是，存在用户或维护人员误录入数据到业务主键中的问题。例如错把 RMB 录入为 RXB  ，相关的引用都是引用了错误的数据，一旦需要修改则非常麻烦。如果使用逻辑主键则问题很好解决，如果使用业务主键则会影响到其他表的外键数据，当然也可以通过级联更新方式解决，但是不是所有都能级联得了的。</p>
<p>​    使用业务主键的主要原因是，增加逻辑主键就是增加了一个业务无关的字段，而用户通常都是对于业务相关的字段进行查找（比如员工的工号，书本的 ISBN No.  ），这样我们除了为逻辑主键加索引，还必须为这些业务字段加索引，这样数据库的性能就会下降，而且也增加了存储空间的开销。所以对于业务上确实不常改变的基础数据而言，使用业务主键不失是一个比较好的选择。另一方面，对于基础数据而言，一般的增、删、改都比较少，所以这部分的开销也不会太多，而如果这时候对于业务逻辑的改变有担忧的话，也是可以考虑使用逻辑主键的，这就需要具体问题具体分析了。</p>
<p>​    使用业务主键的另外一个原因是，对于用户操作而言，都是通过业务字段进行的，所以在这些情况下，如果使用逻辑主键的话，必须要多做一次映射转换的动作。我认为这种担心是多余的，直接使用业务主键查询就能得到结果，根本不用管逻辑主键，除非业务主键本身就不唯一。另外，如果在设计的时候就考虑使用逻辑主键的话，编码的时候也是会以主键为主进行处理的，在系统内部传输、处理和存储都是相同的主键，不存在转换问题。除非现有系统是使用业务主键，要把现有系统改成使用逻辑主键，这种情况才会存在转换问题。暂时没有想到还有什么场景是存在这样的转换的。</p>
<p>​    使用业务主键的再一个原因是，对于银行系统而言安全性比性能更加重要，这时候就会考虑使用业务主键，既可以作为主键也可以作为冗余数据，避免因为使用逻辑主键带来的关联丢失问题。如果由于某种原因导致主表和子表关联关系丢失的话，银行可是会面临无法挽回的损失的。为了杜绝这种情况的发生，业务主键需要在重要的表中有冗余存在，这种情况最好的处理方式就是直接使用业务主键了。例如身份证号、存折号、卡号等。所以通常银行系统都要求使用业务主键，这个需求并不是出于性能的考虑而是出于安全性的考虑。</p>
<p>​    使用复合主键的主要原因和使用业务主键是相关的，通常业务主键只使用一个字段不能解决问题，那就只能使用多个字段了。例如使用姓名字段不够用了，再加个生日字段。这种使用复合主键方式效率非常低，主要原因和上面对于较大的业务主键的情况类似。另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。</p>
<p>​    使用复合主键的另外一个原因是，对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。</p>
<p>综合来说，网上大多数人是倾向于用逻辑主键的，而对于实体表用复合主键方式的应该没有多少人认同。支持业务主键的人通常有种误解，认为逻辑主键必须对用户来说有意义，其实逻辑主键只是系统内部使用的，对用户来说是无需知道的。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>主键</tag>
      </tags>
  </entry>
  <entry>
    <title>hive两个聚合函数的计算结果拼接成表并做进一步计算</title>
    <url>/2020/07/23/%E4%B8%A4%E4%B8%AA%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E6%8B%BC%E6%8E%A5/</url>
    <content><![CDATA[<h4 id="hive两个聚合函数的计算结果拼接成表并做进一步计算"><a href="#hive两个聚合函数的计算结果拼接成表并做进一步计算" class="headerlink" title="hive两个聚合函数的计算结果拼接成表并做进一步计算"></a>hive两个聚合函数的计算结果拼接成表并做进一步计算</h4><p>hive两个聚合函数的计算结果拼接成表让LZ头疼了很久，一度想到用python处理，或者新建两张临时表保存聚合函数的结果然后再取出数据进行计算，或者使用UDF, 但总觉得还有其他方法。经过一番探索，发现WITH AS 可以方便快捷解决此问题。</p>
<a id="more"></a>

<p>WITH AS短语，也叫做子查询部分（subquery factoring），可以让你做很多事情，定义一个SQL片断，该SQL片断会被整个SQL语句所用到。</p>
<p>需求：统计test1表满足某条件的记录数和test2表满足某条件的记录数然后做除法。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> c1</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*） <span class="keyword">as</span> aa <span class="keyword">from</span> test1 ),</span><br><span class="line">c2</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*）　<span class="keyword">as</span> bb <span class="keyword">from</span> test2)</span><br><span class="line"><span class="keyword">select</span> a.aa/b.bb <span class="keyword">from</span> c1 a, c2 b ;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title>mac连接公司vpn的坑</title>
    <url>/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>1.出现IPSec共享密匙丢失</p>
<p>2.能访问公司内网,无法访问公网</p>
<a id="more"></a>

<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h3 id="1-出现IPSec共享密匙丢失"><a href="#1-出现IPSec共享密匙丢失" class="headerlink" title="1.出现IPSec共享密匙丢失"></a>1.出现IPSec共享密匙丢失</h3><p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/FEHZDY59BSnXNCP.png" alt="FEHZDY59BSnXNCP"></p>
<h4 id="原因"><a href="#原因" class="headerlink" title="原因:"></a>原因:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;etc&#x2F;ppp&#x2F;options</span><br><span class="line">包含缺省应用于系统中所有 PPP 链路的特征（例如，计算机是否要求对等点对其本身进行验证）的文件。如果不存在此文件，将禁止非超级用户使用 PPP。</span><br></pre></td></tr></table></figure>

<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>知其然，所以知其后然，这时候的解决方法就是在<code>/etc/ppp/</code>目录下建立options`这个配置文件，并且配置ppp链路l2tp不需要ipsec密钥。</p>
<p>下面就是vim命令操作，如果想系统学习相关命令可查看 <a href="https://link.jianshu.com?t=http%3A%2F%2Fwww.cnblogs.com%2Fpeida%2Farchive%2F2012%2F12%2F05%2F2803591.html" target="_blank" rel="noopener">每天一个linux命令目录</a>，这里不打算详细讲解，有兴趣同学可以另行学习。</p>
<p><strong>操作步骤</strong><br> （1）在终端任意路径下输入命令： <code>sudo vim /etc/ppp/options</code><br> 然后输入电脑密码后，显示vim操作界面后按键盘<code>i</code>进入插入模式，输入下面内容：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">plugin</span> <span class="selector-tag">L2TP</span><span class="selector-class">.ppp</span></span><br><span class="line"><span class="selector-tag">l2tpnoipsec</span></span><br></pre></td></tr></table></figure>

<p>（2）然后按<code>esc</code>键退出插入模式，最后输入<code>:wq!</code>，强制保存并退出vim模式。</p>
<h3 id="2-能访问公司内网-无法访问公网"><a href="#2-能访问公司内网-无法访问公网" class="headerlink" title="2.能访问公司内网,无法访问公网"></a>2.能访问公司内网,无法访问公网</h3><p>在vpn的高级里设置:</p>
<p>1.</p>
<p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/v3IlwSyEFjTxR95-20200724191748915.png" alt="v3IlwSyEFjTxR95-20200724191748915"></p>
<p>2.</p>
<p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/fDRpcNP2a79kMUS.png" alt="fDRpcNP2a79kMUS"></p>
<p>即可</p>
]]></content>
      <categories>
        <category>公司vpn</category>
      </categories>
      <tags>
        <tag>远程访问</tag>
        <tag>公司vpn</tag>
      </tags>
  </entry>
  <entry>
    <title>同比与环比</title>
    <url>/2020/07/24/%E5%90%8C%E6%AF%94%E7%8E%AF%E6%AF%94/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>同比：本期与同期做对比。</p>
<p>环比：本期与上期做对比。</p>
<p>简单点说，同比和环比用于表示某一事物在对比时期内发展变化的方向和程度。以历史同期为基期，例如2016年2月份与2015年2月份、2016年上半年与2015年上半年的比较，就是同比。以前一个统计时间段为基期，例如2016年6月份与2016年5月份、2016年二季度与2016年一季度的比较，就是环比。</p>
<p><img src="/2020/07/24/%E5%90%8C%E6%AF%94%E7%8E%AF%E6%AF%94/c5EoSijy31QIDAx.png" alt="c5EoSijy31QIDAx"></p>
<a id="more"></a>

<p>环比一般是用在月、日很少用在年上，主要是对比很短时间内涨幅程度，不过由于行业差异，比如旅游，会受到淡旺季影响。</p>
<p>同比一般用在相邻两年，相同时间段内，查看涨幅程度，一般用在两年相同月份，很少用在两月相同日期</p>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><h3 id="一、同比增长计算公式："><a href="#一、同比增长计算公式：" class="headerlink" title="一、同比增长计算公式："></a>一、同比增长计算公式：</h3><p>1、同比增长率=（本期数－同期数）÷同期数×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值300万，同比增长率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<p>2、当同期数为负值的情况，公式应当完善如下：</p>
<p>同比增长率=（本期数－同期数）÷ |同期数|×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值50万，同比增长率是多少？</p>
<p>本题中，同比增长率=(50W-(-100W))/|-100W||×100%=150%</p>
<h3 id="二、环比增长计算公式："><a href="#二、环比增长计算公式：" class="headerlink" title="二、环比增长计算公式："></a>二、<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>计算公式：</h3><p><a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率=（本期数-上期数）/上期数×100%。</p>
<p>例子：比如说今年3月的产值100万，2月的产值300万，<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>同比与环比的区别</p>
<p>同比、环比与定基比，都可以用百分数或倍数表示。定基比发展速度，也简称总速度，一般是指报告期水平与某一固定时期水平之比，表明这种现象在较长时期内总的发展速度。</p>
<p>同比发展速度，一般指是指本期发展水平与<a href="https://www.baidu.com/s?wd=上年同期&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">上年同期</a>发展水平对比，而达到的相对发展速度。环比发展速度，一般是指报告期水平与前一时期水平之比，表明现象逐期的发展速度。</p>
<p>同比和环比，这两者所反映的虽然都是变化速度，但由于采用基期的不同，其反映的内涵是完全不同的；一般来说，环比可以与环比相比较，而不能拿同比与环比相比较；而对于同一个地方，考虑时间纵向上发展趋势的反映，则往往要把同比与环比放在一起进行对照。</p>
]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>同比/环比</tag>
      </tags>
  </entry>
  <entry>
    <title>hive on spark与spark on hive的区别</title>
    <url>/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h3 id="SparkSQL简介"><a href="#SparkSQL简介" class="headerlink" title="SparkSQL简介"></a>SparkSQL简介</h3><p>SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，hive应运而生，它是当时唯一运行在Hadoop上的SQL-on-hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，Shark应运而生，但又因为Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等),2014年spark团队停止对Shark的开发，将所有资源放SparkSQL项目上</p>
<a id="more"></a>

<h3 id="SparkSQL、Hive-on-Spark的关系"><a href="#SparkSQL、Hive-on-Spark的关系" class="headerlink" title="SparkSQL、Hive on Spark的关系"></a><strong>SparkSQL、Hive on Spark的关系</strong></h3><p><img src="/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/UPVGS1qbLynB639.png" alt="UPVGS1qbLynB639"></p>
<p>由上图可以看出，SparkSQL之所以要从Shark中孵化出来，初衷就是为了剥离Shark对于Hive的太多依赖。SparkSQL作为Spark生态中独立的一员继续发展，不在受限于Hive，只是兼容Hive；而Hive on  Spark是Hive的发展计划，该计划将Spark作为Hive最底层的引擎之一，Hive不在受限于一个引擎（之前只支持map-reduce），可以采用map-reduce、Tez、Spark等计算引擎。</p>
<p>hive on  Spark是有Cloudera发起，有Intel、MapR等公司共同参与的开源项目，其目的就是将Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上面进行计算。通过该项目，可以提高Hive查询的性能，同事为已经部署了Hive或者Spark的用户提供了更加灵活地选择，从而进一步提高Hive和Spark的普及率。</p>
<p>hive on Spark和SparkSQL的结构类似，只是SQL引擎不同，但是计算引擎都是spark</p>
<p>sparkSQL通过sqlcontext来进行使用，hive on  spark通过hivecontext来使用。sqlcontext和hivecontext都是来自于同一个包，从这个层面上理解，其实hive on spark和sparkSQL并没有太大差别。</p>
<p>结构上来看，Hive on Spark和SparkSQL都是一个翻译曾，将SQL翻译成分布是可以执行的Spark程序。</p>
<p>SQLContext：spark处理结构化数据的入口，允许创建DataFrame以及sql查询。</p>
<p>HiveContext：Spark sql执行引擎，集成hive数据，读取在classpath的hive-site.xml配置文件配置hive。所以ye</p>
<h3 id="SparkSQL组件和运行架构"><a href="#SparkSQL组件和运行架构" class="headerlink" title="SparkSQL组件和运行架构"></a><strong>SparkSQL组件和运行架构</strong></h3><p>1-SQLContext：Spark SQL提供SQLContext封装Spark中的所有关系型功能。可以用之前的示例中的现有SparkContext创建SQLContext。<br>2-DataFrame：DataFrame是一个分布式的，按照命名列的形式组织的数据集合。DataFrame基于R语言中的data frame概念，与关系型数据库中的数据库表类似。通过调用将DataFrame的内容作为行RDD（RDD of  Rows）返回的rdd方法，可以将DataFrame转换成RDD。可以通过如下数据源创建DataFrame：已有的RDD、结构化数据文件、JSON数据集、Hive表、外部数据库。<br>了私语关系型数据库，SparkSQL中的SQL语句也是由Projection、Data source、Filter但部分组成，分别对应于sql查询过程中的Result、Data  source和Operation；SQL语句是按照Operation-》Data Source -》Result的次序来描述的。如下所示：</p>
<p><img src="/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/XQZNzEvBbMxS6r1.png" alt="XQZNzEvBbMxS6r1"></p>
<p>下面对上图中展示的SparkSQL语句的执行顺序进行详细解释：</p>
<p>1-对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范；<br>Projection：简单说就是select选择的列的集合，参考：SQL Projection<br>2-将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的；<br>3-一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）；<br>4-计划执行（Execute），按Operation–&gt;Data Source–&gt;Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。</p>
<h3 id="SQLContext和HiveContext"><a href="#SQLContext和HiveContext" class="headerlink" title="SQLContext和HiveContext"></a><strong>SQLContext和HiveContext</strong></h3><p>当使用SparkSQL时，根据是否要使用Hive，有两个不同的入口。推荐使用入口HiveContext，HiveContext继承自SQLContext。它可以提供HiveQL以及其他依赖于Hive的功能的支持。更为基础的SQLContext则仅仅支持SparlSQL功能的一个子集，子集中去掉了需要依赖Hive的功能。这种分离主要视为那些可能会因为引入Hive的全部依赖而陷入依赖冲突的用户而设计的。因为使用HiveContext的时候不需要事先部署好Hive。如果要把一个Spark  SQL链接到部署好的Hive上面，必须将hive-site.xml复制到Spark的配置文件目录中（$SPARK_HOME/conf）。即使没有部署好Hive，SparkSQL也可以运行，如果没有部署好Hive，但是还要使用HiveContext的话，那么SparkSQL将会在当前的工作目录中创建出自己的Hive元数据仓库，叫做metastore_db。，如果使用HiveQL中的CREATETABLE语句来创建表，那么这些表将会被放在默认的文件系统中的/user/hive/warehouse目录中，这里默认的文件系统视情况而定，如果配置了hdfs-site.xml那么就会存放在HDFS上面，否则就存放在本地文件系统中。</p>
<p>运行HiveContext的时候hive环境并不是必须，但是需要hive-site.xml配置文件。</p>
<hr>
<h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>hive on spark大体与SparkSQL结构类似，只是SQL引擎不同，但是计算引擎都是spark！</p>
<p>在pyspark中使用Hive on Spark是中怎么样的体验</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化Spark SQL</span></span><br><span class="line"><span class="comment">#导入Spark SQL</span></span><br><span class="line">from pyspark.sql import HiveContext,Row</span><br><span class="line"><span class="comment"># 当不能引入Hive依赖时</span></span><br><span class="line"><span class="comment"># from pyspark.sql import SQLContext,Row</span></span><br><span class="line"><span class="comment"># 注意，上面那一点才是关键的，他两来自于同一个包，你们区别能有多大</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hiveCtx = HiveContext(sc)   <span class="comment">#创建SQL上下文环境</span></span><br><span class="line">input = hiveCtx.jsonFile(inputFile)   <span class="comment">#基本查询示例</span></span><br><span class="line">input.registerTempTable("tweets")   <span class="comment">#注册输入的SchemaRDD（SchemaRDD在Spark 1.3版本后已经改为DataFrame）</span></span><br><span class="line"><span class="comment">#依据retweetCount(转发计数)选出推文</span></span><br><span class="line">topTweets = hiveCtx.sql("<span class="keyword">SELECT</span> <span class="built_in">text</span>,retweetCount <span class="keyword">FROM</span> tweets <span class="keyword">ORDER</span> <span class="keyword">BY</span> retweetCount <span class="keyword">LIMIT</span> <span class="number">10</span><span class="string">")</span></span><br></pre></td></tr></table></figure>

<p>我们可以看到，sqlcontext和hivecontext都是出自于pyspark.sql包，可以从这里理解的话，其实hive on spark和sparksql并没有太大差别</p>
<p>结构上Hive On Spark和SparkSQL都是一个翻译层，把一个SQL翻译成分布式可执行的Spark程序。而且大家的引擎都是spark</p>
<p>SparkSQL和Hive On  Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On  Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。—From <a href="http://blog.csdn.net/yeruby/article/details/51448188" target="_blank" rel="noopener">SparkSQL与Hive on Spark的比较</a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive on spark</tag>
        <tag>spark on hive</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习项目01-鸢尾花分类</title>
    <url>/2020/07/24/ML-flower/</url>
    <content><![CDATA[<p>机器学习项目之鸢尾花分类:</p>
<h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述:"></a>任务描述:</h3><p>构建一个模型，根据鸢尾花的花萼和花瓣大小将其分为三种不同的品种。</p>
<p><img src="/2020/07/24/ML-flower/dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c.png" alt="dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c"></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> colors     </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm            </span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">'/Users/taoxuefeng/Desktop/jupyterlab/jupyterlab_python/MLData/flower_data/iris.data'</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iris_type</span><span class="params">(s)</span>:</span></span><br><span class="line">    it = &#123;<span class="string">b'Iris-setosa'</span>:<span class="number">0</span>, <span class="string">b'Iris-versicolor'</span>:<span class="number">1</span>, <span class="string">b'Iris-virginica'</span>:<span class="number">2</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> it[s]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.loadtxt(data_path,                <span class="comment">#数据文件路径</span></span><br><span class="line">                 dtype=float,               <span class="comment">#数据文件类型</span></span><br><span class="line">                 delimiter=<span class="string">','</span>,             <span class="comment">#数据分隔符</span></span><br><span class="line">                 converters=&#123;<span class="number">4</span>:iris_type&#125;)  <span class="comment">#将第5列使用函数iris_type进行转换</span></span><br><span class="line"><span class="comment">#print(data)</span></span><br><span class="line"><span class="comment">#print(data.shape)#(150, 5)</span></span><br><span class="line">x, y = np.split(data,     <span class="comment">#要切分的数组</span></span><br><span class="line">               (<span class="number">4</span>,),      <span class="comment">#沿轴切分的位置，第5列开始往后为y</span></span><br><span class="line">               axis = <span class="number">1</span>)  <span class="comment">#代表纵向分割，按列分割</span></span><br><span class="line">x = x[:, <span class="number">0</span>:<span class="number">2</span>]<span class="comment">#在X中我们取前两列作为特征，为了后面的可视化。x[:,0:2]代表第一维(行)全取，第二维(列)取0~2</span></span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line">x_train, x_test, y_train, y_test = model_selection.train_test_split(x,<span class="comment">#索要划分的样本特征集</span></span><br><span class="line">                                                                   y,<span class="comment">#所要划分的样本结果</span></span><br><span class="line">                                                                   random_state = <span class="number">1</span>,<span class="comment">#随机数种子</span></span><br><span class="line">                                                                   test_size = <span class="number">0.3</span>) <span class="comment">#测试样本占比</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**********************SVM分类器构建*************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#clf = svm.SVC(C=0.8,kernel='rbf', gamma=50,decision_function_shape='ovr')</span></span><br><span class="line">    clf = svm.SVC(C=<span class="number">0.5</span>,                         <span class="comment">#误差项惩罚系数,默认值是1</span></span><br><span class="line">                  kernel=<span class="string">'linear'</span>,               <span class="comment">#线性核 kenrel="rbf":高斯核</span></span><br><span class="line">                  decision_function_shape=<span class="string">'ovr'</span>) <span class="comment">#决策函数</span></span><br><span class="line">    <span class="keyword">return</span> clf</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.定义模型：SVM模型定义</span></span><br><span class="line">clf = classifier()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练SVM模型</span></span><br><span class="line">train(clf,x_train,y_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**************并判断a b是否相等，计算acc的均值*************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span><span class="params">(a, b, tip)</span>:</span></span><br><span class="line">    acc = a.ravel() == b.ravel()</span><br><span class="line">    print(<span class="string">'%s Accuracy:%.3f'</span> %(tip, np.mean(acc)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_accuracy</span><span class="params">(clf,x_train,y_train,x_test,y_test)</span>:</span></span><br><span class="line">    <span class="comment">#分别打印训练集和测试集的准确率  score(x_train,y_train):表示输出x_train,y_train在模型上的准确率</span></span><br><span class="line">    print(<span class="string">'trianing prediction:%.3f'</span> %(clf.score(x_train, y_train)))</span><br><span class="line">    print(<span class="string">'test data prediction:%.3f'</span> %(clf.score(x_test, y_test)))</span><br><span class="line">    <span class="comment">#原始结果与预测结果进行对比   predict()表示对x_train样本进行预测，返回样本类别</span></span><br><span class="line">    show_accuracy(clf.predict(x_train), y_train, <span class="string">'traing data'</span>)</span><br><span class="line">    show_accuracy(clf.predict(x_test), y_test, <span class="string">'testing data'</span>)</span><br><span class="line">    <span class="comment">#计算决策函数的值，表示x到各分割平面的距离</span></span><br><span class="line">    print(<span class="string">'decision_function:\n'</span>, clf.decision_function(x_train))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.模型评估</span></span><br><span class="line">print_accuracy(clf,x_train,y_train,x_test,y_test)</span><br></pre></td></tr></table></figure>

<pre><code>trianing prediction:0.819
test data prediction:0.778
traing data Accuracy:0.819
testing data Accuracy:0.778
decision_function:
 [[-0.30200388  1.26702365  2.28292526]
 [ 2.1831931  -0.19913458  1.06956422]
 [ 2.25424706  0.79489006 -0.20587224]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 0.95815482  2.18401419 -0.17375192]
 [ 2.23120771  0.84075865 -0.19144453]
 [ 2.17327158 -0.14884286  0.92795057]
 [-0.28667175  1.11372202  2.28302495]
 [-0.27989264  1.21274017  2.25881762]
 [-0.29313813  1.24442795  2.2732035 ]
 [-0.27008816  1.2272086   2.22682127]
 [-0.25981661  2.21998499  1.20479842]
 [-0.17071168  0.99542159  2.17180911]
 [-0.30018876  1.25829325  2.2829419 ]
 [-0.17539342  2.15368837  1.06772814]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.23988847  2.23286001  1.06656755]
 [-0.26915223  2.23333222  1.21679709]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 2.2530903   0.85932358 -0.2359772 ]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.26803658  0.80468578 -0.24299359]
 [-0.24030826  1.18556963  2.19011259]
 [-0.25881807  1.17240759  2.23535197]
 [-0.27273902  1.20332527  2.24866913]
 [-0.20956348  2.19674141  1.06726512]
 [-0.26556065  1.16490628  2.24871607]
 [-0.22965507  1.17870942  2.17146651]
 [ 2.25807657 -0.22526231  0.80881977]
 [-0.27322701  2.25917947  1.17077691]
 [-0.26638767  1.21631409  2.22685842]
 [-0.26740532  1.20784059  2.23528903]
 [-0.12135744  2.22922779  0.79343961]
 [-0.2365929   1.12219635  2.21706342]
 [-0.21558048  2.22640865  0.92573306]
 [ 2.22344499 -0.19955645  0.88288227]
 [ 2.22671228  0.93600592 -0.21794279]
 [ 2.26578978 -0.24701281  0.82742467]
 [-0.26556065  1.16490628  2.24871607]
 [ 2.26204658  0.89725133 -0.25453765]
 [-0.2518152   2.22343258  1.17120859]
 [-0.27340098  1.23624732  2.22678409]
 [-0.21624631  2.17118121  1.14723861]
 [ 2.22874494 -0.17513313  0.8269183 ]
 [ 2.2211989   0.87213971 -0.19151045]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.22671228  0.93600592 -0.21794279]
 [-0.29609931  1.25285329  2.27596663]
 [-0.25476857  1.20746943  2.20485252]
 [-0.29672783  1.24461331  2.28083131]
 [-0.27578664  1.21663499  2.24864564]
 [-0.28091389  2.25930846  1.21661886]
 [-0.21369288  1.05233452  2.20512234]
 [-0.27669555  1.12529292  2.27023906]
 [-0.16942442  2.17056098  0.99533295]
 [ 2.24933086 -0.25468768  1.0709247 ]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.18638944  1.20994285 -0.24936796]
 [-0.22656825  2.23557826  0.92551338]
 [-0.27989264  1.21274017  2.25881762]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.28390119  1.23920595  2.25400509]
 [ 2.24837463  0.81114157 -0.20592544]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.22765797  1.07419821  2.21710769]
 [-0.18996302  2.19089984  0.99497945]
 [-0.27357394  1.19278157  2.25408746]
 [ 2.23355717  0.86019975 -0.2060317 ]
 [ 2.25277813 -0.21394322  0.80875361]
 [-0.18611572  1.10670475  2.14746524]
 [ 2.25454797  0.88341904 -0.24307373]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.23794605  0.91585392 -0.22774264]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.0914977   1.20089769 -0.21820392]
 [ 2.25962348  0.84878847 -0.24304703]
 [-0.25213485  1.16423702  2.22696973]
 [ 2.26725005  0.88232062 -0.25923379]
 [-0.14201734  2.14344591  0.99568721]
 [ 2.25731     0.95572321 -0.25455798]
 [-0.22656825  2.23557826  0.92551338]
 [-0.19708433  2.25161696  0.79328185]
 [ 2.23957622  0.81769302 -0.19137855]
 [ 2.21575566  1.0173258  -0.21798639]
 [ 1.02668315  2.21468275 -0.21824732]
 [ 2.27472592  0.77777882 -0.24294008]
 [-0.21624631  2.17118121  1.14723861]
 [-0.24730284  1.20252603  2.19004536]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.27273902  1.20332527  2.24866913]
 [-0.19455078  2.17814555  1.06749683]
 [-0.28027257  2.2623408   1.20447285]
 [-0.28054312  1.20372124  2.26304729]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.17896853 -0.12686338  0.8824238 ]
 [ 2.19820639  1.04471124 -0.20619077]
 [-0.26313706  2.23602532  1.18984329]
 [-0.25331913  2.21599142  1.18997806]
 [-0.28966527  1.23403227  2.27016072]
 [-0.23157808  2.22314802  1.06680048]
 [-0.26533811  1.22371567  2.21684157]
 [-0.25751543  1.18608093  2.22693265]
 [-0.27562627  2.24825903  1.21670804]
 [-0.27273902  1.20332527  2.24866913]
 [ 2.22671228  0.93600592 -0.21794279]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(clf, x)</span>:</span></span><br><span class="line">    iris_feature = <span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>, <span class="string">'petal lenght'</span>, <span class="string">'petal width'</span></span><br><span class="line">    <span class="comment"># 开始画图</span></span><br><span class="line">    x1_min, x1_max = x[:, <span class="number">0</span>].min(), x[:, <span class="number">0</span>].max()               <span class="comment">#第0列的范围</span></span><br><span class="line">    x2_min, x2_max = x[:, <span class="number">1</span>].min(), x[:, <span class="number">1</span>].max()               <span class="comment">#第1列的范围</span></span><br><span class="line">    x1, x2 = np.mgrid[x1_min:x1_max:<span class="number">200j</span>, x2_min:x2_max:<span class="number">200j</span>]   <span class="comment">#生成网格采样点</span></span><br><span class="line">    grid_test = np.stack((x1.flat, x2.flat), axis=<span class="number">1</span>)            <span class="comment">#stack():沿着新的轴加入一系列数组</span></span><br><span class="line">    print(<span class="string">'grid_test:\n'</span>, grid_test)</span><br><span class="line">    <span class="comment"># 输出样本到决策面的距离</span></span><br><span class="line">    z = clf.decision_function(grid_test)</span><br><span class="line">    print(<span class="string">'the distance to decision plane:\n'</span>, z)</span><br><span class="line">    </span><br><span class="line">    grid_hat = clf.predict(grid_test)                           <span class="comment"># 预测分类值 得到【0,0.。。。2,2,2】</span></span><br><span class="line">    print(<span class="string">'grid_hat:\n'</span>, grid_hat)  </span><br><span class="line">    grid_hat = grid_hat.reshape(x1.shape)                       <span class="comment"># reshape grid_hat和x1形状一致</span></span><br><span class="line">                                                                <span class="comment">#若3*3矩阵e，则e.shape()为3*3,表示3行3列   </span></span><br><span class="line"> </span><br><span class="line">    cm_light = mpl.colors.ListedColormap([<span class="string">'#A0FFA0'</span>, <span class="string">'#FFA0A0'</span>, <span class="string">'#A0A0FF'</span>])</span><br><span class="line">    cm_dark = mpl.colors.ListedColormap([<span class="string">'g'</span>, <span class="string">'b'</span>, <span class="string">'r'</span>])</span><br><span class="line"> </span><br><span class="line">    plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)                                   <span class="comment"># pcolormesh(x,y,z,cmap)这里参数代入</span></span><br><span class="line">                                                                                      <span class="comment"># x1，x2，grid_hat，cmap=cm_light绘制的是背景。</span></span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=np.squeeze(y), edgecolor=<span class="string">'k'</span>, s=<span class="number">50</span>, cmap=cm_dark) <span class="comment"># 样本点</span></span><br><span class="line">    plt.scatter(x_test[:, <span class="number">0</span>], x_test[:, <span class="number">1</span>], s=<span class="number">120</span>, facecolor=<span class="string">'none'</span>, zorder=<span class="number">10</span>)       <span class="comment"># 测试点</span></span><br><span class="line">    plt.xlabel(iris_feature[<span class="number">0</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(iris_feature[<span class="number">1</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.title(<span class="string">'svm in iris data classification'</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.模型使用</span></span><br><span class="line">draw(clf,x)</span><br></pre></td></tr></table></figure>

<pre><code>grid_test:
 [[4.3       2.       ]
 [4.3       2.0120603]
 [4.3       2.0241206]
 ...
 [7.9       4.3758794]
 [7.9       4.3879397]
 [7.9       4.4      ]]
the distance to decision plane:
 [[ 2.17689921  1.23467171 -0.25941323]
 [ 2.17943684  1.23363096 -0.25941107]
 [ 2.18189345  1.23256802 -0.25940892]
 ...
 [-0.27958977  0.83621535  2.28683228]
 [-0.27928358  0.8332275   2.28683314]
 [-0.27897389  0.83034313  2.28683399]]
grid_hat:
 [0. 0. 0. ... 2. 2. 2.]


/Users/taoxuefeng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: MatplotlibDeprecationWarning: shading=&apos;flat&apos; when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading=&apos;auto&apos;, &apos;nearest&apos; or &apos;gouraud&apos;, or set rcParams[&apos;pcolor.shading&apos;].  This will become an error two minor releases later.</code></pre><p><img src="/2020/07/24/ML-flower/tRZPjJlhgFT5YAV.png" alt="tRZPjJlhgFT5YAV"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>鸢尾花分类</tag>
        <tag>机器学习</tag>
        <tag>Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce小文件优化问题</title>
    <url>/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>默认情况下，TextInputformat对任务的切片机智是按文件规划切片，不管文件多小，都会是一个单独的切片，都是交给一个maptask，如果有多个小文件，就会产生大量的maptask，处理效率底下。</p>
<a id="more"></a>

<p>解决办法：</p>
<pre><code>1.从源头上解决，将文件合并后再上传到HDFS处理。
2.如果小文件已经在HDFS中，可以先写一个MapReduce程序对小文件合并
3.可以用另一种InputFormat：CombineInputFormat（它可以将多个文件划分到一个切片中），这样就可以交给一个maptask处理。</code></pre><p> 使用默认的InputFormat</p>
<p><img src="/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3l1bmZhbjY=,size_16,color_FFFFFF,t_70.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3l1bmZhbjY=,size_16,color_FFFFFF,t_70"></p>
<p> 使用CombineTextInputFormat</p>
<pre><code>job.setInputFormatClass(CombineTextInputFormat.class);
CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);
CombineTextInputFormat.setMinInputSplitSize(job, 2097152);</code></pre><p><img src="/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/2019080810591773.png" alt="2019080810591773"></p>
]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase学习笔记-1</title>
    <url>/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一-Hadoop的局限性"><a href="#一-Hadoop的局限性" class="headerlink" title="一.Hadoop的局限性"></a>一.Hadoop的局限性</h2><p>Hbase是一个构建在Hadoop文件系统之上的面向列(列族)的数据库管理系统</p>
<img src="/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/image-20200804194929543.png" alt="image-20200804194929543" style="zoom:50%;">

<a id="more"></a>

<p>Hadoop存在的限制:</p>
<p>Hadoop可以通过HDFS来存储结构化,半结构化甚至非结构化的数据,是传统数据库的补充,是海量数据存储的最佳方法,针对大文件的存储,批量访问和流式访问都做了优化,同时也通过多副本解决容灾问题,使得数据更加安全.</p>
<p>但是Hadoop的缺陷在于它只能执行批处理,并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。</p>
<blockquote>
<p>  注：数据结构分类：</p>
<ul>
<li>结构化数据：即以关系型数据库表形式管理的数据；</li>
<li>半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等；</li>
<li>非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。</li>
</ul>
</blockquote>
<h2 id="二-Hbase简介"><a href="#二-Hbase简介" class="headerlink" title="二. Hbase简介"></a>二. Hbase简介</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<p>HBase 是一种类似于 <code>Google’s Big Table</code> 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性：</p>
<ul>
<li>不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的；</li>
<li>由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储；</li>
<li>支持通过增加机器进行横向扩展；</li>
<li>支持数据分片；</li>
<li>支持 RegionServers 之间的自动故障转移；</li>
<li>易于使用的 Java 客户端 API；</li>
<li>支持 BlockCache 和布隆过滤器；</li>
<li>过滤器支持谓词下推。</li>
</ul>
<h2 id="三-Hbase-Table"><a href="#三-Hbase-Table" class="headerlink" title="三. Hbase Table"></a>三. Hbase Table</h2><p>HBase 是一个面向 <code>列</code> 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 <code>列族</code> 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。</p>
<p>下图为 HBase 中一张表的：</p>
<ul>
<li>RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序；</li>
<li>该表具有两个列族，分别是 personal 和 office;</li>
<li>其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。</li>
</ul>
<img src="/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/image-20200807172505289.png" alt="image-20200807172505289" style="zoom:50%;">

<p>Hbase 的表具有以下特点：</p>
<ul>
<li>容量大：一个表可以有数十亿行，上百万列；</li>
<li>面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担；</li>
<li>稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏 ；</li>
<li>数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面；</li>
<li>存储类型：所有数据的底层存储格式都是字节数组 (byte[])。</li>
</ul>
<h2 id="四-Phoenix"><a href="#四-Phoenix" class="headerlink" title="四. Phoenix"></a>四. Phoenix</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>github.com/heibaiying/BigData-Notes</p>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>实时数仓</title>
    <url>/2020/08/11/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="0x00-数仓为什么要实时"><a href="#0x00-数仓为什么要实时" class="headerlink" title="|0x00 数仓为什么要实时"></a>|0x00 数仓为什么要实时</h3><p>去年开始，<font color="red">实时数仓</font> 的概念突然火了。也许是<strong>传统的离线数仓搞了很多年</strong>，技术相对成熟了，因此大家都把注意力放到了<strong>挑战性更高</strong>的实时上来；也许是随着<strong>存量市场竞争</strong>的到来，对于速度的要求越来越快，<strong>T+1已经不能满足数据的获取要求了</strong>，实时的构建需求也就应运而生了。 </p>
<p><font color="red">总之，时效性开始大于分析性</font> 。</p>
<p>文本简单介绍实时数仓的一些基础理论，更系统性的理论，仍然行业需要更大范围的应用和总结。总之，这是一块<strong>有前景</strong>的<strong>新领域</strong>，值得探索。</p>
<a id="more"></a>

<h3 id="0x01-实时数仓的技术要求"><a href="#0x01-实时数仓的技术要求" class="headerlink" title="|0x01 实时数仓的技术要求"></a>|0x01 实时数仓的技术要求</h3><ol>
<li><font color="red">高并发性</font>：未来的实时数据一定不是仅仅给几个运营或管理层人员使用，会更多的面向商户、用户，那么用户增加的情况下一定会带来并发量的提升，因此实时数仓一定要具备<strong>提供高并发数据服务的能力</strong>。</li>
<li><font color="red">查询速度</font>：目前很多实时指标的应用场景在移动端，移动端对于数据的响应要求远高于PC端，对于大多数数据使用场景希望能够毫秒级返回，未来实时标签如果应用到用户推荐上，<strong>对于响应的速度要求更高</strong>。</li>
<li><font color="red">处理速度</font>：要保证大促期间，在流量峰值的情况下有极强的处理能力，并且<strong>具备消费低延迟性甚至0延迟</strong>。</li>
</ol>
<h3 id="0x02-实时数仓的技术基础：流式技术架构"><a href="#0x02-实时数仓的技术基础：流式技术架构" class="headerlink" title="|0x02 实时数仓的技术基础：流式技术架构"></a>|0x02 实时数仓的技术基础：流式技术架构</h3><p>目前<font color="red">流式计算框架相对成熟</font>，以<strong>Storm、Spark Streaming、Flink</strong>为代表的开源组件也被广泛应用。流式数据处理，简单来讲，就是<strong>系统每产生一条数据，都会被立刻采集并发送到流式任务中心进行处理，不需要额外的定时调度来执行任务</strong>。目前在业界比较广泛采用的框架有Twitter的Storm、Apache的Spark Streaming，以及最近几年流行的Flink。它们整体架构大同小异，但在实现细节上有诸多的不同，需要根据业务场景的特征来灵活选择框架。</p>
<p>流式框架具备以下优点：</p>
<ol>
<li><font color="red">时效性高</font>：延迟粒度通常在<strong>秒级</strong>；</li>
<li><font color="red">任务常驻</font>：流式任务一旦启动，就会一直运行，直到人为终止，且数据源是<strong>无界</strong>的；</li>
<li><font color="red">处理性能高</font>：流式计算通常会采用<strong>较好的服务器</strong>来运行任务，因为一旦处理吞吐量赶不上采集吞吐量，就会出现数据计算延迟；</li>
<li><font color="red">逻辑简单</font>：由于流式计算通常是针对单条数据做处理，缺乏数据之间的关联运算能力，所以在支持的<strong>业务逻辑上相对简单</strong>，并且处理结果会与离线存在一定的差异。</li>
</ol>
<h3 id="0x03-实时数仓的两个常见架构"><a href="#0x03-实时数仓的两个常见架构" class="headerlink" title="|0x03 实时数仓的两个常见架构"></a>|0x03 实时数仓的两个常见架构</h3><p><font color="red">Lambda架构</font>：Lambda架构的核心理念是“<font color="red">流批一体化</font>”，因为随着机器性能和数据框架的不断完善，用户其实不关心底层是如何运行的，批处理也好，流式处理也罢，<strong>能按照统一的模型返回结果就可以了</strong>，这就是Lambda架构诞生的原因。现在很多应用，例如Spark和Flink，都支持这种结构，也就是数据进入平台后，可以选择批处理运行，也可以选择流式处理运行，但不管怎样，<strong>一致性都是相同的</strong>。</p>
<p><font color="red">Kappa架构</font>：Lambda架构虽然理念很好，但用多了会有一个问题：<strong>数据复杂性大大增加</strong>。为了解决复杂性的问题，有人提出了用一套架构解决所有问题的设想，而流行的做法就是基于流计算来做。通过<font color="red">加大流计算的“时间窗口”</font>，来实现逻辑意义上的批处理操作。</p>
<h3 id="0x04-实时数仓的查询引擎"><a href="#0x04-实时数仓的查询引擎" class="headerlink" title="|0x04 实时数仓的查询引擎"></a>|0x04 实时数仓的查询引擎</h3><p>实时数仓的查询依赖于<font color="red">交互式查询引擎</font>，常见于OLAP场景，根据存储数据方式的不同可以分为<strong>ROLAP、MOLAP和HOLAP</strong>：</p>
<p><font color="red">ROLAP</font>：在大数据生态圈中，主流的应用于ROLAP场景的交互式计算引擎包括<strong>Impala和Presto</strong>。基于关系数据库OLAP实现（Relational OLAP），它以关系数据库为核心，以<strong>关系型结构</strong>进行多维数据的表示和存储。它将多维结构划分成两类表：一类是<strong>事实表</strong>，迎来存储数据和维度关键字；另一类是<strong>维度表</strong>，即对每个维度至少使用一个表来存放维度层次、成员类别等维度描述信息。ROLAP的最大好处是可以实时地从源数据中获得最新数据更新，以保持数据实时性，缺点在于运算效率比较低，用户等待时间比较长。</p>
<p><font color="red">MOLAP</font>：MOLAP是一种通过<font color="red">预计算Cube方式</font>加速查询的OLAP引擎，它的核心思想是“<font color="red">空间换时间</font>”，典型的代表包括<strong>Druid和Kylin</strong>。基于多维数据组织的OLAP实现（Multidimensional OLAP），它以多维数据组织方式为核心，使用多维数组存储数据。多维数据在存储系统中形成“<strong>数据立方体（Cube）</strong>”结构，此结构是高度优化的，可以最大程度提高查询性能。MOLAP的优势在于借助数据多位预处理显著提高运算效率，主要缺陷在于占用存储空间大和数据更新有一定延迟。</p>
<p><font color="red">HOLAP</font>：基于混合数据组织的OLAP实现（Hybrid OLAP），用户可以根据自己的业务需求，选择哪些模型采用ROLAP、哪些采用MOLAP。一般来说，<strong>将不常用或需要灵活定义的分析使用ROLAP，而常用、常规模型采用MOLAP实现</strong>。</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>描述</strong></th>
<th><strong>细节数据存储位置</strong></th>
<th><strong>聚合后的数据存储位置</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ROLAP(Relational OLAP)</td>
<td>基于关系数据库的OLAP实现</td>
<td>关系型数据库</td>
<td>关系型数据库</td>
</tr>
<tr>
<td>MOLAP(Multidimensional OLAP)</td>
<td>基于多维数据组织的OLAP实现</td>
<td>数据立方体</td>
<td>数据立方体</td>
</tr>
<tr>
<td>HOLAP(Hybrid OLAP)</td>
<td>基于混合数据组织的OLAP实现</td>
<td>关系型数据库</td>
<td>数据立方体</td>
</tr>
</tbody></table>
<h3 id="0x05-实时数仓的分层模型"><a href="#0x05-实时数仓的分层模型" class="headerlink" title="|0x05 实时数仓的分层模型"></a>|0x05 实时数仓的分层模型</h3><p>实时数仓的分层思路<strong>沿用</strong>了离线的思想。</p>
<ul>
<li><font color="red">CDM层（明细数据层)</font>: CDM层根据业务场景的不同，分为<strong>各个主题域</strong>。</li>
<li><font color="red">DWS层（汇总数据层)</font>：DWS层对各个域进行了<strong>适度汇总</strong>。</li>
<li><font color="red">ADS层 （应用数据层)</font>：ADS层并不完全根据需求一对一建设，而是结合不同的需求对这一层进行统一设计，以快速支撑<strong>更多的需求场景</strong>。</li>
</ul>
<h3 id="0x06-实时技术中的幂等机制"><a href="#0x06-实时技术中的幂等机制" class="headerlink" title="|0x06 实时技术中的幂等机制"></a>|0x06 实时技术中的幂等机制</h3><p><font color="red">幂等</font>是一个数学概念，<strong>特点是任意多次执行所产生的影响均与一次执行的影响相同</strong>，例如setTrue()函数就是一个幂等函数,无论多次执行，其结果都是一样的。在很多复杂情况下，例如网络波动、Storm重启等，会出现重复数据，因此<strong>并不是所有操作都是幂等的</strong>。在幂等的概念下，我们需要了解消息传输保障的三种机制：At most once、At least once和Exactly once。</p>
<ul>
<li><font color="red">At most once</font>：消息传输机制是每条消息传输<strong>零次或者一次</strong>，即消息可能会丢失；</li>
<li><font color="red">At least once</font>：意味着每条消息会进行<strong>多次传输尝试</strong>，至少一次成功，即消息传输可能重复但不会丢失；</li>
<li><font color="red">Exactly once</font>：消息传输机制是<strong>每条消息有且只有一次</strong>，即消息传输既不会丢失也不会重复。</li>
</ul>
<h3 id="0x07-实时数仓中的多表关联"><a href="#0x07-实时数仓中的多表关联" class="headerlink" title="|0x07 实时数仓中的多表关联"></a>|0x07 实时数仓中的多表关联</h3><p>在流式数据处理中，数据的计算是以<font color="red">计算增量</font>为基础的，所以各个环节到达的时间和顺序，<strong>既是不确定的，也是无序的</strong>。在这种情况下，如果两表要关联，势必需要将数据在内存中进行存储，当一条数据到达后，需要去另一个表中查找数据，如果能够查到，则关联成功，写入下游；如果无法查到，可以被分到未分配的数据集合中进行等待。在实际处理中，考虑到数据查找的性能，会把数据按照关联主键进行<font color="red">分桶处理</font>，以<strong>降低查找的数据量，提高性能</strong>。</p>
<h3 id="0x08-实时技术中的洪峰挑战"><a href="#0x08-实时技术中的洪峰挑战" class="headerlink" title="|0x08 实时技术中的洪峰挑战"></a>|0x08 实时技术中的洪峰挑战</h3><p>主要解决思路有如下几种：</p>
<ol>
<li><font color="red">独占资源与共享资源合理分配</font>：在一台机器中，<strong>共享资源池可以被多个实时任务抢占</strong>，如果一个任务80%的时间都需要抢资源，可以考虑分配更多的独占资源；</li>
<li><font color="red">合理设置缓存机制</font>：虽然内存的读写性能是最好的，但很多数据依然需要读库更新，可以考虑将<strong>热门数据尽量多的留在内存中</strong>，通过异步的方式来更新缓存；</li>
<li><font color="red">计算合并单元</font>：在流式计算框架中，拓扑结构的层级越深，性能越差，考虑<strong>合并计算单元</strong>，可以有效降低数据的传输、序列化等时间；</li>
<li><font color="red">内存共享</font>：在海量数据的处理中，大部分的对象都是以字符串形式存在的，在<strong>不同线程间合理共享对象</strong>，可以大幅度降低字符拷贝带来的性能消耗；</li>
<li><font color="red">在高吞吐与低延迟间寻求平衡</font>：高吞吐与低延迟天生就是一对矛盾体，将<strong>多个读写库的操作或者ACK操作合并</strong>时，可以有效降低数据吞吐，但也会增加延迟，可以进行业务上的取舍。</li>
</ol>
<h3 id="0xFF-总结"><a href="#0xFF-总结" class="headerlink" title="|0xFF 总结"></a>|0xFF 总结</h3><p>在整个实时数仓的建设中，业界已经有了常用的方案选型。整体架构设计<strong>通过分层设计为OLAP查询分担压力，让出计算空间，复杂的计算统一在实时计算层处理掉，避免给OLAP查询带来过大的压力</strong>。汇总计算交给OLAP数据库进行。因此，在整个架构中实时计算一般是<font color="red">Spark+Flink</font>配合，<font color="red">消息队列Kafka</font>一家独大，在整个大数据领域消息队列的应用中仍然处于垄断地位，Hbase、Redis和MySQL都在特定场景下有一席之地。</p>
<p>目前还<font color="red">没有一个</font>OLAP系统能够满足各种场景的查询需求。其本质原因是，<font color="red">没有一个系统能同时在数据量、性能、和灵活性三个方面做到完美</font>，每个系统在设计时都需要在这三者间做出取舍。</p>
<p><strong>从长远看，Spark和Flink更有可能成为下一代的Hadoop，值得投入大量时间学习</strong>。</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
      <tags>
        <tag>数仓</tag>
        <tag>实时</tag>
      </tags>
  </entry>
  <entry>
    <title>学习知乎实时数仓实践及架构演进</title>
    <url>/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>“数据智能” (Data Intelligence) 有一个必须且基础的环节，就是数据仓库的建设，同时，数据仓库也是公司数据发展到一定规模后必然会提供的一种基础服务。从智能商业的角度来讲，数据的结果代表了用户的反馈，获取结果的及时性就显得尤为重要，快速的获取数据反馈能够帮助公司更快的做出决策，更好的进行产品迭代，实时数仓在这一过程中起到了不可替代的作用。</p>
<p>本文主要讲述知乎的实时数仓实践以及架构的演进，这包括以下几个方面</p>
<ol>
<li><p>实时数仓 1.0 版本，主题： ETL 逻辑实时化，技术方案：Spark Streaming。</p>
</li>
<li><p>实时数仓 2.0 版本，主题：数据分层，指标计算实时化，技术方案：Flink Streaming。</p>
</li>
<li><p>实时数仓未来展望：Streaming SQL 平台化，元信息管理系统化，结果验收自动化。</p>
<a id="more"></a>

</li>
</ol>
<h1 id="实时数仓架构"><a href="#实时数仓架构" class="headerlink" title="实时数仓架构"></a>实时数仓架构</h1><h3 id="实时数仓-1-0-版本"><a href="#实时数仓-1-0-版本" class="headerlink" title="实时数仓 1.0 版本"></a>实时数仓 1.0 版本</h3><p>1.0 版本的实时数仓主要是对流量数据做实时 ETL，并不计算实时指标，也未建立起实时数仓体系，实时场景比较单一，对实时数据流的处理主要是为了提升数据平台的服务能力。实时数据的处理向上依赖数据的收集，向下关系到数据的查询和可视化，下图是实时数仓 1.0 版本的整体数据架构图。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-630f5d70d2cc1c85be4ea020f99c0554_1440w.jpg" alt="v2-630f5d70d2cc1c85be4ea020f99c0554_1440w"></p>
<p>第一部分是数据采集，由三端 SDK 采集数据并通过 Log Collector Server 发送到 Kafka。第二部分是数据 ETL，主要完成对原始数据的清洗和加工并分实时和离线导入 Druid。第三部分是数据可视化，由 Druid 负责计算指标并通过 Web Server 配合前端完成数据可视化。</p>
<p>其中第一、三部分的相关内容请分别参考：<a href="https://zhuanlan.zhihu.com/p/40985361" target="_blank" rel="noopener">知乎客户端埋点流程、模型和平台技术</a>，<a href="https://zhuanlan.zhihu.com/p/48046671" target="_blank" rel="noopener">Druid 与知乎数据分析平台</a>，此处我们详细介绍第二部分。由于实时数据流的稳定性不如离线数据流，当实时流出现问题后需要离线数据重刷历史数据，因此实时处理部分我们采用了 lambda 架构。</p>
<p>Lambda 架构有高容错、低延时和可扩展的特点，为了实现这一设计，我们将 ETL 工作分为两部分：Streaming ETL 和 Batch ETL。</p>
<h3 id="Streaming-ETL"><a href="#Streaming-ETL" class="headerlink" title="Streaming ETL"></a>Streaming ETL</h3><p>这一部分我会介绍实时计算框架的选择、数据正确性的保证、以及 Streaming 中一些通用的 ETL 逻辑，最后还会介绍 Spark Streaming 在实时 ETL 中的稳定性实践。</p>
<h3 id="计算框架选择"><a href="#计算框架选择" class="headerlink" title="计算框架选择"></a>计算框架选择</h3><p>在 2016 年年初，业界用的比较多的实时计算框架有 Storm 和 Spark Streaming。Storm 是纯流式框架，Spark Streaming 用 Micro Batch 模拟流式计算，前者比后者更实时，后者比前者吞吐量大且生态系统更完善，考虑到知乎的日志量以及初期对实时性的要求，我们选择了 Spark Streaming 作为实时数据的处理框架。</p>
<h3 id="数据正确性保证"><a href="#数据正确性保证" class="headerlink" title="数据正确性保证"></a>数据正确性保证</h3><p>Spark Streaming 的端到端 Exactly-once 需要下游支持幂等、上游支持流量重放，这里我们在 Spark Streaming 这一层做到了 At-least-once，正常情况下数据不重不少，但在程序重启时可能会重发部分数据，为了实现全局的 Exactly-once，我们在下游做了去重逻辑，关于如何去重后面我会讲到。</p>
<h3 id="通用-ETL-逻辑"><a href="#通用-ETL-逻辑" class="headerlink" title="通用 ETL 逻辑"></a>通用 ETL 逻辑</h3><p>ETL 逻辑和埋点的数据结构息息相关，我们所有的埋点共用同一套 Proto Buffer Schema，大致如下所示。</p>
<figure class="highlight protobuf"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">LogEntry</span> </span>&#123;</span><br><span class="line">  <span class="keyword">optional</span> BaseInfo base = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">optional</span> DetailInfo detail = <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">optional</span> ExtraInfo extra = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>BaseInfo：日志中最基本的信息，包括用户信息、客户端信息、时间信息、网络信息等日志发送时的必要信息。DetailInfo：日志中的视图信息，包括当前视图、上一个视图等用于定位用户所在位置的信息。ExtraInfo：日志中与特定业务相关的额外信息。</p>
<p>针对上述三种信息我们将 ETL 逻辑分为通用和非通用两类，通用逻辑和各个业务相关，主要应用于 Base 和 Detail 信息，非通用逻辑则是由需求方针对某次需求提出，主要应用于 Extra 信息。这里我们列举 3 个通用逻辑进行介绍，这包括：动态配置 Streaming、UTM 参数解析、新老用户识别。</p>
<h3 id="动态配置-Streaming"><a href="#动态配置-Streaming" class="headerlink" title="动态配置 Streaming"></a>动态配置 Streaming</h3><p>由于 Streaming 任务需要 7 * 24 小时运行，但有些业务逻辑，比如：存在一个元数据信息中心，当这个元数据发生变化时，需要将这种变化映射到数据流上方便下游使用数据，这种变化可能需要停止 Streaming 任务以更新业务逻辑，但元数据变化的频率非常高，且在元数据变化后如何及时通知程序的维护者也很难。动态配置 Streaming 为我们提供了一个解决方案，该方案如下图所示。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-af21d0b60e548a1b56317ab42fa5d083_1440w.jpg" alt="img"></p>
<p>我们可以把经常变化的元数据作为 Streaming Broadcast 变量，该变量扮演的角色类似于只读缓存，同时针对该变量可设置 TTL，缓存过期后 Executor 节点会重新向 Driver 请求最新的变量。通过这种机制可以非常自然的将元数据的变化映射到数据流上，无需重启任务也无需通知程序的维护者。</p>
<h3 id="UTM-参数解析"><a href="#UTM-参数解析" class="headerlink" title="UTM 参数解析"></a>UTM 参数解析</h3><p>UTM 的全称是 Urchin Tracking Module，是用于追踪网站流量来源的利器，关于 UTM 背景知识介绍可以参考网上其他内容，这里不再赘述。下图是我们解析 UTM 信息的完整逻辑。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-c988423536bd9ca342ee5aa21688d7e3_1440w.jpg" alt="img"></p>
<p>流量数据通过 UTM 参数解析后，我们可以很容易满足以下需求</p>
<ol>
<li>查看各搜索引擎导流情况以及这些流量来自于哪些热门搜索词。</li>
<li>市场部某次活动带来的流量大小，如：页面浏览数、独立访问用户数等。</li>
<li>从站内分享出去的链接在各分享平台（如：微信、微博）被浏览的情况。</li>
</ol>
<h3 id="新老用户识别"><a href="#新老用户识别" class="headerlink" title="新老用户识别"></a>新老用户识别</h3><p>对于互联网公司而言，增长是一个永恒的话题，实时拿到新增用户量，对于增长运营十分重要。例如：一次投放 n 个渠道，如果能拿到每个渠道的实时新增用户数，就可以快速判断出那些渠道更有价值。我们用下图来表达 Streaming ETL 中是如何识别新老用户的。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-e073c2397c34f614cc26fac279f2863a_1440w.jpg" alt="img"></p>
<p>判断一个用户是不是新用户，最简单的办法就是维护一个历史用户池，对每条日志判断该用户是否存在于用户池中。 由于日志量巨大，为了不影响 Streaming 任务的处理速度，我们设计了两层缓存：Thread Local Cache 和 Redis Cache，同时用 HBase 做持久化存储以保存历史用户。访问速度：本地内存 &gt; 远端内存 &gt; 远端磁盘，对于我们这个任务来说，只有 1% 左右的请求会打到 HBase，日志高峰期 26w/s，完全不会影响任务的实时性。当然本地缓存 LruCache 的容量大小和 Redis 的性能也是影响实时性的两个因素。</p>
<p>Streaming ETL 除了上述几个通用场景外，还有一些其他逻辑，这些逻辑的存在有的是为了满足下游更方便的使用数据的需求，有的是对某些错误埋点的修复，总之 Streaming ETL 在整个实时数仓中处于指标计算的上游，有着不可替代的作用。</p>
<h3 id="Spark-Streaming-稳定性实践"><a href="#Spark-Streaming-稳定性实践" class="headerlink" title="Spark Streaming 稳定性实践"></a>Spark Streaming 稳定性实践</h3><h4 id="在实时数仓-1-0-中的"><a href="#在实时数仓-1-0-中的" class="headerlink" title="在实时数仓 1.0 中的"></a>在实时数仓 1.0 中的</h4><ol>
<li>Spark Streaming 消费 Kafka 数据推荐使用 Direct 模式。我们早期使用的是 High Level 或者叫 Receiver 模式并使用了 checkpoint 功能，这种方式在更新程序逻辑时需要删除 checkpoint 否则新的程序逻辑就无法生效。另外，由于使用了 checkpoint 功能，Streaming 任务会保持和 Hdfs 通信，可能会因为 NameNode 的抖动导致 Streaming 任务抖动。因此，推荐使用 Direct 模式，关于这种模式和 Receiver 模式的详细对比，可以参考官方文档。</li>
<li>保证 Spark Streaming 任务的资源稳定。以 Yarn 为例，运行 Streaming 任务的队列能够分配到的最小资源小于了任务所需要的资源，任务会出现频繁丢失 Executor 的情况，这会导致 Streaming 任务变慢，因为丢失的 Executor 所对应的数据需要重新计算，同时还需要重新分配 Executor。</li>
<li>Spark Streaming 消费 Kafka 时需要做数据流限速。默认情况下 Spark Streaming 以尽可能大的速度读取消息队列，当 Streaming 任务挂了很久之后再次被启动时，由于拉取的数据量过大可能会导致上游的 Kafka 集群 IO 被打爆进而出现 Kafka 集群长时间阻塞。可以使用 Streaming Conf 参数做限速，限定每秒拉取的最大速度。</li>
<li>Spark Streaming 任务失败后需要自动拉起。长时间运行发现，Spark Streaming 并不能 7 * 24h 稳定运行，我们用 Supervisor 管理 Driver 进程，当任务挂掉后 Driver 进程将不复存在，此时 Supervisor 将重新拉起 Streaming 任务。</li>
</ol>
<h3 id="Batch-ETL"><a href="#Batch-ETL" class="headerlink" title="Batch ETL"></a>Batch ETL</h3><p>接下来要介绍的是 Lambda 架构的第二个部分：Batch ETL，此部分我们需要解决数据落地、离线 ETL、数据批量导入 Druid 等问题。针对数据落地我们自研了 map reduce 任务 Batch Loader，针对数据修复我们自研了离线任务 Repair ETL，离线修复逻辑和实时逻辑共用一套 ETL Lib，针对批量导入 ProtoParquet 数据到 Druid，我们扩展了 Druid 的导入插件。</p>
<h3 id="Repair-ETL"><a href="#Repair-ETL" class="headerlink" title="Repair ETL"></a>Repair ETL</h3><p>数据架构图中有两个 Kafka，第一个 Kafka 存放的是原始日志，第二个 Kafka 存放的是实时 ETL 后的日志，我们将两个 Kafka 的数据全部落地，这样做的目的是为了保证数据链路的稳定性。因为实时 ETL 中有大量的业务逻辑，未知需求的逻辑也许会给整个流量数据带来安全隐患，而上游的 Log Collect Server 不存在任何业务逻辑只负责收发日志，相比之下第一个 Kafka 的数据要安全和稳定的多。Repair ETL 并不是经常启用，只有当实时 ETL 丢失数据或者出现逻辑错误时，才会启用该程序用于修复日志。</p>
<h3 id="Batch-Load-2-HDFS"><a href="#Batch-Load-2-HDFS" class="headerlink" title="Batch Load 2 HDFS"></a>Batch Load 2 HDFS</h3><p>前面已经介绍过，我们所有的埋点共用同一套 Proto Buffer Schema，数据传输格式全部为二进制。我们自研了落地 Kafka PB 数据到 Hdfs 的 Map Reduce 任务 BatchLoader，该任务除了落地数据外，还负责对数据去重。在 Streaming ETL 阶段我们做到了 At-least-once，通过此处的BatchLoader 去重我们实现了全局 Exactly-once。BatchLoader 除了支持落地数据、对数据去重外，还支持多目录分区（p_date/p_hour/p_plaform/p_logtype）、数据回放、自依赖管理（早期没有统一的调度器）等。截止到目前，BatchLoader 落地了 40+ 的 Kakfa Topic 数据。</p>
<h3 id="Batch-Load-2-Druid"><a href="#Batch-Load-2-Druid" class="headerlink" title="Batch Load 2 Druid"></a>Batch Load 2 Druid</h3><p>采用 Tranquility 实时导入 Druid，这种方式强制需要一个时间窗口，当上游数据延迟超过窗值后会丢弃窗口之外的数据，这种情况会导致实时报表出现指标错误。为了修复这种错误，我们通过 Druid 发起一个离线 Map Reduce 任务定期重导上一个时间段的数据。通过这里的 Batch 导入和前面的实时导入，实现了实时数仓的 Lambda 架构。</p>
<h3 id="实时数仓-1-0-的不足之处"><a href="#实时数仓-1-0-的不足之处" class="headerlink" title="实时数仓 1.0 的不足之处"></a>实时数仓 1.0 的不足之处</h3><p>到目前为止我们已经介绍完 Lambda 架构实时数仓的几个模块，1.0 版本的实时数仓有以下几个不足</p>
<ol>
<li>所有的流量数据存放在同一个 Kafka Topic 中，如果下游每个业务线都要消费，这会导致全量数据被消费多次，Kafka 出流量太高无法满足该需求。</li>
<li>所有的指标计算全部由 Druid 承担，Druid 同时兼顾实时数据源和离线数据源的查询，随着数据量的暴涨 Druid 稳定性急剧下降，这导致各个业务的核心报表不能稳定产出。</li>
<li>由于每个业务使用同一个流量数据源配置报表，导致查询效率低下，同时无法对业务做数据隔离和成本计算。</li>
</ol>
<h3 id="实时数仓-2-0-版本"><a href="#实时数仓-2-0-版本" class="headerlink" title="实时数仓 2.0 版本"></a>实时数仓 2.0 版本</h3><p>随着数据量的暴涨，Druid 中的流量数据源经常查询超时同时各业务消费实时数据的需求也开始增多，如果继续沿用实时数仓 1.0 架构，需要付出大量的额外成本。于是，在实时数仓 1.0 的基础上，我们建立起了实时数仓 2.0，梳理出了新的架构设计并开始着手建立实时数仓体系，新的架构如下图所示。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-8fa67c34aaa02587dc97db2c95f6b240_1440w.jpg" alt="img"></p>
<h3 id="原始层"><a href="#原始层" class="headerlink" title="原始层"></a>原始层</h3><p>实时数仓 1.0 我们只对流量数据做 ETL 处理，在 2.0 版本中我们加入了对业务库的变更日志 Binlog 的处理，Binlog 日志在原始层为库级别或者 Mysql 实例级别，即：一个库或者实例的变更日志存放在同一个 Kafka Topic 中。同时随着公司业务的发展不断有新 App 产生，在原始层不仅采集「知乎」日志，像知乎极速版以及内部孵化项目的埋点数据也需要采集，不同 App 的埋点数据仍然使用同一套 PB Schema。</p>
<h3 id="明细层"><a href="#明细层" class="headerlink" title="明细层"></a>明细层</h3><p>明细层是我们的 ETL 层，这一层数据是由原始层经过 Streaming ETL 后得到。其中对 Binlog 日志的处理主要是完成库或者实例日志到表日志的拆分，对流量日志主要是做一些通用 ETL 处理，由于我们使用的是同一套 PB 结构，对不同 App 数据处理的逻辑代码可以完全复用，这大大降低了我们的开发成本。</p>
<h3 id="汇总层之明细汇总"><a href="#汇总层之明细汇总" class="headerlink" title="汇总层之明细汇总"></a>汇总层之明细汇总</h3><p>明细汇总层是由明细层通过 ETL 得到，主要以宽表形式存在。业务明细汇总是由业务事实明细表和维度表 Join 得到，流量明细汇总是由流量日志按业务线拆分和流量维度 Join 得到。流量按业务拆分后可以满足各业务实时消费的需求，我们在流量拆分这一块做到了自动化，下图演示了流量数据自动切分的过程。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-6deaffa4c25cf1dbb356931626b6f444_1440w.jpg" alt="img"></p>
<p>Streaming Proxy 是流量分发模块，它消费上游 ETL 后的全量数据并定期读取埋点元信息，通过将流量数据与元信息数据进行「Join」完成按业务进行流量拆分的逻辑，同时也会对切分后的流量按业务做 ETL 处理。 只要埋点元信息中新增一个埋点，那么这个埋点对应的数据就会自动切分到该业务的 Kafka 中，最终业务 Kafka 中的数据是独属于当前业务的且已经被通用 ETL 和业务 ETL 处理过，这大大降低了各个业务使用数据的成本。</p>
<h3 id="汇总层之指标汇总"><a href="#汇总层之指标汇总" class="headerlink" title="汇总层之指标汇总"></a>汇总层之指标汇总</h3><p>指标汇总层是由明细层或者明细汇总层通过聚合计算得到，这一层产出了绝大部分的实时数仓指标，这也是与实时数仓 1.0 最大的区别。知乎是一个生产内容的平台，对业务指标的汇总我们可以从内容角度和用户角度进行汇总，从内容角度我们可以实时统计内容（内容可以是答案、问题、文章、视频、想法）的被点赞数、被关注数、被收藏数等指标，从用户角度我可以实时统计用户的粉丝数、回答数、提问数等指标。对流量指标的汇总我们分为各业务指标汇总和全局指标汇总。对各业务指标汇总，我们可以实时统计首页、搜索、视频、想法等业务的卡片曝光数、卡片点击数、CTR 等，对全局指标汇总我们主要以实时会话为主，实时统计一个会话内的 PV 数、卡片曝光数、点击数、浏览深度、会话时长等指标。</p>
<h3 id="指标汇总层的存储选型"><a href="#指标汇总层的存储选型" class="headerlink" title="指标汇总层的存储选型"></a>指标汇总层的存储选型</h3><p>不同于明细层和明细汇总层，指标汇总层需要将实时计算好的指标存储起来以供应用层使用。我们根据不同的场景选用了 HBase 和 Redis 作为实时指标的存储引擎。Redis 的场景主要是满足带 Update 操作且 OPS 较高的需求，例如：实时统计全站所有内容（问题、答案、文章等）的累计 PV 数，由于浏览内容产生大量的 PV 日志，可能高达几万或者几十万每秒，需要对每一条内容的 PV 进行实时累加，这种场景下选用 Redis 更为合适。HBase 的场景主要是满足高频 Append 操作、低频随机读取且指标列较多的需求，例如：每分钟统计一次所有内容的被点赞数、被关注数、被收藏数等指标，将每分钟聚合后的结果行 Append 到 HBase 并不会带来性能和存储量的问题，但这种情况下 Redis 在存储量上可能会出现瓶颈。</p>
<h3 id="指标计算打通"><a href="#指标计算打通" class="headerlink" title="指标计算打通"></a>指标计算打通</h3><h4 id="指标系统和可视化系统"><a href="#指标系统和可视化系统" class="headerlink" title="指标系统和可视化系统"></a>指标系统和可视化系统</h4><p>指标口径管理依赖指标系统，指标可视化依赖可视化系统，我们通过下图的需求开发过程来讲解如何将三者联系起来。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-6ea544ac5b0473365f142d15f0ea1948_1440w.jpg" alt="img"></p>
<ol>
<li><p>需求方整理好需求文档后向数仓工程师提出需求并约会议评审需求，需求文档中必须包含指标的计算口径和指标对应的维度。</p>
</li>
<li><p>数仓工程师根据需求文档对需求进行评审，评审不通过则返回需求方进一步整理需求并重新提需。</p>
</li>
<li><p>在需求评审通过后，数仓工程师开始排期开发</p>
</li>
<li><ol>
<li>首先在可视化系统中创建一个数据源，这个数据源是后期配置实时报表的数据源，创建数据源也即在 HBase 中创建一张 HBase 表。</li>
<li>针对该数据源创建指标列，创建指标列也即在 HBase 列族中创建列，创建指标列的同时会将该指标信息录入指标管理系统。</li>
<li>针对该数据源绑定维表，这个维表是后期配置多维报表时选用维度值要用的，如果要绑定的维表已经存在，则直接绑定，否则需要导入维表。</li>
<li>一个完整的数据源创建后，数仓工程师才能开发实时应用程序，通过应用程序将多维指标实时写入已创建的数据源中。</li>
</ol>
</li>
<li><p>需求方根据已创建的数据源直接配置实时报表。</p>
</li>
</ol>
<h3 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h3><p>应用层主要是使用汇总层数据以满足业务需求。应用层主要分三块：1.通过直接读取指标汇总数据做实时可视化，满足固化的实时报表需求，这部分由实时大盘服务承担；2.推荐算法等业务直接消费明细汇总数据做实时推荐；3.通过 Tranquility 程序实时摄入明细汇总数据到 Druid，满足实时多维即席分析需求。</p>
<h3 id="实时数仓-2-0-中的技术实现"><a href="#实时数仓-2-0-中的技术实现" class="headerlink" title="实时数仓 2.0 中的技术实现"></a>实时数仓 2.0 中的技术实现</h3><p>相比实时数仓 1.0 以 Spark Streaming 作为主要实现技术，在实时数仓 2.0 中，我们将 Flink 作为指标汇总层的主要计算框架。Flink 相比 Spark Streaming 有更明显的优势，主要体现在：低延迟、Exactly-once 语义支持、Streaming SQL 支持、状态管理、丰富的时间类型和窗口计算、CEP 支持等。</p>
<p>我们在实时数仓 2.0 中主要以 Flink 的 Streaming SQL 作为实现方案。使用 Streaming SQL 有以下优点：易于平台化、开发效率高、维度成本低等。目前 Streaming SQL 使用起来也有一些缺陷：1.语法和 Hive SQL 有一定区别，初使用时需要适应；2.UDF 不如 Hive 丰富，写 UDF 的频率高于 Hive。</p>
<h3 id="实时数仓-2-0-取得的进展"><a href="#实时数仓-2-0-取得的进展" class="headerlink" title="实时数仓 2.0 取得的进展"></a>实时数仓 2.0 取得的进展</h3><ol>
<li>在明细汇总层通过流量切分满足了各个业务实时消费日志的需求。目前完成流量切分的业务达到 14+，由于各业务消费的是切分后的流量，Kafka 出流量下降了一个数量级。</li>
<li>各业务核心实时报表可以稳定产出。由于核心报表的计算直接由数仓负责，可视化系统直接读取实时结果，保证了实时报表的稳定性，目前多个业务拥有实时大盘，实时报表达 40+。</li>
<li>提升了即席查询的稳定性。核心报表的指标计算转移到数仓，Druid 只负责即席查询，多维分析类的需求得到了满足。</li>
<li>成本计算需求得到了解决。由于各业务拥有了独立的数据源且各核心大盘由不同的实时程序负责，可以方便的统计各业务使用的存储资源和计算资源。</li>
</ol>
<h3 id="实时数仓未来展望"><a href="#实时数仓未来展望" class="headerlink" title="实时数仓未来展望"></a>实时数仓未来展望</h3><p>从实时数仓 1.0 到 2.0，不管是数据架构还是技术方案，我们在深度和广度上都有了更多的积累。随着公司业务的快速发展以及新技术的诞生，实时数仓也会不断的迭代优化。短期可预见的我们会从以下方面进一步提升实时数仓的服务能力。</p>
<ol>
<li>Streaming SQL 平台化。目前 Streaming SQL 任务是以代码开发 maven 打包的方式提交任务，开发成本高，后期随着 Streaming SQL 平台的上线，实时数仓的开发方式也会由 Jar 包转变为 SQL 文件。</li>
<li>实时数据元信息管理系统化。对数仓元信息的管理可以大幅度降低使用数据的成本，离线数仓的元信息管理已经基本完善，实时数仓的元信息管理才刚刚开始。</li>
<li>实时数仓结果验收自动化。对实时结果的验收只能借助与离线数据指标对比的方式，以 Hive 和 Kafka 数据源为例，分别执行 Hive SQL 和 Flink SQL，统计结果并对比是否一致实现实时结果验收的自动化。</li>
</ol>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>实时计算</tag>
        <tag>实时数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-streaming学习1</title>
    <url>/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一、流处理"><a href="#一、流处理" class="headerlink" title="一、流处理"></a>一、流处理</h2><h3 id="1-1-静态数据处理"><a href="#1-1-静态数据处理" class="headerlink" title="1.1 静态数据处理"></a>1.1 静态数据处理</h3><p>在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171353524.png" alt="image-20200817171353524"></p>
<a id="more"></a>

<h3 id="1-2-流处理"><a href="#1-2-流处理" class="headerlink" title="1.2 流处理"></a>1.2 流处理</h3><p>而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。</p>
<p>大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。</p>
<p>接收和发送数据流并执行应用程序或分析逻辑的系统称为<strong>流处理器</strong>。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171405470.png" alt="image-20200817171405470"></p>
<p>流处理带来了静态数据处理所不具备的众多优点：</p>
<ul>
<li><strong>应用程序立即对数据做出反应</strong>：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期；</li>
<li><strong>流处理可以处理更大的数据量</strong>：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量；</li>
<li><strong>流处理更贴近现实的数据模型</strong>：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求；</li>
<li><strong>流处理分散和分离基础设施</strong>：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。</li>
</ul>
<h2 id="二、Spark-Streaming"><a href="#二、Spark-Streaming" class="headerlink" title="二、Spark Streaming"></a>二、Spark Streaming</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>Spark Streaming 是 Spark 的一个子模块，用于快速构建可扩展，高吞吐量，高容错的流处理程序。具有以下特点：</p>
<ul>
<li>通过高级 API 构建应用程序，简单易用；</li>
<li>支持多种语言，如 Java，Scala 和 Python；</li>
<li>良好的容错性，Spark Streaming 支持快速从失败中恢复丢失的操作状态；</li>
<li>能够和 Spark 其他模块无缝集成，将流处理与批处理完美结合；</li>
<li>Spark Streaming 可以从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，也支持自定义数据源。</li>
</ul>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171416026.png" alt="image-20200817171416026"></p>
<h3 id="2-2-DStream"><a href="#2-2-DStream" class="headerlink" title="2.2 DStream"></a>2.2 DStream</h3><p>Spark Streaming 提供称为离散流 (DStream) 的高级抽象，用于表示连续的数据流。 DStream 可以从来自 Kafka，Flume 和 Kinesis 等数据源的输入数据流创建，也可以由其他 DStream 转化而来。<strong>在内部，DStream 表示为一系列 RDD</strong>。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171426304.png" alt="image-20200817171426304"></p>
<h3 id="2-3-Spark-amp-Storm-amp-Flink"><a href="#2-3-Spark-amp-Storm-amp-Flink" class="headerlink" title="2.3 Spark &amp; Storm &amp; Flink"></a>2.3 Spark &amp; Storm &amp; Flink</h3><p>storm 和 Flink 都是真正意义上的流计算框架，但 Spark Streaming 只是将数据流进行极小粒度的拆分，拆分为多个批处理，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。</p>
]]></content>
      <categories>
        <category>spark streaming</category>
      </categories>
      <tags>
        <tag>流处理</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase学习2-系统架构与数据结构</title>
    <url>/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>一个典型的 Hbase Table 表如下：</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172516439.png" alt="image-20200817172516439"></p>
<a id="more"></a>

<h3 id="1-1-Row-Key-行键"><a href="#1-1-Row-Key-行键" class="headerlink" title="1.1 Row Key (行键)"></a>1.1 Row Key (行键)</h3><p><code>Row Key</code> 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：</p>
<ul>
<li>通过指定的 <code>Row Key</code> 进行访问；</li>
<li>通过 Row Key 的 range 进行访问，即访问指定范围内的行；</li>
<li>进行全表扫描。</li>
</ul>
<p><code>Row Key</code> 可以是任意字符串，存储时数据按照 <code>Row Key</code> 的字典序进行排序。这里需要注意以下两点：</p>
<ul>
<li>因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。</li>
<li>行的一次读写操作时原子性的 (不论一次读写多少列)。</li>
</ul>
<h3 id="1-2-Column-Family（列族）"><a href="#1-2-Column-Family（列族）" class="headerlink" title="1.2 Column Family（列族）"></a>1.2 Column Family（列族）</h3><p>HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族。</p>
<h3 id="1-3-Column-Qualifier-列限定符"><a href="#1-3-Column-Qualifier-列限定符" class="headerlink" title="1.3 Column Qualifier (列限定符)"></a>1.3 Column Qualifier (列限定符)</h3><p>列限定符，你可以理解为是具体的列名，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族，它们的列限定符分别是 <code>history</code> 和 <code>math</code>。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。</p>
<h3 id="1-4-Column-列"><a href="#1-4-Column-列" class="headerlink" title="1.4 Column(列)"></a>1.4 Column(列)</h3><p>HBase 中的列由列族和列限定符组成，它们由 <code>:</code>(冒号) 进行分隔，即一个完整的列名应该表述为 <code>列族名 ：列限定符</code>。</p>
<h3 id="1-5-Cell"><a href="#1-5-Cell" class="headerlink" title="1.5 Cell"></a>1.5 Cell</h3><p><code>Cell</code> 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。</p>
<h3 id="1-6-Timestamp-时间戳"><a href="#1-6-Timestamp-时间戳" class="headerlink" title="1.6 Timestamp(时间戳)"></a>1.6 Timestamp(时间戳)</h3><p>HBase 中通过 <code>row key</code> 和 <code>column</code> 确定的为一个存储单元称为 <code>Cell</code>。每个 <code>Cell</code> 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 <code>Cell</code> 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。</p>
<h2 id="二、存储结构"><a href="#二、存储结构" class="headerlink" title="二、存储结构"></a>二、存储结构</h2><h3 id="2-1-Regions"><a href="#2-1-Regions" class="headerlink" title="2.1 Regions"></a>2.1 Regions</h3><p>HBase Table 中的所有行按照 <code>Row Key</code> 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 <code>Region</code>, 一个 <code>Region</code> 包含了在 start key 和 end key 之间的所有行。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172532668.png" alt="image-20200817172532668"></p>
<p>每个表一开始只有一个 <code>Region</code>，随着数据不断增加，<code>Region</code> 会不断增大，当增大到一个阀值的时候，<code>Region</code> 就会等分为两个新的 <code>Region</code>。当 Table 中的行不断增多，就会有越来越多的 <code>Region</code>。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172542401.png" alt="image-20200817172542401"></p>
<p><code>Region</code> 是 HBase 中<strong>分布式存储和负载均衡的最小单元</strong>。这意味着不同的 <code>Region</code> 可以分布在不同的 <code>Region Server</code> 上。但一个 <code>Region</code> 是不会拆分到多个 Server 上的。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172550839.png" alt="image-20200817172550839"></p>
<h3 id="2-2-Region-Server"><a href="#2-2-Region-Server" class="headerlink" title="2.2 Region Server"></a>2.2 Region Server</h3><p><code>Region Server</code> 运行在 HDFS 的 DataNode 上。它具有以下组件：</p>
<ul>
<li><strong>WAL(Write Ahead Log，预写日志)</strong>：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。</li>
<li><strong>BlockCache</strong>：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 <code>最近最少使用原则</code> 清除多余的数据。</li>
<li><strong>MemStore</strong>：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。</li>
<li><strong>HFile</strong> ：将行数据按照 Key\Values 的形式存储在文件系统上。</li>
</ul>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172601304.png" alt="image-20200817172601304"></p>
<p>Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 <code>Store</code> 实例，每个 <code>Store</code> 会有 0 个或多个 <code>StoreFile</code> 与之对应，每个 <code>StoreFile</code> 则对应一个 <code>HFile</code>，HFile 就是实际存储在 HDFS 上的文件。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172611931.png" alt="image-20200817172611931"></p>
<h2 id="三、Hbase系统架构"><a href="#三、Hbase系统架构" class="headerlink" title="三、Hbase系统架构"></a>三、Hbase系统架构</h2><h3 id="3-1-系统架构"><a href="#3-1-系统架构" class="headerlink" title="3.1 系统架构"></a>3.1 系统架构</h3><p>HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：</p>
<p><strong>Zookeeper</strong></p>
<ol>
<li>保证任何时候，集群中只有一个 Master；</li>
<li>存贮所有 Region 的寻址入口；</li>
<li>实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；</li>
<li>存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。</li>
</ol>
<p><strong>Master</strong></p>
<ol>
<li>为 Region Server 分配 Region ；</li>
<li>负责 Region Server 的负载均衡 ；</li>
<li>发现失效的 Region Server 并重新分配其上的 Region；</li>
<li>GFS 上的垃圾文件回收；</li>
<li>处理 Schema 的更新请求。</li>
</ol>
<p><strong>Region Server</strong></p>
<ol>
<li>Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求；</li>
<li>Region Server 负责切分在运行过程中变得过大的 Region。</li>
</ol>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172621158.png" alt="image-20200817172621158"></p>
<h3 id="3-2-组件间的协作"><a href="#3-2-组件间的协作" class="headerlink" title="3.2 组件间的协作"></a>3.2 组件间的协作</h3><p>HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：</p>
<ul>
<li>每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；</li>
<li>所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；</li>
<li>如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。</li>
</ul>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172629352.png" alt="image-20200817172629352"></p>
<h2 id="四、数据的读写流程简述"><a href="#四、数据的读写流程简述" class="headerlink" title="四、数据的读写流程简述"></a>四、数据的读写流程简述</h2><h3 id="4-1-写入数据的流程"><a href="#4-1-写入数据的流程" class="headerlink" title="4.1 写入数据的流程"></a>4.1 写入数据的流程</h3><ol>
<li>Client 向 Region Server 提交写请求；</li>
<li>Region Server 找到目标 Region；</li>
<li>Region 检查数据是否与 Schema 一致；</li>
<li>如果客户端没有指定版本，则获取当前系统时间作为数据版本；</li>
<li>将更新写入 WAL Log；</li>
<li>将更新写入 Memstore；</li>
<li>判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。</li>
</ol>
<blockquote>
<p>  更为详细写入流程可以参考：<a href="http://hbasefly.com/2016/03/23/hbase_writer/" target="_blank" rel="noopener">HBase － 数据写入流程解析</a></p>
</blockquote>
<h3 id="4-2-读取数据的流程"><a href="#4-2-读取数据的流程" class="headerlink" title="4.2 读取数据的流程"></a>4.2 读取数据的流程</h3><p>以下是客户端首次读写 HBase 上数据的流程：</p>
<ol>
<li>客户端从 Zookeeper 获取 <code>META</code> 表所在的 Region Server；</li>
<li>客户端访问 <code>META</code> 表所在的 Region Server，从 <code>META</code> 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 <code>META</code> 表的位置；</li>
<li>客户端从行键所在的 Region Server 上获取数据。</li>
</ol>
<p>如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 <code>META</code> 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。</p>
<p>注：<code>META</code> 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172639070.png" alt="image-20200817172639070"></p>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Install ClickHouse with RPM packages from Altinity&#39;s repo(s)</title>
    <url>/2020/08/19/How-to-Install-ClickHouse-with-RPM-packages-from-Altinity-s-repo-s/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="What-is-this"><a href="#What-is-this" class="headerlink" title="What is this"></a>What is this</h2><p>This is a detailed explanation on how to install ready-to-use ClickHouse RPMs from Altinity’s repos (either <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">general repo</a> or <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">stable repo</a>) located on <a href="https://packagecloud.io/Altinity" target="_blank" rel="noopener">packagecloud.io</a>. This is <strong>not</strong> an instructions on how to build your own hand-made RPMs. However, if you need to build your own RPMs, there is a <a href="https://github.com/Altinity/clickhouse-rpm" target="_blank" rel="noopener">detailed explanation</a> on how to build ClickHouse RPMs from sources with the help of Altinity’s <a href="https://github.com/Altinity/clickhouse-rpm" target="_blank" rel="noopener">RPM builder</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="general-and-stable-repos"><a href="#general-and-stable-repos" class="headerlink" title="general and stable repos"></a><code>general</code> and <code>stable</code> repos</h3><p>Altinity provides two repos:</p>
<ul>
<li><p><code>general</code> <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">repo</a> with general ClickHouse releases.</p>
</li>
<li><p><code>stable</code> <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">repo</a> with Altinity Stable ClickHouse releases.</p>
<a id="more"></a>

</li>
</ul>
<h3 id="Supported-OSes"><a href="#Supported-OSes" class="headerlink" title="Supported OSes"></a>Supported OSes</h3><p>All instructions in this manual were tested on Centos 6.10, CentOS 7.5 and Amazon Linux 2.</p>
<p><strong>IMPORTANT for Amazon Linux users</strong> Amazon Linux is being detected as CentOS 6, while RPMs built for CentOS 7 are the best choice. So we need to explicitly install CentOS 7 RPMs More details further in the doc.</p>
<h3 id="Register-repo"><a href="#Register-repo" class="headerlink" title="Register repo"></a>Register repo</h3><p>In order to install ClickHouse RPM packages from Altinity’s repo, we need to register it (repo) with our <code>yum</code>, making <code>yum</code> aware of additional packages installable from external source.</p>
<p>In general, repositories are listed in <code>/etc/yum.repos.d</code> folder, so we need to add Altinity’s repo description in there.</p>
<p>This can be done either <a href="https://github.com/Altinity/clickhouse-rpm-install#manual-installation" target="_blank" rel="noopener">manually</a> or via <a href="https://github.com/Altinity/clickhouse-rpm-install#script-based-installation" target="_blank" rel="noopener">script</a>, provided by <code>packagecloud.io</code>. In any case, as a result, we’ll have ClickHouse packages available for installation via <code>yum</code>.</p>
<p><strong>IMPORTANT for Amazon Linux users</strong> Amazon Linux is being detected as CentOS 6 by the script, so we need to explicitly instruct it to use CentOS 7 repo.</p>
<ul>
<li>In case of <a href="https://github.com/Altinity/clickhouse-rpm-install#manual-installation" target="_blank" rel="noopener">manual installation</a>, just use <a href="https://github.com/Altinity/clickhouse-rpm-install#el7-repo-file" target="_blank" rel="noopener">EL7 repo file</a>. It is compatible with Amazon Linux</li>
<li>In case of <a href="https://github.com/Altinity/clickhouse-rpm-install#script-based-installation" target="_blank" rel="noopener">script-based installation</a>, script provided by packagecloud should be explicitly instructed to use CentOS 7 repo, instead of CentOS 6 repo, which is being used by default for Amazon Linux. More details further in the doc.</li>
</ul>
<p>Let’s start with script-based installation, since this approach looks like more user-friendly.</p>
<h2 id="Script-based-installation"><a href="#Script-based-installation" class="headerlink" title="Script-based installation"></a>Script-based installation</h2><p>For our convenience,<code>packagecloud.io</code> provides nice and user-friendly way to add repos with their <code>shell script</code>. We’ll need to download and run <strong>packagecloud</strong>‘s <code>shell script</code>, which will do all required steps.</p>
<h3 id="Install-dependencies"><a href="#Install-dependencies" class="headerlink" title="Install dependencies"></a>Install dependencies</h3><p>Installation process requires <code>curl</code> in order to download packages. ClickHouse test package has some dependencies in EPEL, so <code>epel-release</code> has to be installed as well, in case you’d like to install ClickHouse test package. Some installations do not have <code>sudo</code> installed, so we need to ensure it is availbale also.</p>
<p>Ensure <code>sudo</code> is available:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y sudo</span><br></pre></td></tr></table></figure>

<p>Ensure <code>curl</code> is available:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y curl</span><br><span class="line"># in case test package would be installed, add epel-release</span><br><span class="line">sudo yum install -y epel-release</span><br></pre></td></tr></table></figure>

<p>Let’s download and run installation <code>shell script</code>, provided by <code>packagecloud.io</code>. First of all, we need to point what script (from <code>general</code> or <code>stable</code> repo) we’ll be using:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># For &#39;general&#39; repo use this URL:</span><br><span class="line">SCRIPT_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;altinity&#x2F;clickhouse&#x2F;script.rpm.sh&quot;</span><br><span class="line"></span><br><span class="line"># For &#39;stable&#39; repo use this URL:</span><br><span class="line">SCRIPT_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;altinity&#x2F;clickhouse-altinity-stable&#x2F;script.rpm.sh&quot;</span><br></pre></td></tr></table></figure>

<p>Now we can register Altiniry’s repo in the system by running appropriate script.</p>
<p><strong>for CentOS 6 and 7</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s &quot;$&#123;SCRIPT_URL&#125;&quot; | sudo bash</span><br></pre></td></tr></table></figure>

<p><strong>for Amazon Linux</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s &quot;$&#123;SCRIPT_URL&#125;&quot; | sudo os&#x3D;centos dist&#x3D;7 bash</span><br></pre></td></tr></table></figure>

<p>pay attention to <code>os=centos dist=7</code> explicitly specified.</p>
<p>At this point we have <code>yum</code> aware of additional RPM packages available.</p>
<p>We are ready to install ClickHouse.</p>
<h3 id="Install-packages"><a href="#Install-packages" class="headerlink" title="Install packages"></a>Install packages</h3><p>First of all, ensure we have ClickHouse packages available for installation</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*&#39;</span><br></pre></td></tr></table></figure>

<p>ClickHouse packages should be listed as available, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64            19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-common-static.x86_64     19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-compressor.x86_64        1.1.54336-3.el7       Altinity_clickhouse</span><br><span class="line">clickhouse-debuginfo.x86_64         19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-odbc.x86_64              1.0.0.20190611-1      Altinity_clickhouse</span><br><span class="line">clickhouse-server.x86_64            19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-server-common.x86_64     19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-test.x86_64              19.13.3.26-1.el7      Altinity_clickhouse</span><br></pre></td></tr></table></figure>

<p>There are multiple packages available (new versions and old tools as well), some of them are deprecated already, so there is no need to install all available RPMs.</p>
<p>Now let’s install ClickHouse main parts - server and client applications.</p>
<h4 id="Install-latest-ClickHouse-version"><a href="#Install-latest-ClickHouse-version" class="headerlink" title="Install latest ClickHouse version"></a>Install latest ClickHouse version</h4><p>In case we’d like to just install latest ClickHouse (it is so in most cases), we can simply install <code>clickhouse-server</code> and <code>clickhouse-client</code> as following:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<p>However, sometimes we’d like to install specific version of ClickHouse.</p>
<h4 id="Install-specific-ClickHouse-version"><a href="#Install-specific-ClickHouse-version" class="headerlink" title="Install specific ClickHouse version"></a>Install specific ClickHouse version</h4><p>We can either just want to install latest version from specific branch, or we may know what ClickHouse version we’d like to install exactly, or we can look over availbale (older) versions available for installation. These cases are little bit different, let’s take a look on both of them.</p>
<p><strong>Select latest version from specific branch</strong></p>
<p>In case we’d like to install latest version of <code>19.11.X.Y</code> family, we can list available latest <code>19.11.*</code> packages</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*19.11.*&#39;</span><br></pre></td></tr></table></figure>

<p>We’ll see packages of one proposed version (latest) only:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.9.52-1.el7</span><br><span class="line">clickhouse-debuginfo.x86_64      19.11.9.52-1.el7</span><br><span class="line">clickhouse-server.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-server-common.x86_64  19.11.9.52-1.el7</span><br><span class="line">clickhouse-test.x86_64           19.11.9.52-1.el7</span><br></pre></td></tr></table></figure>

<p><strong>Select specific version from specific branch</strong></p>
<p>In case we’d like to see all available versions of <code>19.11.X.Y</code> family, then select preferred version out of availbale for installation:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*19.11.*&#39; --showduplicates</span><br></pre></td></tr></table></figure>

<p>We’ll see all available package versions from <code>19.11.X.Y</code> family:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64         19.11.2.7-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.3.11-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.4.24-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.6.31-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.7.40-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.8.46-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.2.7-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.3.11-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.4.24-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.6.31-1.el7</span><br><span class="line">...</span><br><span class="line">and more</span><br></pre></td></tr></table></figure>

<p><strong>Install specific version</strong></p>
<p>By now, we have picked up specific version (out of available) - let’s install it:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y clickhouse-server-19.11.7.40 clickhouse-client-19.11.7.40</span><br></pre></td></tr></table></figure>

<p>and verify it is listed as installed</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list installed &#39;clickhouse*&#39;</span><br></pre></td></tr></table></figure>

<p>ClickHouse packages should be listed as installed, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Installed Packages</span><br><span class="line">clickhouse-client.x86_64                 19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-common-static.x86_64          19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-server.x86_64                 19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-server-common.x86_64          19.11.7.40-1.el7           @Altinity_clickhouse</span><br></pre></td></tr></table></figure>

<p>Ensure ClickHouse server is running</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;clickhouse-server restart</span><br></pre></td></tr></table></figure>

<p>And connect to it with <code>clickhouse-client</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure>

<p>ClickHouse server should respond, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ClickHouse client version 19.11.7.40.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 19.11.7 revision 54423.</span><br><span class="line"></span><br><span class="line">ip-172-31-37-226.ec2.internal :)</span><br></pre></td></tr></table></figure>

<p>Well, all looks fine and ClickHouse installed from <strong>RPM</strong> packages!</p>
<p>We are all done!</p>
<h2 id="Manual-installation"><a href="#Manual-installation" class="headerlink" title="Manual installation"></a>Manual installation</h2><p>Let’s add any of Altinity’s repos - <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">general</a> or <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">stable</a> manually</p>
<h3 id="Install-required-packages"><a href="#Install-required-packages" class="headerlink" title="Install required packages"></a>Install required packages</h3><p>We’ll need the following packages installed beforehands:</p>
<ul>
<li><code>pygpgme</code> - helps handling gpg-signatures</li>
<li><code>yum-utils</code> - contains tools for handling source RPMs</li>
<li><code>coreutils</code> - contains core utils and we’ll need <code>tee</code> command later</li>
<li><code>epel-release</code> - contains ClickHouse test package dependencies</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y pygpgme yum-utils coreutils epel-release</span><br></pre></td></tr></table></figure>

<h3 id="Create-required-files"><a href="#Create-required-files" class="headerlink" title="Create required files"></a>Create required files</h3><p>Now let’s create <code>yum</code>‘s repository configuration file: <code>/etc/yum.repos.d/altinity_clickhouse.repo</code>. Depending on what CentOS version you are running you may need files for EL 6 or 7 version.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># For &#39;general&#39; repo use this URL</span><br><span class="line">BASE_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;altinity&#x2F;clickhouse&quot;</span><br><span class="line"></span><br><span class="line"># For &#39;stable&#39; repo use this URL</span><br><span class="line">BASE_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;altinity&#x2F;clickhouse-altinity-stable&quot;</span><br></pre></td></tr></table></figure>

<h4 id="EL6-repo-file"><a href="#EL6-repo-file" class="headerlink" title="EL6 repo file"></a>EL6 repo file</h4><p>EL6 (<strong>do NOT use with Amazon Linux</strong>) ready-to-copy+paste command to create <code>yum</code>‘s repo config file.<br>It writes <code>/etc/yum.repos.d/altinity_clickhouse.repo</code> file:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;altinity_clickhouse.repo</span><br><span class="line">[altinity_clickhouse]</span><br><span class="line">name&#x3D;altinity_clickhouse</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;6&#x2F;\$basearch</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line"></span><br><span class="line">[altinity_clickhouse-source]</span><br><span class="line">name&#x3D;altinity_clickhouse-source</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;6&#x2F;SRPMS</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h4 id="EL7-repo-file"><a href="#EL7-repo-file" class="headerlink" title="EL7 repo file"></a>EL7 repo file</h4><p>EL7 <strong>and Amazon Linux</strong> ready-to-copy+paste command to create <code>yum</code>‘s repo config file.<br>It writes <code>/etc/yum.repos.d/altinity_clickhouse.repo</code> file:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;altinity_clickhouse.repo</span><br><span class="line">[altinity_clickhouse]</span><br><span class="line">name&#x3D;altinity_clickhouse</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;7&#x2F;\$basearch</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line"></span><br><span class="line">[altinity_clickhouse-source]</span><br><span class="line">name&#x3D;altinity_clickhouse-source</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;7&#x2F;SRPMS</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h3 id="Update-cache"><a href="#Update-cache" class="headerlink" title="Update cache"></a>Update cache</h3><p>After repo files created, let’s update <code>yum</code>‘s cache with packges from newly added repo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum -q makecache -y --disablerepo&#x3D;&#39;*&#39; --enablerepo&#x3D;&#39;altinity*&#39;</span><br></pre></td></tr></table></figure>

<h3 id="Install-packages-manually"><a href="#Install-packages-manually" class="headerlink" title="Install packages manually"></a>Install packages manually</h3><p>Packages can be installed the same way as in section <a href="https://github.com/Altinity/clickhouse-rpm-install#install-packages" target="_blank" rel="noopener">Install packages</a> after script-based installation.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Now we have ClickHouse <strong>RPM</strong> packages available for easy installation.</p>
]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark结构化流-窗口操作与水印</title>
    <url>/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="事件时间窗口操作"><a href="#事件时间窗口操作" class="headerlink" title="事件时间窗口操作"></a>事件时间窗口操作</h3><p>滑动事件时间窗口上的聚合对于结构化流而言非常简单，并且与分组聚合非常相似。在分组聚合中，在用户指定的分组列中为每个唯一值维护聚合值（例如，计数）。在基于窗口的聚合的情况下，行事件时间所属的每个窗口都会维护聚合值。让我们通过插图来了解这一点。</p>
<p>想象一下我们的<a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#quick-example" target="_blank" rel="noopener">快速示例</a>已被修改，并且流现在包含行以及生成行的时间。而不是运行字数统计，我们希望在10分钟的窗口内对字数进行计数，每5分钟更新一次。也就是说，在10分钟窗口12:00-12：10、12：05-12：15、12：10-12:20等之间接收的单词中的单词计数。请注意，12：00-12:10表示数据12:00之后但12:10之前到达。现在，考虑在12:07收到的单词。此字应增加对应于两个窗口12:00-12:10和12:05-12:15的计数。因此，计数将通过分组键（即单词）和窗口（可以从事件时间计算）来索引。</p>
<p>结果表如下所示。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-window.png" alt="窗口操作"></p>
<a id="more"></a>

<p>由于此窗口化类似于分组，因此在代码中，您可以使用<code>groupBy()</code>和<code>window()</code>操作来表示窗口化聚合。您可以在<a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala" target="_blank" rel="noopener">Scala</a> / <a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java" target="_blank" rel="noopener">Java</a> / <a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py" target="_blank" rel="noopener">Python中</a>看到以下示例的完整代码 。</p>
<ul>
<li>scala</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words = ... <span class="comment">// streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Group the data by window and word and compute the count of each group</span></span><br><span class="line"><span class="keyword">val</span> windowedCounts = words.groupBy(</span><br><span class="line">  window($<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>, <span class="string">"5 minutes"</span>),</span><br><span class="line">  $<span class="string">"word"</span></span><br><span class="line">).count()</span><br></pre></td></tr></table></figure>

<h3 id="处理后期数据和加水印"><a href="#处理后期数据和加水印" class="headerlink" title="处理后期数据和加水印"></a>处理后期数据和加水印</h3><p>现在考虑如果事件之一迟到应用程序会发生什么。例如，例如，应用程序可以在12:11接收在12:04（即事件时间）生成的单词。应用程序应使用12:04而不是12:11来更新窗口的旧计数<code>12:00 - 12:10</code>。这在我们基于窗口的分组中很自然地发生–结构化流可以长时间保持部分聚合的中间状态，以便后期数据可以正确更新旧窗口的聚合，如下所示。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-late-data.png" alt="处理后期数据"></p>
<p>但是，要连续几天运行此查询，系统必须限制其累积的中间内存状态量。这意味着系统需要知道何时可以从内存中状态删除旧聚合，因为应用程序将不再接收该聚合的最新数据。为此，我们在Spark 2.1中引入了 <strong>水印功能</strong>，该功能使引擎自动跟踪数据中的当前事件时间，并尝试相应地清除旧状态。您可以通过指定事件时间列和有关事件时间期望数据延迟的阈值来定义查询的水印。对于在时间开始的特定窗口<code>T</code>，引擎将维持状态并允许以后的数据更新状态，直到<code>(max event time seen by the engine - late threshold &gt; T)</code>。换句话说，阈值内的较晚数据将被汇总，但阈值后的数据将被丢弃。让我们通过一个例子来理解这一点。我们可以使用<code>withWatermark()</code>以下示例在上一个示例中轻松定义水印。</p>
<ul>
<li>scala</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words = ... <span class="comment">// streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Group the data by window and word and compute the count of each group</span></span><br><span class="line"><span class="keyword">val</span> windowedCounts = words</span><br><span class="line">    .withWatermark(<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>)</span><br><span class="line">    .groupBy(</span><br><span class="line">        window($<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>, <span class="string">"5 minutes"</span>),</span><br><span class="line">        $<span class="string">"word"</span>)</span><br><span class="line">    .count()</span><br></pre></td></tr></table></figure>

<p>在此示例中，我们将在“时间戳”列的值上定义查询的水印，还将“ 10分钟”定义为允许数据晚到的阈值。如果此查询在“更新输出”模式下运行（稍后在“ <a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes" target="_blank" rel="noopener">输出模式”</a>部分中讨论），则引擎将在“结果表”中保持窗口的更新计数，直到该窗口早于水印为止，该时间滞后于“列”中的当前事件时间。时间戳”的时间缩短10分钟。这是一个例子。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-watermark-update-mode.png" alt="更新模式中的水印"></p>
<p>如图所示，引擎跟踪的最大事件时间是 <em>蓝色虚线</em>，<code>(max event time - &#39;10 mins&#39;)</code> 在每次触发的开始时设置的水印是红线。例如，当引擎观察到数据时 <code>(12:14, dog)</code>，它将为下一个触发器为<code>12:04</code>。此水印可让引擎再保持10分钟的中间状态，以便对较晚的数据进行计数。例如，数据<code>(12:09, cat)</code>不正确且延迟，并且落在windows <code>12:05 - 12:15</code>和中<code>12:10 - 12:20</code>。由于它仍在<code>12:04</code>触发器中的水印之前，因此引擎仍将中间计数保持为状态并正确更新相关窗口的计数。但是，当水印更新为<code>12:11</code>，<code>(12:00 - 12:10)</code>清除了窗口的中间状态，并且所有后续数据（例如<code>(12:04, donkey)</code>）都被认为“为时已晚”，因此被忽略。请注意，在每次触发后，更新的计数（即紫色行）都会写入更新，作为更新输出指示的触发输出。</p>
<p>某些接收器（例如文件）可能不支持更新模式所需的细粒度更新。为了与他们合作，我们还支持追加模式，其中仅将<em>最终计数</em>写入接收器。如下所示。</p>
<p>请注意，<code>withWatermark</code>在非流数据集上使用是no-op。由于水印不应以任何方式影响任何批量查询，因此我们将直接忽略它。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-watermark-append-mode.png" alt="追加模式中的水印"></p>
<p>与之前的更新模式类似，引擎为每个窗口维护中间计数。但是，部分计数不会更新到结果表，也不会写入接收器。引擎等待”10分钟”来计算延迟日期，然后丢弃窗口的中间状态&lt;水印，并将最终计数附加到结果表/接收器。例如，<code>12:00 - 12:10</code>仅在将水印更新为之后，窗口的最终计数才添加到结果表中<code>12:11</code>。</p>
<p><strong>用于加水印以清除聚合状态</strong> 的条件重要的是要注意，对于加水印以清除聚合查询中的状态必须满足以下条件<em>（从Spark 2.1.1开始，将来可能会更改）</em>。</p>
<ul>
<li><strong>输出模式必须为追加或更新。</strong>完整模式要求保留所有聚合数据，因此不能使用水印删除中间状态。有关 每种输出模式的语义的详细说明，请参见“ <a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes" target="_blank" rel="noopener">输出模式”</a>部分。</li>
<li>聚合必须在“事件时间”列上或<code>window</code>“事件时间”列上有一个。</li>
<li><code>withWatermark</code>必须在与汇总中使用的时间戳列相同的列上调用。例如， <code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()</code>在附加输出模式下无效，因为水印是在与聚合列不同的列上定义的。</li>
<li><code>withWatermark</code>必须在使用水印详细信息的聚合之前调用。例如，<code>df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;)</code>在追加输出模式下无效。</li>
</ul>
]]></content>
      <categories>
        <category>spark-structured-streaming</category>
      </categories>
      <tags>
        <tag>流处理</tag>
        <tag>spark</tag>
        <tag>窗口操作</tag>
        <tag>水印操作</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker介绍</title>
    <url>/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>为了更好的理解 Docker 是什么，我们先来讲个故事：</p>
<p>我需要盖一个房子，于是我搬石头、砍木头、画图纸、盖房子。一顿操作，终于把这个房子盖好了。</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640.png" alt="img"></p>
<a id="more"></a>

<p>结果，住了一段时间，心血来潮想搬到海边去。这时候按以往的办法，我只能去海边，再次搬石头、砍木头、画图纸、盖房子。</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940565.png" alt="img"></p>
<p>烦恼之际，跑来一个魔法师教会我一种魔法。这种魔法可以把我盖好的房子复制一份，做成「镜像」，放在我的背包里。</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940574.png" alt="img"></p>
<p>等我到了海边，就用这个「镜像」，复制一套房子，拎包入住。</p>
<p>是不是很神奇？对应到我们的项目中来，房子就是项目本身，镜像就是项目的复制，背包就是镜像仓库。</p>
<p>如果要动态扩容，从仓库中取出项目镜像，随便复制就可以了。Build once，Run anywhere!</p>
<p>不用再关注版本、兼容、部署等问题，彻底解决了「上线即崩，无休止构建」的尴尬。</p>
<h2 id="虚拟机与容器"><a href="#虚拟机与容器" class="headerlink" title="虚拟机与容器"></a>虚拟机与容器</h2><p>开始之前，我们来做一些基础知识的储备：</p>
<p><strong>①虚拟机：虚拟化硬件</strong></p>
<p>虚拟机 Virtual Machine 指通过软件模拟的具有完整硬件系统功能的、运行在一个完全隔离环境中的完整计算机系统。在实体计算机中能够完成的工作在虚拟机中都能够实现。</p>
<p>在计算机中创建虚拟机时，需要将实体机的部分硬盘和内存容量作为虚拟机的硬盘和内存容量。</p>
<p>每个虚拟机都有独立的 CMOS、硬盘和操作系统，可以像使用实体机一样对虚拟机进行操作。在容器技术之前，业界的网红是虚拟机。</p>
<p>虚拟机技术的代表，是 VMWare 和 OpenStack。</p>
<p><strong>②容器：将操作系统层虚拟化，是一个标准的软件单元</strong></p>
<p><code>其特点如下</code>：</p>
<ul>
<li><strong>随处运行：</strong>容器可以将代码与配置文件和相关依赖库进行打包，从而确保在任何环境下的运行都是一致的。</li>
<li><strong>高资源利用率：</strong>容器提供进程级的隔离，因此可以更加精细地设置 CPU 和内存的使用率，进而更好地利用服务器的计算资源。</li>
<li><strong>快速扩展：</strong>每个容器都可作为单独的进程予以运行，并且可以共享底层操作系统的系统资源，这样一来可以加快容器的启动和停止效率。</li>
</ul>
<p><code>区别与联系</code>：</p>
<ul>
<li>虚拟机虽然可以隔离出很多「子电脑」，但占用空间更大，启动更慢。虚拟机软件可能还要花钱，例如 VMWare。</li>
<li>容器技术不需要虚拟出整个操作系统，只需要虚拟一个小规模的环境，类似「沙箱」。</li>
<li>运行空间，虚拟机一般要几 GB 到 几十 GB 的空间，而容器只需要 MB 级甚至 KB 级。</li>
</ul>
<p><code>我们来看一下对比数据</code>：</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640.jpeg" alt="img"></p>
<p>虚拟机属于虚拟化技术，而 Docker 这样的容器技术，属于轻量级的虚拟化。</p>
<p>与虚拟机相比，容器更轻量且速度更快，因为它利用了 Linux 底层操作系统在隔离的环境中运行。</p>
<p>虚拟机的 Hypervisor 创建了一个非常牢固的边界，以防止应用程序突破它，而容器的边界不那么强大。</p>
<h2 id="认识-Docker"><a href="#认识-Docker" class="headerlink" title="认识 Docker"></a>认识 Docker</h2><p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940571.png" alt="img"></p>
<p>Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。</p>
<p><code>Docker 技术的三大核心概念，分别是</code>：</p>
<ul>
<li><strong>镜像 Image</strong></li>
<li><strong>容器 Container</strong></li>
<li><strong>仓库 Repository</strong></li>
</ul>
<p>Docker 轻量级的原因是什么？相信你也会有这样的疑惑：为什么 Docker 启动快？如何做到和宿主机共享内核？</p>
<p>当我们请求 Docker 运行容器时，Docker 会在计算机上设置一个资源隔离的环境。</p>
<p>然后将打包的应用程序和关联的文件复制到 Namespace 内的文件系统中，此时环境的配置就完成了。之后 Docker 会执行我们预先指定的命令，运行应用程序。</p>
<p>镜像不包含任何动态数据，其内容在构建之后也不会被改变。</p>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p><code>核心概念如下</code>：</p>
<ul>
<li>Build，Ship and Run（搭建、运输、运行）。</li>
<li>Build once，Run anywhere（一次搭建，处处运行）。</li>
<li>Docker 本身并不是容器，它是创建容器的工具，是应用容器引擎。</li>
<li>Docker 三大核心概念，分别是：镜像 Image，容器 Container、仓库 Repository。</li>
<li>Docker 技术使用 Linux 内核和内核功能（例如 Cgroups 和 namespaces）来分隔进程，以便各进程相互独立运行。</li>
<li>由于 Namespace 和 Cgroups 功能仅在 Linux 上可用，因此容器无法在其他操作系统上运行。那么 Docker 如何在 macOS 或 Windows 上运行？Docker 实际上使用了一个技巧，并在非 Linux 操作系统上安装 Linux 虚拟机，然后在虚拟机内运行容器。</li>
<li>镜像是一个可执行包，其包含运行应用程序所需的代码、运行时、库、环境变量和配置文件，容器是镜像的运行时实例。</li>
</ul>
<p>更多关于 Docker 的原理，可以查看《Docker 工作原理及容器化简易指南》，这里不再赘述：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;dockone.io&#x2F;article&#x2F;8788</span><br></pre></td></tr></table></figure>

<h2 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h2><p><strong>①命令行安装</strong></p>
<p>Homebrew 的 Cask 已经支持 Docker for Mac，因此可以很方便的使用 Homebrew Cask 来进行安装，执行如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">brew cask install docker</span><br></pre></td></tr></table></figure>

<p>更多安装方式，请查看官方文档：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;www.docker.com&#x2F;get-started</span><br></pre></td></tr></table></figure>

<p><strong>②查看版本</strong></p>
<p>命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker -v</span><br></pre></td></tr></table></figure>

<p><strong>③配置镜像加速</strong></p>
<p><code>设置 Docker Engine 写入配置</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  registry-mirrors: [</span><br><span class="line">    http:&#x2F;&#x2F;hub-mirror.c.163.com&#x2F;,</span><br><span class="line">    https:&#x2F;&#x2F;registry.docker-cn.com</span><br><span class="line">  ],</span><br><span class="line">  insecure-registries:[],</span><br><span class="line">  experimental: false,</span><br><span class="line">  debug: true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>④安装桌面端</strong></p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940573.png" alt="img"></p>
<p>桌面端操作非常简单，先去官网下载[1]。通过 Docker 桌面端，我们可以方便的操作：</p>
<ul>
<li><strong>clone：</strong>克隆一个项目。</li>
<li><strong>build：</strong>打包镜像。</li>
<li><strong>run：</strong>运行实例。</li>
<li><strong>share：</strong>共享镜像。</li>
</ul>
<p>好了，准备工作就绪，下面可以大展身手了！</p>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><p>安装完 Docker 之后，我们先打个实际项目的镜像，边学边用。</p>
<p><strong>①首先需要大致了解一下我们将会用到的 11 个命令</strong></p>
<p><code>如下图</code>：</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940580.jpeg" alt="img"></p>
<p><strong>②新建项目</strong></p>
<p>为了快捷，我们直接使用 Vue 脚手架构建项目：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vue create docker-demo</span><br></pre></td></tr></table></figure>

<p>尝试启动一下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn serve</span><br></pre></td></tr></table></figure>

<p>访问地址：<a href="http://localhost:8080/。项目就绪，我们接着为项目打包：" target="_blank" rel="noopener">http://localhost:8080/。项目就绪，我们接着为项目打包：</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn build</span><br></pre></td></tr></table></figure>

<p>这时候，项目目录下的 Dist 就是我们要部署的静态资源了，我们继续下一步。</p>
<p>需要注意：前端项目一般分两类，一类直接 Nginx 静态部署，一类需要启动 Node 服务。本节我们只考虑第一种。关于 Node 服务，后文我会详细说明。</p>
<p><strong>③新建 Dockerfile</strong></p>
<p><code>命令如下</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd docker-demo &amp;&amp; touch Dockerfile</span><br></pre></td></tr></table></figure>

<p><code>此时的项目目录如下</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── Dockerfile</span><br><span class="line">├── README.md</span><br><span class="line">├── babel.config.js</span><br><span class="line">├── dist</span><br><span class="line">├── node_modules</span><br><span class="line">├── package.json</span><br><span class="line">├── public</span><br><span class="line">├── src</span><br><span class="line">└── yarn.lock</span><br></pre></td></tr></table></figure>

<p>可以看到我们已经在 docker-demo 目录下成功创建了 Dockerfile 文件。</p>
<p><strong>④准备 Nginx 镜像</strong></p>
<p>运行你的 Docker 桌面端，就会默认启动实例，我们在控制台拉取 Nginx 镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull nginx</span><br></pre></td></tr></table></figure>

<p><code>控制台会出现如下信息</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library&#x2F;nginx</span><br><span class="line">8559a31e96f4: Pull complete</span><br><span class="line">8d69e59170f7: Pull complete</span><br><span class="line">3f9f1ec1d262: Pull complete</span><br><span class="line">d1f5ff4f210d: Pull complete</span><br><span class="line">1e22bfa8652e: Pull complete</span><br><span class="line">Digest: sha256:21f32f6c08406306d822a0e6e8b7dc81f53f336570e852e25fbe1e3e3d0d0133</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br><span class="line">docker.io&#x2F;library&#x2F;nginx:latest</span><br></pre></td></tr></table></figure>

<p>如果你出现这样的异常，请确认 Docker 实例是否正常运行。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Cannot connect to the Docker daemon at unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock. Is the docker daemon running?</span><br></pre></td></tr></table></figure>

<p>镜像准备 OK，我们在根目录创建 Nginx 配置文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch default.conf</span><br></pre></td></tr></table></figure>

<p><code>写入</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    #charset koi8-r;</span><br><span class="line">    access_log  &#x2F;var&#x2F;log&#x2F;nginx&#x2F;host.access.log  main;</span><br><span class="line">    error_log  &#x2F;var&#x2F;log&#x2F;nginx&#x2F;error.log  error;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    error_page   500 502 503 504  &#x2F;50x.html;</span><br><span class="line">    location &#x3D; &#x2F;50x.html &#123;</span><br><span class="line">        root   &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>⑤配置镜像</strong></p>
<p>打开 Dockerfile ，写入如下内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM nginx</span><br><span class="line">COPY dist&#x2F; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;</span><br><span class="line">COPY default.conf &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf</span><br></pre></td></tr></table></figure>

<p><code>我们逐行解释一下代码</code>：</p>
<ul>
<li>FROM nginx 指定该镜像是基于 nginx:latest 镜像而构建的。</li>
<li>COPY dist/ /usr/share/nginx/html/ 命令的意思是将项目根目录下 dist 文件夹中的所有文件复制到镜像中 /usr/share/nginx/html/ 目录下。</li>
<li>COPY default.conf /etc/nginx/conf.d/default.conf 将 default.conf 复制到 etc/nginx/conf.d/default.conf，用本地的 default.conf 配置来替换 Nginx 镜像里的默认配置。</li>
</ul>
<p><strong>⑥构建镜像</strong></p>
<p>Docker 通过 build 命令来构建镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build -t jartto-docker-demo .</span><br></pre></td></tr></table></figure>

<p>按照惯例，我们解释一下上述代码`：</p>
<ul>
<li>-t 参数给镜像命名 jartto-docker-demo。</li>
<li>. 是基于当前目录的 Dockerfile 来构建镜像。</li>
</ul>
<p><code>执行成功后，将会输出</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sending build context to Docker daemon  115.4MB</span><br><span class="line">Step 1&#x2F;3 : FROM nginx</span><br><span class="line"> ---&gt; 2622e6cca7eb</span><br><span class="line">Step 2&#x2F;3 : COPY dist&#x2F; &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 82b31f98dce6</span><br><span class="line">Step 3&#x2F;3 : COPY default.conf &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf</span><br><span class="line"> ---&gt; 7df6efaf9592</span><br><span class="line">Successfully built 7df6efaf9592</span><br><span class="line">Successfully tagged jartto-docker-demo:latest</span><br></pre></td></tr></table></figure>

<p>镜像制作成功！我们来查看一下容器：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker image ls | grep jartto-docker-demo</span><br></pre></td></tr></table></figure>

<p>可以看到，我们打出了一个 133MB 的项目镜像：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jartto-docker-demo latest 7df6efaf9592 About a minute ago 133MB</span><br></pre></td></tr></table></figure>

<p>镜像也有好坏之分，后续我们将介绍如何优化，这里可以先暂时忽略。</p>
<p><strong>⑦运行容器</strong></p>
<p><code>命令如下</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d -p 3000:80 --name docker-vue jartto-docker-demo</span><br></pre></td></tr></table></figure>

<p><code>这里解释一下参数</code>：</p>
<ul>
<li>-d 设置容器在后台运行。</li>
<li>-p 表示端口映射，把本机的 3000 端口映射到 container 的 80 端口（这样外网就能通过本机的 3000 端口访问了。</li>
<li>–name 设置容器名 docker-vue。</li>
<li>jartto-docker-demo 是我们上面构建的镜像名字。</li>
</ul>
<p>补充一点：在控制台，我们可以通过 docker ps 查看刚运行的 Container 的 ID：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure>

<p><code>控制台会输出</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CONTAINER ID IMAGE              COMMAND                  CREATED       STATUS PORTS  NAMES</span><br><span class="line">ab1375befb0b jartto-docker-demo &#x2F;docker-entrypoint.…   8 minutes ago Up 7 minutes  0.0.0.0:3000-&gt;80&#x2F;tcp  docker-vue</span><br></pre></td></tr></table></figure>

<p>如果你使用桌面端，那么打开 Docker Dashboard 就可以看到容器列表了，如下图：</p>
<p><img src="/2020/08/22/Docker%E4%BB%8B%E7%BB%8D/640-20200822115940841.png" alt="img"></p>
<p><strong>⑧访问项目</strong></p>
<p><code>因为我们映射了本机 3000 端口，所以执行</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -v -i localhost:3000</span><br></pre></td></tr></table></figure>

<p>或者打开浏览器，访问：localhost:3000。</p>
<p><strong>⑨发布镜像</strong></p>
<p>如果你想为社区贡献力量，那么需要将镜像发布，方便其他开发者使用。</p>
<p><code>发布镜像需要如下步骤</code>：</p>
<ul>
<li>登陆 dockerhub[2]，注册账号。</li>
<li>命令行执行 docker login，之后输入我们的账号密码，进行登录。</li>
<li>推送镜像之前，需要打一个 Tag，执行 docker tag <image> <username>/<repository>:<tag>。</tag></repository></username></image></li>
</ul>
<p>全流程结束，以后我们要使用，再也不需要「搬石头、砍木头、画图纸、盖房子」了，拎包入住。这也是 Docker 独特魅力所在。</p>
<h2 id="常规操作"><a href="#常规操作" class="headerlink" title="常规操作"></a>常规操作</h2><p>到这里，恭喜你已经完成了 Docker 的入门项目！如果还想继续深入，不妨接着往下看看。</p>
<p><strong>①参数使用</strong></p>
<p><code>FROM</code>：</p>
<ul>
<li>指定基础镜像，所有构建的镜像都必须有一个基础镜像，且 FROM 命令必须是 Dockerfile 的第一个命令</li>
<li>FROM <image> [AS <name>] 指定从一个镜像构建起一个新的镜像名字</name></image></li>
<li>FROM <image>[:<tag>] [AS <name>] 指定镜像的版本 Tag</name></tag></image></li>
<li>示例：FROM mysql:5.0 AS database</li>
</ul>
<p><code>MAINTAINER</code>：</p>
<ul>
<li>镜像维护人的信息</li>
<li>MAINTAINER <name></name></li>
<li>示例：MAINTAINER Jartto <a href="mailto:Jartto@qq.com">Jartto@qq.com</a></li>
</ul>
<p><code>RUN</code>：</p>
<ul>
<li>构建镜像时要执行的命令</li>
<li>RUN <command></li>
<li>示例：RUN [executable, param1, param2]</li>
</ul>
<p><code>ADD</code>：</p>
<ul>
<li>将本地的文件添加复制到容器中去，压缩包会解压，可以访问网络上的文件，会自动下载</li>
<li>ADD <src> <dest></dest></src></li>
<li>示例：ADD *.js /app 添加 js 文件到容器中的 app 目录下</li>
</ul>
<p><code>COPY</code>：</p>
<ul>
<li>功能和 ADD 一样，只是复制，不会解压或者下载文件</li>
</ul>
<p><code>CMD</code>：</p>
<ul>
<li>启动容器后执行的命令，和 RUN 不一样，RUN 是在构建镜像是要运行的命令</li>
<li>当使用 docker run 运行容器的时候，这个可以在命令行被覆盖</li>
<li>示例：CMD [executable, param1, param2]</li>
</ul>
<p><code>ENTRYPOINT</code>：</p>
<ul>
<li>也是执行命令，和 CMD 一样，只是这个命令不会被命令行覆盖</li>
<li>ENTRYPOINT [executable, param1, param2]</li>
<li>示例：ENTRYPOINT [donnet, myapp.dll]</li>
</ul>
<p><code>LABEL：为镜像添加元数据，key-value 形式</code></p>
<ul>
<li>LABEL <key>=<value> <key>=<value> …</value></key></value></key></li>
<li>示例：LABEL version=1.0 description=这是一个web应用</li>
</ul>
<p><code>ENV：设置环境变量，有些容器运行时会需要某些环境变量</code></p>
<ul>
<li>ENV <key> <value> 一次设置一个环境变量</value></key></li>
<li>ENV <key>=<value> <key>=<value> <key>=<value> 设置多个环境变量</value></key></value></key></value></key></li>
<li>示例：ENV JAVA_HOME /usr/java1.8/</li>
</ul>
<p><code>EXPOSE：暴露对外的端口（容器内部程序的端口，虽然会和宿主机的一样，但是其实是两个端口）</code></p>
<ul>
<li>EXPOSE <port></port></li>
<li>示例：EXPOSE 80</li>
<li>容器运行时，需要用 -p 映射外部端口才能访问到容器内的端口</li>
</ul>
<p><code>VOLUME：指定数据持久化的目录，官方语言叫做挂载</code></p>
<ul>
<li>VOLUME /var/log 指定容器中需要被挂载的目录，会把这个目录映射到宿主机的一个随机目录上，实现数据的持久化和同步</li>
<li>VOLUME [/var/log,/var/test…..] 指定容器中多个需要被挂载的目录，会把这些目录映射到宿主机的多个随机目录上，实现数据的持久化和同步</li>
<li>VOLUME /var/data var/log 指定容器中的 var/log 目录挂载到宿主机上的 /var/data 目录，这种形式可以手动指定宿主机上的目录</li>
</ul>
<p><code>WORKDIR：设置工作目录，设置之后 ，RUN、CMD、COPY、ADD 的工作目录都会同步变更</code></p>
<ul>
<li>WORKDIR <path></path></li>
<li>示例：WORKDIR /app/test</li>
</ul>
<p><code>USER：指定运行命令时所使用的用户，为了安全和权限起见，根据要执行的命令选择不同用户</code></p>
<ul>
<li>USER <user>:[<group>]</group></user></li>
<li>示例：USER test</li>
</ul>
<p><code>ARG：设置构建镜像是要传递的参数</code></p>
<ul>
<li>ARG <name>[=<value>]</value></name></li>
<li>ARG name=sss</li>
</ul>
<p><code>更多操作，请移步官方使用文档[3]</code>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;docs.docker.com&#x2F;</span><br></pre></td></tr></table></figure>

<p>最佳实践</p>
<p>在掌握 Docker 常规操作之后，我们很容易就可以打出自己想要的项目镜像。</p>
<p>然而不同的操作打出的镜像也是千差万别。究竟是什么原因导致镜像差异，我们不妨继续探索。</p>
<p><code>以下是在应用 Docker 过程中整理的最佳实践，请尽量遵循如下准则</code>：</p>
<ul>
<li><strong>Require 明确：</strong>需要什么镜像。</li>
<li><strong>步骤精简：</strong>变化较少的 Step 优先。</li>
<li><strong>版本明确：</strong>镜像命名明确。</li>
<li><strong>说明文档：</strong>整个镜像打包步骤可以重现。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>容器化技术必将是云时代不可或缺的技能之一，而 Docker 只是沧海一粟。随之而来的还有集群容器管理 Kubernetes、Service Mesh 、Istio 等技术。</p>
<p>打开 Docker 的大门，不断抽丝剥茧，逐层深入，你将感受到容器化的无穷魅力。</p>
<p><em><code>相关链接</code>：</em></p>
<ul>
<li><p><em><a href="https://www.docker.com/products/docker-desktop" target="_blank" rel="noopener">https://www.docker.com/products/docker-desktop</a></em></p>
</li>
<li><p><em><a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a></em></p>
</li>
<li><p><em><a href="https://docs.docker.com/" target="_blank" rel="noopener">https://docs.docker.com/</a></em></p>
<p>转载自:<a href="https://mp.weixin.qq.com/s/8p8t-rQqNIG8OVKDAUyc8w" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/8p8t-rQqNIG8OVKDAUyc8w</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark数据倾斜处理方法总结</title>
    <url>/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="Spark性能优化之道——解决Spark数据倾斜（Data-Skew）的N种姿势"><a href="#Spark性能优化之道——解决Spark数据倾斜（Data-Skew）的N种姿势" class="headerlink" title="Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势"></a>Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势</h2><p>本文结合实例详细阐明了Spark数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义Partitioner，使用Map侧Join代替Reduce侧Join，给倾斜Key加上随机前缀等。</p>
<blockquote>
<p>  本文转发自技术世界，原文链接<a href="http://www.jasongj.com/spark/skew/" target="_blank" rel="noopener">http://www.jasongj.com/spark/skew/</a></p>
</blockquote>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文结合实例详细阐明了Spark数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义Partitioner，使用Map侧Join代替Reduce侧Join，给倾斜Key加上随机前缀等。</p>
<h1 id="为何要处理数据倾斜（Data-Skew）"><a href="#为何要处理数据倾斜（Data-Skew）" class="headerlink" title="为何要处理数据倾斜（Data Skew）"></a>为何要处理数据倾斜（Data Skew）</h1><h2 id="什么是数据倾斜"><a href="#什么是数据倾斜" class="headerlink" title="什么是数据倾斜"></a>什么是数据倾斜</h2><p>对Spark/Hadoop这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。</p>
<p>何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p>
<p>对于分布式系统而言，理想情况下，随着系统规模（节点数量）的增加，应用整体耗时线性下降。如果一台机器处理一批大量数据需要120分钟，当机器数量增加到三时，理想的耗时为120 / 3 = 40分钟，如下图所示</p>
<p>但是，上述情况只是理想情况，实际上将单机任务转换成分布式任务后，会有overhead，使得总的任务量较之单机时有所增加，所以每台机器的执行时间加起来比单台机器时更大。这里暂不考虑这些overhead，假设单机任务转换成分布式任务后，总任务量不变。<br>　　<br>但即使如此，想做到分布式情况下每台机器执行时间是单机时的，就必须保证每台机器的任务量相等。不幸的是，很多时候，任务的分配是不均匀的，甚至不均匀到大部分任务被分配到个别机器上，其它大部分机器所分配的任务量只占总得的小部分。比如一台机器负责处理80%的任务，另外两台机器各处理10%的任务，如下图所示<br><a href="http://www.jasongj.com/img/spark/spark1_skew/skew_time.png" target="_blank" rel="noopener"><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/skew_time.png" alt="unideal scale out"></a></p>
<a id="more"></a>　　
<p>在上图中，机器数据增加为三倍，但执行时间只降为原来的80%，远低于理想值。 　　</p>
<h2 id="数据倾斜的危害"><a href="#数据倾斜的危害" class="headerlink" title="数据倾斜的危害"></a>数据倾斜的危害</h2><p>从上图可见，当出现数据倾斜时，小量任务耗时远高于其它任务，从而使得整体耗时过大，未能充分发挥分布式系统的并行计算优势。<br>　　<br>另外，当发生数据倾斜时，部分任务处理的数据量过大，可能造成内存不足使得任务失败，并进而引进整个应用失败。 　　</p>
<h2 id="数据倾斜是如何造成的"><a href="#数据倾斜是如何造成的" class="headerlink" title="数据倾斜是如何造成的"></a>数据倾斜是如何造成的</h2><p>在Spark中，同一个Stage的不同Partition可以并行处理，而具有依赖关系的不同Stage之间是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。</p>
<p>由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。</p>
<p>Stage的数据来源主要分为如下两类</p>
<ul>
<li>从数据源直接读取。如读取HDFS，Kafka</li>
<li>读取上一个Stage的Shuffle数据</li>
</ul>
<h1 id="如何缓解-消除数据倾斜"><a href="#如何缓解-消除数据倾斜" class="headerlink" title="如何缓解/消除数据倾斜"></a>如何缓解/消除数据倾斜</h1><h2 id="避免数据源的数据倾斜-—读Kafka"><a href="#避免数据源的数据倾斜-—读Kafka" class="headerlink" title="避免数据源的数据倾斜 —读Kafka"></a>避免数据源的数据倾斜 —读Kafka</h2><p>以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。</p>
<p>如《<a href="http://www.jasongj.com/2015/03/10/KafkaColumn1/#Producer消息路由" target="_blank" rel="noopener">Kafka设计解析（一）- Kafka背景及架构介绍</a>》一文所述，Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。</p>
<p>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。</p>
<h2 id="避免数据源的数据倾斜-—读文件"><a href="#避免数据源的数据倾斜-—读文件" class="headerlink" title="避免数据源的数据倾斜 —读文件"></a>避免数据源的数据倾斜 —读文件</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>Spark以通过<code>textFile(path, minPartitions)</code>方法读取文件时，使用TextFileFormat。</p>
<p>对于不可切分的文件，每个文件对应一个Split从而对应一个Partition。此时各文件大小是否一致，很大程度上决定了是否存在数据源侧的数据倾斜。另外，对于不可切分的压缩文件，即使压缩后的文件大小一致，它所包含的实际数据量也可能差别很多，因为源文件数据重复度越高，压缩比越高。反过来，即使压缩文件大小接近，但由于压缩比可能差距很大，所需处理的数据量差距也可能很大。</p>
<p>此时可通过在数据生成端将不可切分文件存储为可切分文件，或者保证各文件包含数据量相同的方式避免数据倾斜。</p>
<p>对于可切分的文件，每个Split大小由如下算法决定。其中goalSize等于所有文件总大小除以minPartitions。而blockSize，如果是HDFS文件，由文件本身的block大小决定；如果是Linux本地文件，且使用本地模式，由<code>fs.local.block.size</code>决定。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>默认情况下各Split的大小不会太大，一般相当于一个Block大小（在Hadoop 2中，默认值为128MB），所以数据倾斜问题不明显。如果出现了严重的数据倾斜，可通过上述参数调整。</p>
<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><p>现通过脚本生成一些文本文件，并通过如下代码进行简单的单词计数。为避免Shuffle，只计单词总个数，不须对单词进行分组计数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SparkConf sparkConf &#x3D; new SparkConf()</span><br><span class="line">    .setAppName(&quot;ReadFileSkewDemo&quot;);</span><br><span class="line">JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line">long count &#x3D; javaSparkContext.textFile(inputFile, minPartitions)</span><br><span class="line">    .flatMap((String line) -&gt; Arrays.asList(line.split(&quot; &quot;)).iterator()).count();</span><br><span class="line">System.out.printf(&quot;total words : %s&quot;, count);</span><br><span class="line">javaSparkContext.stop();</span><br></pre></td></tr></table></figure>



<p>总共生成如下11个csv文件，其中10个大小均为271.9MB，另外一个大小为8.5GB。<br><a href="http://www.jasongj.com/img/spark/spark1_skew/uncompressedfiles.png" target="_blank" rel="noopener"><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/uncompressedfiles.png" alt="uncompressed files"></a></p>
<p>之后将8.5GB大小的文件使用gzip压缩，压缩后大小仅为25.3MB。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/compressedfiles.png" alt="compressed files">]</p>
<p>使用如上代码对未压缩文件夹进行单词计数操作。Split大小为 max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9 <em>10+8.5</em> 1024) / 1 MB, 128 MB) = 128MB。无明显数据倾斜。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/splitable_unskewed.png" alt="splitable_unskewed">]</p>
<p>使用同样代码对包含压缩文件的文件夹进行同样的单词计数操作。未压缩文件的Split大小仍然为128MB，而压缩文件（gzip压缩）由于不可切分，且大小仅为25.3MB，因此该文件作为一个单独的Split/Partition。虽然该文件相对较小，但是它由8.5GB文件压缩而来，包含数据量是其它未压缩文件的32倍，因此处理该Split/Partition/文件的Task耗时为4.4分钟，远高于其它Task的10秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/compressedfileskew.png" alt="compressed file skew">]</p>
<p>由于上述gzip压缩文件大小为25.3MB，小于128MB的Split大小，不能证明gzip压缩文件不可切分。现将minPartitions从默认的1设置为229，从而目标Split大小为max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9 * 10+25.3) / 229 MB, 128 MB) = 12 MB。如果gzip压缩文件可切分，则所有Split/Partition大小都不会远大于12。反之，如果仍然存在25.3MB的Partition，则说明gzip压缩文件确实不可切分，在生成不可切分文件时需要如上文所述保证各文件数量大大致相同。</p>
<p>如下图所示，gzip压缩文件对应的Split/Partition大小为25.3MB，其它Split大小均为12MB左右。而该Task耗时4.7分钟，远大于其它Task的4秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/unsplitable_skew.png" alt="compressed unsplitable file skew">]</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>数据源侧存在不可切分文件，且文件内包含的数据量相差较大。</p>
<p><strong><em>解决方案\</em></strong><br>尽量使用可切分的格式代替不可切分的格式，或者保证各文件实际包含数据量大致相同。</p>
<p><strong><em>优势\</em></strong><br>可撤底消除数据源侧数据倾斜，效果显著。</p>
<p><strong><em>劣势\</em></strong><br>数据源一般来源于外部系统，需要外部系统的支持。</p>
<h2 id="调整并行度分散同一个Task的不同Key"><a href="#调整并行度分散同一个Task的不同Key" class="headerlink" title="调整并行度分散同一个Task的不同Key"></a>调整并行度分散同一个Task的不同Key</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。</p>
<p>如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/changeparallelism.png" alt="spark change parallelism">]</p>
<h3 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h3><p>现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5亿条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CASE WHEN id &lt; 940000000 THEN (9500000  + (CAST (RAND() * 8 AS INTEGER)) * 12 )</span><br><span class="line">       ELSE CAST(id&#x2F;100 AS INTEGER)</span><br><span class="line">       END,</span><br><span class="line">       name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br></pre></td></tr></table></figure>

<p>通过上述处理，一份可能造成后续数据倾斜的测试数据即以准备好。接下来，使用Spark读取该测试数据，并通过<code>groupByKey(12)</code>对id分组处理，且Shuffle并行度为12。代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SparkDataSkew &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkSession sparkSession &#x3D; SparkSession.builder()</span><br><span class="line">      .appName(&quot;SparkDataSkewTunning&quot;)</span><br><span class="line">      .config(&quot;hive.metastore.uris&quot;, &quot;thrift:&#x2F;&#x2F;hadoop1:9083&quot;)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate();</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; dataframe &#x3D; sparkSession.sql( &quot;select * from test&quot;);</span><br><span class="line">    dataframe.toJavaRDD()</span><br><span class="line">      .mapToPair((Row row) -&gt; new Tuple2&lt;Integer, String&gt;(row.getInt(0),row.getString(1)))</span><br><span class="line">      .groupByKey(12)</span><br><span class="line">      .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123;</span><br><span class="line">        int id &#x3D; tuple._1();</span><br><span class="line">        AtomicInteger atomicInteger &#x3D; new AtomicInteger(0);</span><br><span class="line">        tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        return new Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get());</span><br><span class="line">      &#125;).count();</span><br><span class="line"></span><br><span class="line">      sparkSession.stop();</span><br><span class="line">      sparkSession.close();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本次实验所使用集群节点数为4，每个节点可被Yarn使用的CPU核数为16，内存为16GB。使用如下方式提交上述应用，将启动4个Executor，每个Executor可使用核数为12（该配置并非生产环境下的最优配置，仅用于本文实验），可用内存为12GB。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar</span><br></pre></td></tr></table></figure>



<p>GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为4500万，远大于（9倍于）其它11个Task处理的500万记录。而Task 8所耗费的时间为38秒，远高于其它11个Task的平均时间（16秒）。整个Stage的时间也为38秒，该时间主要由最慢的Task 8决定。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew12.png" alt="data skew">]</p>
<p>在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。</p>
<p>通过<code>groupByKey(48)</code>将Shuffle并行度调整为48，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew48.png" alt="add parallelism">]</p>
<p>从上图可知，记录数最多的Task 20处理的记录数约为1125万，相比于并行度为12时Task 8的4500万，降低了75%左右，而其耗时从原来Task 8的38秒降到了24秒。</p>
<p>在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过<code>groupByKey(11)</code>将Shuffle并行度调整为11，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/differentkeyskew11.png" alt="reduce parallelism">]</p>
<p>从上图可见，处理记录数最多的Task 6所处理的记录数约为1045万，耗时为23秒。处理记录数最少的Task 1处理的记录数约为545万，耗时12秒。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong><em>解决方案\</em></strong><br>调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。</p>
<p><strong><em>优势\</em></strong><br>实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用<code>spark.default.parallelism</code>设置。如果是Spark SQL，还可通过<code>SET spark.sql.shuffle.partitions=[num_tasks]</code>设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p>
<p><strong><em>劣势\</em></strong><br>适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p>
<h2 id="自定义Partitioner"><a href="#自定义Partitioner" class="headerlink" title="自定义Partitioner"></a>自定义Partitioner</h2><h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。</p>
<h3 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h3><p>以上述数据集为例，继续将并发度设置为12，但是在<code>groupByKey</code>算子上，使用自定义的<code>Partitioner</code>（实现如下）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.groupByKey(new Partitioner() &#123;</span><br><span class="line">  @Override</span><br><span class="line">  public int numPartitions() &#123;</span><br><span class="line">    return 12;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  @Override</span><br><span class="line">  public int getPartition(Object key) &#123;</span><br><span class="line">    int id &#x3D; Integer.parseInt(key.toString());</span><br><span class="line">    if(id &gt;&#x3D; 9500000 &amp;&amp; id &lt;&#x3D; 9500084 &amp;&amp; ((id - 9500000) % 12) &#x3D;&#x3D; 0) &#123;</span><br><span class="line">      return (id - 9500000) &#x2F; 12;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return id % 12;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>



<p>由下图可见，使用自定义Partition后，耗时最长的Task 6处理约1000万条数据，用时15秒。并且各Task所处理的数据集大小相当。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/customizedpartition.png" alt="customizec partitioner">]</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong><em>解决方案\</em></strong><br>使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p>
<p><strong><em>优势\</em></strong><br>不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p>
<p><strong><em>劣势\</em></strong><br>适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p>
<h2 id="将Reduce-side-Join转变为Map-side-Join"><a href="#将Reduce-side-Join转变为Map-side-Join" class="headerlink" title="将Reduce side Join转变为Map side Join"></a>将Reduce side Join转变为Map side Join</h2><h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoin.png" alt="spark map join">]</p>
<h3 id="案例-3"><a href="#案例-3" class="headerlink" title="案例"></a>案例</h3><p>通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CAST(CASE WHEN id &lt; 980000000 THEN (95000000  + (CAST (RAND() * 4 AS INT) + 1) * 48 )</span><br><span class="line">       ELSE CAST(id&#x2F;10 AS INT) END AS STRING),</span><br><span class="line">       name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br></pre></td></tr></table></figure>



<p>使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test_new</span><br><span class="line">SELECT CAST(CAST(id&#x2F;10 AS INT) AS STRING),</span><br><span class="line">       name</span><br><span class="line">FROM student_delta_external</span><br><span class="line">WHERE id BETWEEN 950000000 AND 950500000;</span><br></pre></td></tr></table></figure>



<p>直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>该SQL对应的DAG如下图所示。从该图可见，该执行过程总共分为三个Stage，前两个用于从Hive中读取数据，同时二者进行Shuffle，通过最后一个Stage进行Join并将结果写入表test_join中。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoindag.png" alt="reduce join DAG">]</p>
<p>从下图可见，Join Stage各Task处理的数据倾斜严重，处理数据量最大的Task耗时7.1分钟，远高于其它无数据倾斜的Task约2秒的耗时。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoinlaststage.png" alt="reduce join DAG">]</p>
<p>接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过<code>CACHE TABLE test_new</code>将小表test_new进行cache。现通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CACHE TABLE test_new;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>通过如下DAG图可见，该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoincachedag.png" alt="reduce join DAG">]</p>
<p>并且数据倾斜仍然存在。如下图所示，最慢的Task耗时为7.1分钟，远高于其它Task的约2秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/reducejoincachelaststage.png" alt="reduce join DAG">]</p>
<p>正确的使用Broadcast实现Map侧Join的方式是，通过<code>SET spark.sql.autoBroadcastJoinThreshold=104857600;</code>将Broadcast的阈值设置得足够大。</p>
<p>再次通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET spark.sql.autoBroadcastJoinThreshold&#x3D;104857600;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>



<p>通过如下DAG图可见，该方案只包含一个Stage。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoindag.png" alt="reduce join DAG">]</p>
<p>并且从下图可见，各Task耗时相当，无明显数据倾斜现象。并且总耗时为1.5分钟，远低于Reduce侧Join的7.3分钟。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/mapjoinlaststage.png" alt="reduce join DAG">]</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。</p>
<p><strong><em>解决方案\</em></strong><br>在Java/Scala代码中将小数据集数据拉取到Driver，然后通过Broadcast方案将小数据集的数据广播到各Executor。或者在使用SQL前，将Broadcast的阈值调整得足够大，从而使用Broadcast生效。进而将Reduce侧Join替换为Map侧Join。</p>
<p><strong><em>优势\</em></strong><br>避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p>
<p><strong><em>劣势\</em></strong><br>要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。</p>
<h2 id="为skew的key增加随机前-后缀"><a href="#为skew的key增加随机前-后缀" class="headerlink" title="为skew的key增加随机前/后缀"></a>为skew的key增加随机前/后缀</h2><h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/randomprefix.png" alt="spark random prefix">]</p>
<h3 id="案例-4"><a href="#案例-4" class="headerlink" title="案例"></a>案例</h3><p>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。</p>
<p>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000  + (CAST (RAND() * 2 AS INT) + 1) * 48 )</span><br><span class="line">  ELSE CAST(id&#x2F;100 AS INT) END AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE TABLE test_new</span><br><span class="line">SELECT CAST(CAST(id&#x2F;100 AS INT) AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_delta_external</span><br><span class="line">WHERE id BETWEEN 950000000 AND 950500000;</span><br></pre></td></tr></table></figure>

<p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">    sparkConf.setAppName(&quot;DemoSparkDataFrameWithSkewedBigTableDirect&quot;);</span><br><span class="line">    sparkConf.set(&quot;spark.default.parallelism&quot;, String.valueOf(parallelism));</span><br><span class="line">    JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">          return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    leftRDD.join(rightRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()))</span><br><span class="line">      .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">        AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinallstage.png" alt="few skewed key join">]</p>
<p>通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinlaststage.png" alt="few skewed key join">]</p>
<p>现通过如下操作，实现倾斜Key的分散处理</p>
<ul>
<li>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD</li>
<li>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD</li>
<li>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD</li>
<li>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD</li>
<li>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD</li>
<li>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集</li>
</ul>
<p>具体实现代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">      int parallelism &#x3D; 48;</span><br><span class="line">      SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">      sparkConf.setAppName(&quot;SolveDataSkewWithRandomPrefix&quot;);</span><br><span class="line">      sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);</span><br><span class="line">      JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">      JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">        .mapToPair((String row) -&gt; &#123;</span><br><span class="line">          String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">            return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">          .mapToPair((String row) -&gt; &#123;</span><br><span class="line">            String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">              return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">          &#125;);</span><br><span class="line"></span><br><span class="line">        String[] skewedKeyArray &#x3D; new String[]&#123;&quot;9500048&quot;, &quot;9500096&quot;&#125;;</span><br><span class="line">        Set&lt;String&gt; skewedKeySet &#x3D; new HashSet&lt;String&gt;();</span><br><span class="line">        List&lt;String&gt; addList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">        for(int i &#x3D; 1; i &lt;&#x3D;24; i++) &#123;</span><br><span class="line">            addList.add(i + &quot;&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        for(String key : skewedKeyArray) &#123;</span><br><span class="line">            skewedKeySet.add(key);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys &#x3D; javaSparkContext.broadcast(skewedKeySet);</span><br><span class="line">        Broadcast&lt;List&lt;String&gt;&gt; addListKeys &#x3D; javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftSkewRDD &#x3D; leftRDD</span><br><span class="line">          .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + &quot;,&quot; + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightSkewRDD &#x3D; rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">          .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))</span><br><span class="line">          .collect(Collectors.toList())</span><br><span class="line">          .iterator()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; skewedJoinRDD &#x3D; leftSkewRDD</span><br><span class="line">          .join(rightSkewRDD, parallelism)</span><br><span class="line">          .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftUnSkewRDD &#x3D; leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));</span><br><span class="line">        JavaPairRDD&lt;String, String&gt; unskewedJoinRDD &#x3D; leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">          AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.stop();</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。<br>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyrandomjoinallstage.png" alt="few skewed key join">]</p>
<p>通过分析Join Stage的所有Task可知</p>
<ul>
<li>由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96</li>
<li>由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始）</li>
<li>由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况</li>
</ul>
<p>[<img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/fewskewkeyjoinrandomlaststage.png" alt="few skewed key join">]</p>
<p>实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。</p>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p>
<p><strong><em>解决方案\</em></strong><br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p>
<p><strong><em>优势\</em></strong><br>相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p>
<p><strong><em>劣势\</em></strong><br>如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p>
<h2 id="大表随机添加N种随机前缀，小表扩大N倍"><a href="#大表随机添加N种随机前缀，小表扩大N倍" class="headerlink" title="大表随机添加N种随机前缀，小表扩大N倍"></a>大表随机添加N种随机前缀，小表扩大N倍</h2><h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p>如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。<br><img src="/2020/08/24/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/randomprefixandenlargesmalltable.png" alt="spark random prefix"></p>
<h3 id="案例-5"><a href="#案例-5" class="headerlink" title="案例"></a>案例</h3><p>这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class SparkDataSkew &#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">    sparkConf.setAppName(&quot;ResolveDataSkewWithNAndRandom&quot;);</span><br><span class="line">    sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);</span><br><span class="line">    JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; addList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">    for(int i &#x3D; 1; i &lt;&#x3D;48; i++) &#123;</span><br><span class="line">      addList.add(i + &quot;&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Broadcast&lt;List&lt;String&gt;&gt; addListKeys &#x3D; javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRandomRDD &#x3D; leftRDD.mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(new Random().nextInt(48) + &quot;,&quot; + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightNewRDD &#x3D; rightRDD</span><br><span class="line">      .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">      .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))</span><br><span class="line">      .collect(Collectors.toList())</span><br><span class="line">      .iterator()</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; joinRDD &#x3D; leftRandomRDD</span><br><span class="line">      .join(rightNewRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">    joinRDD.foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">      AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">      iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p><strong><em>适用场景\</em></strong><br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。</p>
<p><strong><em>优势\</em></strong><br>对大部分场景都适用，效果不错。</p>
<p><strong><em>劣势\</em></strong><br>需要将一个数据集整体扩大N倍，会增加资源消耗。</p>
<h1 id="总结-6"><a href="#总结-6" class="headerlink" title="总结"></a>总结</h1><p>对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive SQl</title>
    <url>/2020/09/01/Hive-SQl/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="添加分区"><a href="#添加分区" class="headerlink" title="添加分区"></a>添加分区</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">ALTER TABLE table_name ADD <span class="title">PARTITION</span> <span class="params">(partCol = <span class="string">'value1'</span>)</span> location 'loc1'</span>; <span class="comment">//示例</span></span><br><span class="line"></span><br><span class="line"><span class="function">ALTER TABLE table_name ADD IF NOT EXISTS <span class="title">PARTITION</span> <span class="params">(dt=<span class="string">'20130101'</span>)</span> LOCATION '/user/hadoop/warehouse/table_name/dt</span>=<span class="number">20130101</span><span class="string">'; //一次添加一个分区</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ALTER TABLE page_view ADD PARTITION (dt='</span><span class="number">2008</span>-<span class="number">08</span>-<span class="number">08</span><span class="string">', country='</span>us<span class="string">') location '</span>/path/to/us/part080808<span class="string">' PARTITION (dt='</span><span class="number">2008</span>-<span class="number">08</span>-<span class="number">09</span><span class="string">', country='</span>us<span class="string">') location '</span>/path/to/us/part080809<span class="string">';  //一次添加多个分区</span></span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h3 id="删除分区"><a href="#删除分区" class="headerlink" title="删除分区"></a>删除分区</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE login DROP IF EXISTS PARTITION (dt&#x3D;&#39;2008-08-08&#39;);</span><br><span class="line"></span><br><span class="line">ALTER TABLE page_view DROP IF EXISTS PARTITION (dt&#x3D;&#39;2008-08-08&#39;, country&#x3D;&#39;us&#39;);</span><br></pre></td></tr></table></figure>

<h3 id="修改分区"><a href="#修改分区" class="headerlink" title="修改分区"></a>修改分区</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">ALTER TABLE table_name <span class="title">PARTITION</span> <span class="params">(dt=<span class="string">'2008-08-08'</span>)</span> SET LOCATION "new location"</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">ALTER TABLE table_name <span class="title">PARTITION</span> <span class="params">(dt=<span class="string">'2008-08-08'</span>)</span> RENAME TO <span class="title">PARTITION</span> <span class="params">(dt=<span class="string">'20080808'</span>)</span></span>;</span><br></pre></td></tr></table></figure>

<h3 id="添加列"><a href="#添加列" class="headerlink" title="添加列"></a>添加列</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE table_name ADD COLUMNS (col_name STRING);  &#x2F;&#x2F;在所有存在的列后面，但是在分区列之前添加一列</span><br></pre></td></tr></table></figure>

<h3 id="修改列"><a href="#修改列" class="headerlink" title="修改列"></a>修改列</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function">CREATE TABLE <span class="title">test_change</span> <span class="params">(a <span class="keyword">int</span>, b <span class="keyword">int</span>, c <span class="keyword">int</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// will change column a's name to a1</span></span><br><span class="line"></span><br><span class="line">ALTER TABLE test_change CHANGE a a1 INT; </span><br><span class="line"></span><br><span class="line"><span class="comment">// will change column a's name to a1, a's data type to string, and put it after column b. The new table's structure is: b int, a1 string, c int</span></span><br><span class="line"></span><br><span class="line">ALTER TABLE test_change CHANGE a a1 STRING AFTER b; </span><br><span class="line"></span><br><span class="line"><span class="comment">// will change column b's name to b1, and put it as the first column. The new table's structure is: b1 int, a string, c int</span></span><br><span class="line"></span><br><span class="line">ALTER TABLE test_change CHANGE b b1 INT FIRST;</span><br></pre></td></tr></table></figure>

<h3 id="修改表属性"><a href="#修改表属性" class="headerlink" title="修改表属性:"></a>修改表属性:</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alter table table_name set TBLPROPERTIES (&#39;EXTERNAL&#39;&#x3D;&#39;TRUE&#39;);  &#x2F;&#x2F;内部表转外部表 </span><br><span class="line"></span><br><span class="line">alter table table_name set TBLPROPERTIES (&#39;EXTERNAL&#39;&#x3D;&#39;FALSE&#39;);  &#x2F;&#x2F;外部表转内部表</span><br></pre></td></tr></table></figure>

<h3 id="表的重命名"><a href="#表的重命名" class="headerlink" title="表的重命名"></a>表的重命名</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name</span><br></pre></td></tr></table></figure>

<h3 id="get-json-object-string-json-string-string-path"><a href="#get-json-object-string-json-string-string-path" class="headerlink" title="get_json_object(string json_string, string path)"></a>get_json_object(string json_string, string path)</h3><p>说明：<br>第一个参数填写json对象变量，第二个参数使用$表示json变量标识，然后用 . 或 [] 读取对象或数组；如果输入的json字符串无效，那么返回NULL。<br>每次只能返回一个数据项。</p>
<p>举例：<br>data 为 test表中的字段，数据结构如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data &#x3D;</span><br><span class="line">&#123;</span><br><span class="line"> &quot;store&quot;:</span><br><span class="line">        &#123;</span><br><span class="line">         &quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;, &#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],  </span><br><span class="line">         &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125;</span><br><span class="line">         &#125;, </span><br><span class="line"> &quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;, </span><br><span class="line"> &quot;owner&quot;:&quot;amy&quot; </span><br><span class="line">&#125;12345678910</span><br></pre></td></tr></table></figure>

<p>1.get单层值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select  get_json_object(data, &#39;$.owner&#39;) from test;</span><br><span class="line">结果：amy12</span><br></pre></td></tr></table></figure>

<p>2.get多层值.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select  get_json_object(data, &#39;$.store.bicycle.price&#39;) from test;</span><br><span class="line">结果：19.9512</span><br></pre></td></tr></table></figure>

<p>3.get数组值[]</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select  get_json_object(data, &#39;$.store.fruit[0]&#39;) from test;</span><br><span class="line">结果：&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="json-解析"><a href="#json-解析" class="headerlink" title="json 解析"></a>json 解析</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">-- 测试json数据</span><br><span class="line">&#123;</span><br><span class="line">  &quot;store&quot;: &#123;</span><br><span class="line">    &quot;fruit&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;weight&quot;: 8,</span><br><span class="line">        &quot;type&quot;: &quot;apple&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;weight&quot;: 9,</span><br><span class="line">        &quot;type&quot;: &quot;pear&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;bicycle&quot;: &#123;</span><br><span class="line">      &quot;price&quot;: 19.951,</span><br><span class="line">      &quot;color&quot;: &quot;red1&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;email&quot;: &quot;amy@only_for_json_udf_test.net&quot;,</span><br><span class="line">  &quot;owner&quot;: &quot;amy1&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">-- 测试json数据</span><br><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;bssid&quot;: &quot;12:34:56:78:c4:90&quot;,</span><br><span class="line">    &quot;ssid&quot;: &quot;wifi-guest&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;bssid&quot;: &quot;12:34:56:78:c4:a9&quot;,</span><br><span class="line">    &quot;ssid&quot;: &quot;wifi-guest&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;bssid&quot;: &quot;12:34:56:78:c4:a8&quot;,</span><br><span class="line">    &quot;ssid&quot;: &quot;wifi-inc&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">-- 测试json数据</span><br><span class="line">&#123;</span><br><span class="line">  &quot;optional&quot;: &quot;unknown&quot;,</span><br><span class="line">  &quot;wifiList&quot;: &quot;[&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:85\&quot;,\&quot;ssid\&quot;:\&quot;Xiaomi_XS\&quot;&#125;,&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:86\&quot;,\&quot;ssid\&quot;:\&quot;Xiaomi_5G\&quot;&#125;,&#123;\&quot;bssid\&quot;:\&quot;7c:03:c9:e3:95:74\&quot;,\&quot;ssid\&quot;:\&quot;ChinaNet-HfVw\&quot;&#125;]&quot;,</span><br><span class="line">  &quot;IMSI&quot;: &quot;351036391715285&quot;,</span><br><span class="line">  &quot;dpi&quot;: &quot;588&quot;,</span><br><span class="line">  &quot;timestamp&quot;: &quot;1542365379014&quot;,</span><br><span class="line">  &quot;coordinates&quot;: &quot;36.733577|125.237956&quot;,</span><br><span class="line">  &quot;startupTime&quot;: &quot;1542391575&quot;,</span><br><span class="line">  &quot;totalMemory&quot;: &quot;5964230656&quot;,</span><br><span class="line">  &quot;availableMemory&quot;: &quot;1474846720&quot;,</span><br><span class="line">  &quot;serial&quot;: &quot;abcd&quot;,</span><br><span class="line">  &quot;currentWifi&quot;: &quot;[&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:85\&quot;,\&quot;rssi\&quot;:-34,\&quot;ssid\&quot;:\&quot;Xiaomi_XS\&quot;&#125;]&quot;,</span><br><span class="line">  &quot;systemVolume&quot;: &quot;83&quot;</span><br><span class="line">&#125;</span><br><span class="line">*&#x2F;</span><br></pre></td></tr></table></figure>

<p>&amp;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select get_json_object(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&quot;bicycle&quot;:&#123;&quot;price&quot;:19.951,&quot;color&quot;:&quot;red1&quot;&#125;&#125;,&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy1&quot;&#125;&#39;, &#39;$.owner&#39;);</span><br><span class="line">-- amy1</span><br><span class="line"></span><br><span class="line">select get_json_object(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&quot;bicycle&quot;:&#123;&quot;price&quot;:19.951,&quot;color&quot;:&quot;red1&quot;&#125;&#125;,&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy1&quot;&#125;&#39;, &#39;$.store.fruit[0].weight&#39;);</span><br><span class="line">-- 8</span><br><span class="line"></span><br><span class="line">select get_json_object(&#39;[&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:90&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a9&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a8&quot;,&quot;ssid&quot;:&quot;wifi-inc&quot;&#125;]&#39;, &#39;$.[0].ssid&#39;);</span><br><span class="line">-- wifi-guest</span><br><span class="line"></span><br><span class="line">select get_json_object(&#39;[&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:90&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a9&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a8&quot;,&quot;ssid&quot;:&quot;wifi-inc&quot;&#125;]&#39;, &#39;$.[*].ssid&#39;);</span><br><span class="line">-- [&quot;wifi-guest&quot;,&quot;wifi-guest&quot;,&quot;wifi-inc&quot;]</span><br></pre></td></tr></table></figure>

<p>&amp;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select get_json_object(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&quot;bicycle&quot;:&#123;&quot;price&quot;:19.951,&quot;color&quot;:&quot;red1&quot;&#125;&#125;,&quot;currentWifi&quot;:&quot;[&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:85\&quot;,\&quot;rssi\&quot;:-34,\&quot;ssid\&quot;:\&quot;Xiaomi_XS\&quot;&#125;]&quot;,&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy1&quot;&#125;&#39;, &#39;$.store.fruit[*].type&#39;);</span><br><span class="line">-- NULL</span><br><span class="line"></span><br><span class="line">select get_json_object(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&quot;bicycle&quot;:&#123;&quot;price&quot;:19.951,&quot;color&quot;:&quot;red1&quot;&#125;&#125;,&quot;currentWifi&quot;:[&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:90&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a9&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a8&quot;,&quot;ssid&quot;:&quot;wifi-inc&quot;&#125;],&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy1&quot;&#125;&#39;, &#39;$.store.fruit[*].type&#39;);</span><br><span class="line">-- [&quot;apple&quot;,&quot;pear&quot;]</span><br><span class="line">-- 我知道问题的原因了，它是支持嵌套json的，但必须要求value的值也是json类型，而不能是符合json格式的字符串，否则整体会报错（返回NULL）。</span><br><span class="line">-- 原因是 get_json_object 的第一个参数只支持json对象，而不是符合json格式的字符串，否则会将其作为非法json来处理，在取任何字段都会返回NULL。处理办法就是嵌套使用 get_json_object 进行提取。</span><br><span class="line"></span><br><span class="line">select get_json_object(data,&#39;$.timestamp&#39;),</span><br><span class="line">		get_json_object(data,&#39;$.currentWifi&#39;), -- [&#123;&quot;bssid&quot;:&quot;12:34:56:78:1f:7f&quot;,&quot;rssi&quot;:50,&quot;ssid&quot;:&quot;zero&quot;&#125;]</span><br><span class="line">		get_json_object(data,&#39;$.currentWifi[0]&#39;), -- null</span><br><span class="line">		get_json_object(data,&#39;$.currentWifi.[0]&#39;), -- null</span><br><span class="line">		get_json_object(get_json_object(data,&#39;$.currentWifi&#39;), &#39;$.[0].ssid&#39;), -- zero</span><br><span class="line">		data</span><br><span class="line">from db_name.table_name</span><br><span class="line">where</span><br><span class="line">	dt BETWEEN $$begindatekey and $$enddatekey and</span><br><span class="line">	data like &#39;%\\&quot;ssid\\&quot;:%&#39; -- 这里的双引号「&quot;」要用「\」进行转义</span><br><span class="line">;</span><br></pre></td></tr></table></figure>

<p>&amp;</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select json_tuple(&#39;&#123;&quot;optional&quot;:&quot;unknown&quot;,&quot;wifiList&quot;:&quot;[&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:85\&quot;,\&quot;ssid\&quot;:\&quot;Xiaomi_XS\&quot;&#125;,&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:86\&quot;,\&quot;ssid\&quot;:\&quot;Xiaomi_5G\&quot;&#125;,&#123;\&quot;bssid\&quot;:\&quot;7c:03:c9:e3:95:74\&quot;,\&quot;ssid\&quot;:\&quot;ChinaNet-HfVw\&quot;&#125;]&quot;,&quot;IMSI&quot;:&quot;351036391715285&quot;,&quot;dpi&quot;:&quot;588&quot;,&quot;timestamp&quot;:&quot;1542365379014&quot;,&quot;coordinates&quot;:&quot;36.733577|125.237956&quot;,&quot;startupTime&quot;:&quot;1542391575&quot;,&quot;totalMemory&quot;:&quot;5964230656&quot;,&quot;availableMemory&quot;:&quot;1474846720&quot;,&quot;serial&quot;:&quot;abcd&quot;,&quot;currentWifi&quot;:&quot;[&#123;\&quot;bssid\&quot;:\&quot;12:34:56:78:c4:85\&quot;,\&quot;rssi\&quot;:-34,\&quot;ssid\&quot;:\&quot;Xiaomi_XS\&quot;&#125;]&quot;,&quot;systemVolume&quot;:&quot;83&quot;&#125;&#39;, &#39;currentWifi&#39;);</span><br><span class="line">-- NULL</span><br><span class="line">-- json_tuple 的第一个参数只支持json对象，而不是符合json格式的字符串，否则会将其作为非法json来处理，在取任何字段都会返回NULL。</span><br><span class="line"></span><br><span class="line">select json_tuple(&#39;&#123;&quot;name&quot;:&quot;jack&quot;,&quot;server&quot;:&quot;www.qq.com&quot;&#125;&#39;,&#39;server&#39;,&#39;name&#39;);</span><br><span class="line">-- www.qq.com	jack</span><br><span class="line"></span><br><span class="line">select json_tuple(&#39;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;],&quot;bicycle&quot;:&#123;&quot;price&quot;:19.951,&quot;color&quot;:&quot;red1&quot;&#125;&#125;,&quot;currentWifi&quot;:[&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:90&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a9&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a8&quot;,&quot;ssid&quot;:&quot;wifi-inc&quot;&#125;],&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy1&quot;&#125;&#39;, &#39;currentWifi&#39;, &#39;currentWifi[0]&#39;);</span><br><span class="line">-- [&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:90&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a9&quot;,&quot;ssid&quot;:&quot;wifi-guest&quot;&#125;,&#123;&quot;bssid&quot;:&quot;12:34:56:78:c4:a8&quot;,&quot;ssid&quot;:&quot;wifi-inc&quot;&#125;]	NULL</span><br><span class="line">-- json_tuple 这个支持一次取多个key，但是不支持取嵌套的内容</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title>实时数仓学习笔记_1</title>
    <url>/2020/09/05/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script>]]></content>
  </entry>
  <entry>
    <title>Mysql Date_fromat时间函数</title>
    <url>/2020/09/09/Mysql-Date-fromat%E6%97%B6%E9%97%B4%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="MySQL-DATE-FORMAT-函数"><a href="#MySQL-DATE-FORMAT-函数" class="headerlink" title="MySQL DATE_FORMAT() 函数"></a>MySQL DATE_FORMAT() 函数</h1><h2 id="定义和用法"><a href="#定义和用法" class="headerlink" title="定义和用法"></a>定义和用法</h2><p>DATE_FORMAT() 函数用于以不同的格式显示日期/时间数据。</p>
<h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DATE_FORMAT(date,format)</span><br></pre></td></tr></table></figure>

<p><em>date</em> 参数是合法的日期。<em>format</em> 规定日期/时间的输出格式。</p>
<a id="more"></a>

<p>可以使用的格式有：</p>
<table>
<thead>
<tr>
<th align="left">格式</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="left">%a</td>
<td align="left">缩写星期名</td>
</tr>
<tr>
<td align="left">%b</td>
<td align="left">缩写月名</td>
</tr>
<tr>
<td align="left">%c</td>
<td align="left">月，数值</td>
</tr>
<tr>
<td align="left">%D</td>
<td align="left">带有英文前缀的月中的天</td>
</tr>
<tr>
<td align="left">%d</td>
<td align="left">月的天，数值(00-31)</td>
</tr>
<tr>
<td align="left">%e</td>
<td align="left">月的天，数值(0-31)</td>
</tr>
<tr>
<td align="left">%f</td>
<td align="left">微秒</td>
</tr>
<tr>
<td align="left">%H</td>
<td align="left">小时 (00-23)</td>
</tr>
<tr>
<td align="left">%h</td>
<td align="left">小时 (01-12)</td>
</tr>
<tr>
<td align="left">%I</td>
<td align="left">小时 (01-12)</td>
</tr>
<tr>
<td align="left">%i</td>
<td align="left">分钟，数值(00-59)</td>
</tr>
<tr>
<td align="left">%j</td>
<td align="left">年的天 (001-366)</td>
</tr>
<tr>
<td align="left">%k</td>
<td align="left">小时 (0-23)</td>
</tr>
<tr>
<td align="left">%l</td>
<td align="left">小时 (1-12)</td>
</tr>
<tr>
<td align="left">%M</td>
<td align="left">月名</td>
</tr>
<tr>
<td align="left">%m</td>
<td align="left">月，数值(00-12)</td>
</tr>
<tr>
<td align="left">%p</td>
<td align="left">AM 或 PM</td>
</tr>
<tr>
<td align="left">%r</td>
<td align="left">时间，12-小时（hh:mm:ss AM 或 PM）</td>
</tr>
<tr>
<td align="left">%S</td>
<td align="left">秒(00-59)</td>
</tr>
<tr>
<td align="left">%s</td>
<td align="left">秒(00-59)</td>
</tr>
<tr>
<td align="left">%T</td>
<td align="left">时间, 24-小时 (hh:mm:ss)</td>
</tr>
<tr>
<td align="left">%U</td>
<td align="left">周 (00-53) 星期日是一周的第一天</td>
</tr>
<tr>
<td align="left">%u</td>
<td align="left">周 (00-53) 星期一是一周的第一天</td>
</tr>
<tr>
<td align="left">%V</td>
<td align="left">周 (01-53) 星期日是一周的第一天，与 %X 使用</td>
</tr>
<tr>
<td align="left">%v</td>
<td align="left">周 (01-53) 星期一是一周的第一天，与 %x 使用</td>
</tr>
<tr>
<td align="left">%W</td>
<td align="left">星期名</td>
</tr>
<tr>
<td align="left">%w</td>
<td align="left">周的天 （0=星期日, 6=星期六）</td>
</tr>
<tr>
<td align="left">%X</td>
<td align="left">年，其中的星期日是周的第一天，4 位，与 %V 使用</td>
</tr>
<tr>
<td align="left">%x</td>
<td align="left">年，其中的星期一是周的第一天，4 位，与 %v 使用</td>
</tr>
<tr>
<td align="left">%Y</td>
<td align="left">年，4 位</td>
</tr>
<tr>
<td align="left">%y</td>
<td align="left">年，2 位</td>
</tr>
</tbody></table>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>下面的脚本使用 DATE_FORMAT() 函数来显示不同的格式。我们使用 NOW() 来获得当前的日期/时间：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DATE_FORMAT(NOW(),&#39;%b %d %Y %h:%i %p&#39;)</span><br><span class="line">DATE_FORMAT(NOW(),&#39;%m-%d-%Y&#39;)</span><br><span class="line">DATE_FORMAT(NOW(),&#39;%d %b %y&#39;)</span><br><span class="line">DATE_FORMAT(NOW(),&#39;%d %b %Y %T:%f&#39;)</span><br></pre></td></tr></table></figure>

<p>结果类似：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Dec 29 2008 11:45 PM</span><br><span class="line">12-29-2008</span><br><span class="line">29 Dec 08</span><br><span class="line">29 Dec 2008 16:25:46.635</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Mysql</category>
      </categories>
      <tags>
        <tag>函数</tag>
        <tag>时间函数</tag>
      </tags>
  </entry>
  <entry>
    <title>Sequel pro关闭后提示应用错误异常退出的应对方法</title>
    <url>/2020/09/11/Sequel-pro%E5%85%B3%E9%97%AD%E5%90%8E%E6%8F%90%E7%A4%BA%E5%BA%94%E7%94%A8%E9%94%99%E8%AF%AF%E5%BC%82%E5%B8%B8%E9%80%80%E5%87%BA%E7%9A%84%E5%BA%94%E5%AF%B9%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>刚下载mac下的Sequel pro ，关闭会话总是会报错。</p>
<p>尤其是想关闭一个回话的时候，整个程序退出还弹个错误对话框。</p>
<p>应该是适配问题.</p>
<a id="more"></a>

<p>可以使用测试版,亲测无弹窗</p>
<p><a href="https://sequelpro.com/test-builds" target="_blank" rel="noopener">https://sequelpro.com/test-builds</a></p>
<p><img src="/2020/09/11/Sequel-pro%E5%85%B3%E9%97%AD%E5%90%8E%E6%8F%90%E7%A4%BA%E5%BA%94%E7%94%A8%E9%94%99%E8%AF%AF%E5%BC%82%E5%B8%B8%E9%80%80%E5%87%BA%E7%9A%84%E5%BA%94%E5%AF%B9%E6%96%B9%E6%B3%95/image-20200911181432758.png" alt="image-20200911181432758"></p>
<p>终于不用再弹框了。</p>
]]></content>
      <categories>
        <category>Sequel pro</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive解析json方法</title>
    <url>/2020/09/14/Hive%E8%A7%A3%E6%9E%90json%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="1-get-json-object"><a href="#1-get-json-object" class="headerlink" title="1.get_json_object"></a>1.get_json_object</h4><p>底层日志经常会被储存为json字符串的形式，如果想获得各个维度的值，往往需要这个函数把对应的值取出来，具体用法为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">get_json_object(action,&#39;$.evt_id&#39;) 取出1111027379533</span><br><span class="line">此处假设需要解析的字段叫action，action的值如下：</span><br><span class="line">&#123;</span><br><span class="line">&quot;info_json&quot;: </span><br><span class="line">&#123;&quot;a&quot;: &#123;&quot;b&quot;: &quot;300&quot;,&quot;c&quot;: &quot;15000&quot;&#125;,</span><br><span class="line">&quot;d&quot;: &quot;110000&quot;,</span><br><span class="line">&quot;e&quot;: &quot;小花&quot;,</span><br><span class="line">&quot;f&quot;: &quot;小芳&quot;&#125;,</span><br><span class="line">&quot; token&quot;: &quot;1111027379533&quot;</span><br><span class="line">&#125;</span><br><span class="line">同样如果要取出d所对应的110000这个值，可用嵌套的get_json_object方法</span><br><span class="line"> get_json_object(get_json_object(</span><br><span class="line">&#39;&#123;</span><br><span class="line">&quot;info_json&quot;: </span><br><span class="line">&#123;&quot;a&quot;: &#123;&quot;b&quot;: &quot;300&quot;,&quot;c&quot;: &quot;15000&quot;&#125;,</span><br><span class="line">&quot;d&quot;: &quot;110000&quot;,</span><br><span class="line">&quot;e&quot;: &quot;小花&quot;,</span><br><span class="line">&quot;f&quot;: &quot;小芳&quot;&#125;,</span><br><span class="line">&quot;evt_id&quot;: &quot;1111027379533&quot;</span><br><span class="line">&#125;&#39;,&#39;$.info_json&#39;),&#39;$.d&#39;)</span><br><span class="line">1234567891011121314151617181920</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>需要注意的是，key值不要是中文，要不然hive会报错，而且，key值要用双引号括起来，json串的格式要规范。此外hive核心编程这本书中有提到一个json_tuple这个函数的用法，目前还没有用，先占个坑，回头再补。</p>
<h4 id="2-regexp-extract（）"><a href="#2-regexp-extract（）" class="headerlink" title="2.regexp_extract（）"></a>2.regexp_extract（）</h4><p>这个函数相信很多博客都有提及，主要和正则表达式一起使用，以提取自己想要的那部分数据.regexp_extract(str, regexp[, idx]) - extracts a group that matches regexp<br>字符串正则表达式解析函数。<br>– 这个函数有点类似于 substring(str from ‘regexp’) ..<br>参数解释:<br>其中：<br>str是被解析的字符串<br>regexp 是正则表达式<br>idx是返回结果 取表达式的哪一部分 默认值为1。<br>0表示把整个正则表达式对应的结果全部返回<br>1表示返回正则表达式中第一个() 对应的结果 以此类推<br>注意点：<br>要注意的是idx的数字不能大于表达式中()的个数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select regexp_extract(&#39;x&#x3D;a3&amp;x&#x3D;18abc&amp;x&#x3D;2&amp;y&#x3D;3&amp;x&#x3D;4&#39;,&#39;x&#x3D;([0-9]+)([a-z]+)&#39;,0) from default.dual;</span><br><span class="line">得到的结果为:</span><br><span class="line">x&#x3D;18abc</span><br><span class="line">select regexp_extract(&#39;x&#x3D;a3&amp;x&#x3D;18abc&amp;x&#x3D;2&amp;y&#x3D;3&amp;x&#x3D;4&#39;,&#39;x&#x3D;([0-9]+)([a-z]+)&#39;,1) from default.dual;</span><br><span class="line">得到的结果为:</span><br><span class="line">18</span><br><span class="line">select regexp_extract(&#39;x&#x3D;a3&amp;x&#x3D;18abc&amp;x&#x3D;2&amp;y&#x3D;3&amp;x&#x3D;4&#39;,&#39;x&#x3D;([0-9]+)([a-z]+)&#39;,2) from default.dual;</span><br><span class="line">得到的结果为:</span><br><span class="line">abc</span><br><span class="line">我们当前的语句只有2个()表达式 所以当idx&gt;&#x3D;3的时候 就会报错12345678910</span><br></pre></td></tr></table></figure>

<p>此函数用法引自：<a href="http://blog.csdn.net/lxpbs8851/article/details/39202735" target="_blank" rel="noopener">http://blog.csdn.net/lxpbs8851/article/details/39202735</a></p>
<h4 id="3-trim（去除空格）-lower（取小写）-upper（取大写）-split"><a href="#3-trim（去除空格）-lower（取小写）-upper（取大写）-split" class="headerlink" title="3.trim（去除空格）,lower（取小写）,upper（取大写）,split"></a>3.trim（去除空格）,lower（取小写）,upper（取大写）,split</h4><p>split(str,’符号’)[id]，其中符号是用于分隔的符号，如果遇到特殊字符需要转移。也可以用多个分隔符进行分列，如‘?|,’，split(str,’?|,’)[0],表示对str同时用问号和逗号进行分列，并取出第一个数据。其他那几个函数很简单，不赘述。</p>
<h4 id="4-reflect-‘java-net-URLDecoder’-‘decode’，str"><a href="#4-reflect-‘java-net-URLDecoder’-‘decode’，str" class="headerlink" title="4.reflect(‘java.net.URLDecoder’, ‘decode’，str)"></a>4.reflect(‘java.net.URLDecoder’, ‘decode’，str)</h4><p>其中str是需要转义的字符串，比如<code>str=&#39;http://dig.lianjia.com/t.gif?r=1487668122415&amp;d=%7B%22pid%22%3A%22lianjiamweb%22%2C%22key%22%3A%22https%3A%2F%2Fm.lianjia.com%2Fbj%2Flanrenzhaofang%2F%3Ffrom%3Dtimeline%26isappinstalled%3D0%23p3%22%2C%22action%22%3A%7B%22token%22%3A%22c9b71efc903f1f8e59f12845c4cd8978%22%2C%22tag%22%3A%22%E5%8D%95%E8%BA%AB%2F%E5%A4%AB%E5%A6%BB%22%7D%2C%22evt%22%3A%2210217%22%2C%22ljweb_channel_key%22%3A%22m_pages_lanrenzhaofangSearch%22%2C%22cid%22%3A110000%2C%22f%22%3A%22%22%2C%22uuid%22%3A%22399e1e19-18b0-4668-a907-b2a48f079d34%22%2C%22ssid%22%3A%224fe2fdc9-5078-4137-8002-9ce6ca84309d%22%7D&#39;</code>,存成这样是因为避免有一些特殊字符或者中文在传输过程中发生问题，因此将其编码成这种形式，而我们在统计数据的时候需要对其进行解码，此实例解码结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;dig.lianjia.com&#x2F;t.gif?r&#x3D;1487668122415&amp;d&#x3D;&#123;&quot;pid&quot;:&quot;lianjiamweb&quot;,&quot;key&quot;:&quot;https:&#x2F;&#x2F;m.lianjia.com&#x2F;bj&#x2F;lanrenzhaofang&#x2F;?from&#x3D;timeline&amp;isappinstalled&#x3D;0#p3&quot;,&quot;action&quot;:&#123;&quot;token&quot;:&quot;c9b71efc903f1f8e59f12845c4cd8978&quot;,&quot;tag&quot;:&quot;单身&#x2F;夫妻&quot;&#125;,&quot;evt&quot;:&quot;10217&quot;,&quot;ljweb_channel_key&quot;:&quot;m_pages_lanrenzhaofangSearch&quot;,&quot;cid&quot;:110000,&quot;f&quot;:&quot;&quot;,&quot;uuid&quot;:&quot;399e1e19-18b0-4668-a907-b2a48f079d34&quot;,&quot;ssid&quot;:&quot;4fe2fdc9-5078-4137-8002-9ce6ca84309d&quot;&#125;</span><br><span class="line">12</span><br></pre></td></tr></table></figure>

<p>从中可以看出单身/夫妻被解析成功，各种字符如斜杠等也被解析成功。</p>
<h4 id="5-parse-url"><a href="#5-parse-url" class="headerlink" title="5.parse_url"></a>5.parse_url</h4><p>此函数要慎用，此函数用法可参考这篇博客，侵删，<a href="http://www.cnblogs.com/itdyb/p/6236953.html" target="_blank" rel="noopener">http://www.cnblogs.com/itdyb/p/6236953.html</a> ，parse_url(url,’QUERY’)解析的是url中？之后，#号之前的信息，如上面实例所示，如果用此函数解析出的是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">r&#x3D;1487668122415&amp;d&#x3D;&#123;&quot;pid&quot;:&quot;lianjiamweb&quot;,&quot;key&quot;:&quot;https:&#x2F;&#x2F;m.lianjia.com&#x2F;bj&#x2F;lanrenzhaofang&#x2F;?from&#x3D;timeline&amp;isappinstalled&#x3D;01</span><br></pre></td></tr></table></figure>

<p>但是实际过程中，我们想要的可能是这一部分数据（问号之后的所有数据）：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">r&#x3D;1487668122415&amp;d&#x3D;&#123;&quot;pid&quot;:&quot;lianjiamweb&quot;,&quot;key&quot;:&quot;https:&#x2F;&#x2F;m.lianjia.com&#x2F;bj&#x2F;lanrenzhaofang&#x2F;?from&#x3D;timeline&amp;isappinstalled&#x3D;0#p3&quot;,&quot;action&quot;:&#123;&quot;token&quot;:&quot;c9b71efc903f1f8e59f12845c4cd8978&quot;,&quot;tag&quot;:&quot;单身&#x2F;夫妻&quot;&#125;,&quot;evt&quot;:&quot;10217&quot;,&quot;ljweb_channel_key&quot;:&quot;m_pages_lanrenzhaofangSearch&quot;,&quot;cid&quot;:110000,&quot;f&quot;:&quot;&quot;,&quot;uuid&quot;:&quot;399e1e19-18b0-4668-a907-b2a48f079d34&quot;,&quot;ssid&quot;:&quot;4fe2fdc9-5078-4137-8002-9ce6ca84309d&quot;&#125;1</span><br></pre></td></tr></table></figure>

<p>可用函数嵌套的方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">parse_url(regexp_replace(reflect(&#39;java.net.URLDecoder&#39;, &#39;decode&#39;,&quot;http:&#x2F;&#x2F;dig.lianjia.com&#x2F;t.gif?r&#x3D;1487668122415&amp;d&#x3D;%7B%22pid%22%3A%22lianjiamweb%22%2C%22key%22%3A%22https%3A%2F%2Fm.lianjia.com%2Fbj%2Flanrenzhaofang%2F%3Ffrom%3Dtimeline%26isappinstalled%3D0%23p3%22%2C%22action%22%3A%7B%22token%22%3A%22c9b71efc903f1f8e59f12845c4cd8978%22%2C%22tag%22%3A%22%E5%8D%95%E8%BA%AB%2F%E5%A4%AB%E5%A6%BB%22%7D%2C%22evt%22%3A%2210217%22%2C%22ljweb_channel_key%22%3A%22m_pages_lanrenzhaofangSearch%22%2C%22cid%22%3A110000%2C%22f%22%3A%22%22%2C%22uuid%22%3A%22399e1e19-18b0-4668-a907-b2a48f079d34%22%2C%22ssid%22%3A%224fe2fdc9-5078-4137-8002-9ce6ca84309d%22%7D&quot;),&#39;\n| |#&#39;,&#39;&#39;),&#39;QUERY&#39;)1</span><br></pre></td></tr></table></figure>

<p>解析出来的结果即为上面所需结果，需要注意的是，需要用regexp_replace这个函数把\n 、空格和# 替换为空，否则parse_url还是会按照？和#为开始和结束标志进行解析。</p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>解析json</tag>
      </tags>
  </entry>
  <entry>
    <title>Git远程分支</title>
    <url>/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="Git-分支-远程分支"><a href="#Git-分支-远程分支" class="headerlink" title="Git 分支 - 远程分支"></a>Git 分支 - 远程分支</h1><h2 id="远程分支"><a href="#远程分支" class="headerlink" title="远程分支"></a>远程分支</h2><p>远程引用是对远程仓库的引用（指针），包括分支、标签等等。 你可以通过 <code>git ls-remote &lt;remote&gt;</code> 来显式地获得远程引用的完整列表， 或者通过 <code>git remote show &lt;remote&gt;</code> 获得远程分支的更多信息。 然而，一个更常见的做法是利用远程跟踪分支。</p>
<p>远程跟踪分支是远程分支状态的引用。它们是你无法移动的本地引用。一旦你进行了网络通信， Git 就会为你移动它们以精确反映远程仓库的状态。请将它们看做书签， 这样可以提醒你该分支在远程仓库中的位置就是你最后一次连接到它们的位置。</p>
<a id="more"></a>

<p>它们以 <code>&lt;remote&gt;/&lt;branch&gt;</code> 的形式命名。 例如，如果你想要看你最后一次与远程仓库 <code>origin</code> 通信时 <code>master</code> 分支的状态，你可以查看 <code>origin/master</code> 分支。 你与同事合作解决一个问题并且他们推送了一个 <code>iss53</code> 分支，你可能有自己的本地 <code>iss53</code> 分支， 然而在服务器上的分支会以 <code>origin/iss53</code> 来表示。</p>
<p>这可能有一点儿难以理解，让我们来看一个例子。 假设你的网络里有一个在 <code>git.ourcompany.com</code> 的 Git 服务器。 如果你从这里克隆，Git 的 <code>clone</code> 命令会为你自动将其命名为 <code>origin</code>，拉取它的所有数据， 创建一个指向它的 <code>master</code> 分支的指针，并且在本地将其命名为 <code>origin/master</code>。 Git 也会给你一个与 origin 的 <code>master</code> 分支在指向同一个地方的本地 <code>master</code> 分支，这样你就有工作的基础。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note:</span><br><span class="line"></span><br><span class="line">“origin” 并无特殊含义</span><br><span class="line"></span><br><span class="line">远程仓库名字 “origin” 与分支名字 “master” 一样，在 Git 中并没有任何特别的含义一样。 同时 “master” 是当你运行 git init 时默认的起始分支名字，原因仅仅是它的广泛使用， “origin” 是当你运行 git clone 时默认的远程仓库名字。 如果你运行 git clone -o booyah，那么你默认的远程分支名字将会是 booyah&#x2F;master。</span><br></pre></td></tr></table></figure>



<p><img src="/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/remote-branches-1.png" alt="克隆之后的服务器与本地仓库。"></p>
<p>Figure 30. 克隆之后的服务器与本地仓库</p>
<p>如果你在本地的 <code>master</code> 分支做了一些工作，在同一段时间内有其他人推送提交到 <code>git.ourcompany.com</code> 并且更新了它的 <code>master</code> 分支，这就是说你们的提交历史已走向不同的方向。 即便这样，只要你保持不与 <code>origin</code> 服务器连接（并拉取数据），你的 <code>origin/master</code> 指针就不会移动。</p>
<p><img src="/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/remote-branches-2.png" alt="本地与远程的工作可以分叉。"></p>
<p>Figure 31. 本地与远程的工作可以分叉</p>
<p>如果要与给定的远程仓库同步数据，运行 <code>git fetch &lt;remote&gt;</code> 命令（在本例中为 <code>git fetch origin</code>）。 这个命令查找 “origin” 是哪一个服务器（在本例中，它是 <code>git.ourcompany.com</code>）， 从中抓取本地没有的数据，并且更新本地数据库，移动 <code>origin/master</code> 指针到更新之后的位置。</p>
<p><img src="/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/remote-branches-3.png" alt="`git fetch` 更新你的远程仓库引用。"></p>
<p>Figure 32. <code>git fetch</code> 更新你的远程跟踪分支</p>
<p>为了演示有多个远程仓库与远程分支的情况，我们假定你有另一个内部 Git 服务器，仅服务于你的某个敏捷开发团队。 这个服务器位于 <code>git.team1.ourcompany.com</code>。 你可以运行 <code>git remote add</code> 命令添加一个新的远程仓库引用到当前的项目，这个命令我们会在 <a href="https://git-scm.com/book/zh/v2/ch00/ch02-git-basics-chapter" target="_blank" rel="noopener">Git 基础</a> 中详细说明。 将这个远程仓库命名为 <code>teamone</code>，将其作为完整 URL 的缩写。</p>
<p><img src="/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/remote-branches-4.png" alt="添加另一个远程仓库。"></p>
<p>Figure 33. 添加另一个远程仓库</p>
<p>现在，可以运行 <code>git fetch teamone</code> 来抓取远程仓库 <code>teamone</code> 有而本地没有的数据。 因为那台服务器上现有的数据是 <code>origin</code> 服务器上的一个子集， 所以 Git 并不会抓取数据而是会设置远程跟踪分支 <code>teamone/master</code> 指向 <code>teamone</code> 的 <code>master</code> 分支。</p>
<p><img src="/2020/09/15/Git%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF/remote-branches-5.png" alt="远程跟踪分支 `teamone/master`。"></p>
<p>Figure 34. 远程跟踪分支 <code>teamone/master</code></p>
<h3 id="推送"><a href="#推送" class="headerlink" title="推送"></a>推送</h3><p>当你想要公开分享一个分支时，需要将其推送到有写入权限的远程仓库上。 本地的分支并不会自动与远程仓库同步——你必须显式地推送想要分享的分支。 这样，你就可以把不愿意分享的内容放到私人分支上，而将需要和别人协作的内容推送到公开分支。</p>
<p>如果希望和别人一起在名为 <code>serverfix</code> 的分支上工作，你可以像推送第一个分支那样推送它。 运行 <code>git push &lt;remote&gt; &lt;branch&gt;</code>:</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git push origin serverfix</span></span><br><span class="line">Counting objects: 24, done.</span><br><span class="line">Delta compression using up to 8 threads.</span><br><span class="line">Compressing objects: 100% (15/15), done.</span><br><span class="line">Writing objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.</span><br><span class="line">Total 24 (delta 2), reused 0 (delta 0)</span><br><span class="line">To https://github.com/schacon/simplegit</span><br><span class="line"> * [new branch]      serverfix -&gt; serverfix</span><br></pre></td></tr></table></figure>

<p>这里有些工作被简化了。 Git 自动将 <code>serverfix</code> 分支名字展开为 <code>refs/heads/serverfix:refs/heads/serverfix</code>， 那意味着，“推送本地的 <code>serverfix</code> 分支来更新远程仓库上的 <code>serverfix</code> 分支。” 我们将会详细学习 <a href="https://git-scm.com/book/zh/v2/ch00/ch10-git-internals" target="_blank" rel="noopener">Git 内部原理</a> 的 <code>refs/heads/</code> 部分， 但是现在可以先把它放在儿。你也可以运行 <code>git push origin serverfix:serverfix</code>， 它会做同样的事——也就是说“推送本地的 <code>serverfix</code> 分支，将其作为远程仓库的 <code>serverfix</code> 分支” 可以通过这种格式来推送本地分支到一个命名不相同的远程分支。 如果并不想让远程仓库上的分支叫做 <code>serverfix</code>，可以运行 <code>git push origin serverfix:awesomebranch</code> 来将本地的 <code>serverfix</code> 分支推送到远程仓库上的 <code>awesomebranch</code> 分支。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note:</span><br><span class="line">如何避免每次输入密码如果你正在使用 HTTPS URL 来推送，Git 服务器会询问用户名与密码。 默认情况下它会在终端中提示服务器是否允许你进行推送。如果不想在每一次推送时都输入用户名与密码，你可以设置一个 “credential cache”。 最简单的方式就是将其保存在内存中几分钟，可以简单地运行 git config --global credential.helper cache 来设置它。想要了解更多关于不同验证缓存的可用选项，查看 凭证存储。</span><br></pre></td></tr></table></figure>



<p>下一次其他协作者从服务器上抓取数据时，他们会在本地生成一个远程分支 <code>origin/serverfix</code>，指向服务器的 <code>serverfix</code> 分支的引用：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git fetch origin</span></span><br><span class="line">remote: Counting objects: 7, done.</span><br><span class="line">remote: Compressing objects: 100% (2/2), done.</span><br><span class="line">remote: Total 3 (delta 0), reused 3 (delta 0)</span><br><span class="line">Unpacking objects: 100% (3/3), done.</span><br><span class="line">From https://github.com/schacon/simplegit</span><br><span class="line"> * [new branch]      serverfix    -&gt; origin/serverfix</span><br></pre></td></tr></table></figure>

<p>要特别注意的一点是当抓取到新的远程跟踪分支时，本地不会自动生成一份可编辑的副本（拷贝）。 换一句话说，这种情况下，不会有一个新的 <code>serverfix</code> 分支——只有一个不可以修改的 <code>origin/serverfix</code> 指针。</p>
<p>可以运行 <code>git merge origin/serverfix</code> 将这些工作合并到当前所在的分支。 如果想要在自己的 <code>serverfix</code> 分支上工作，可以将其建立在远程跟踪分支之上：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b serverfix origin/serverfix</span></span><br><span class="line">Branch serverfix set up to track remote branch serverfix from origin.</span><br><span class="line">Switched to a new branch 'serverfix'</span><br></pre></td></tr></table></figure>

<p>这会给你一个用于工作的本地分支，并且起点位于 <code>origin/serverfix</code>。</p>
<h3 id="跟踪分支"><a href="#跟踪分支" class="headerlink" title="跟踪分支"></a>跟踪分支</h3><p>从一个远程跟踪分支检出一个本地分支会自动创建所谓的“跟踪分支”（它跟踪的分支叫做“上游分支”）。 跟踪分支是与远程分支有直接关系的本地分支。 如果在一个跟踪分支上输入 <code>git pull</code>，Git 能自动地识别去哪个服务器上抓取、合并到哪个分支。</p>
<p>当克隆一个仓库时，它通常会自动地创建一个跟踪 <code>origin/master</code> 的 <code>master</code> 分支。 然而，如果你愿意的话可以设置其他的跟踪分支，或是一个在其他远程仓库上的跟踪分支，又或者不跟踪 <code>master</code> 分支。 最简单的实例就是像之前看到的那样，运行 <code>git checkout -b &lt;branch&gt; &lt;remote&gt;/&lt;branch&gt;</code>。 这是一个十分常用的操作所以 Git 提供了 <code>--track</code> 快捷方式：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout --track origin/serverfix</span></span><br><span class="line">Branch serverfix set up to track remote branch serverfix from origin.</span><br><span class="line">Switched to a new branch 'serverfix'</span><br></pre></td></tr></table></figure>

<p>由于这个操作太常用了，该捷径本身还有一个捷径。 如果你尝试检出的分支 (a) 不存在且 (b) 刚好只有一个名字与之匹配的远程分支，那么 Git 就会为你创建一个跟踪分支：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout serverfix</span></span><br><span class="line">Branch serverfix set up to track remote branch serverfix from origin.</span><br><span class="line">Switched to a new branch 'serverfix'</span><br></pre></td></tr></table></figure>

<p>如果想要将本地分支与远程分支设置为不同的名字，你可以轻松地使用上一个命令增加一个不同名字的本地分支：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git checkout -b sf origin/serverfix</span></span><br><span class="line">Branch sf set up to track remote branch serverfix from origin.</span><br><span class="line">Switched to a new branch 'sf'</span><br></pre></td></tr></table></figure>

<p>现在，本地分支 <code>sf</code> 会自动从 <code>origin/serverfix</code> 拉取。</p>
<p>设置已有的本地分支跟踪一个刚刚拉取下来的远程分支，或者想要修改正在跟踪的上游分支， 你可以在任意时间使用 <code>-u</code> 或 <code>--set-upstream-to</code> 选项运行 <code>git branch</code> 来显式地设置。</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git branch -u origin/serverfix</span></span><br><span class="line">Branch serverfix set up to track remote branch serverfix from origin.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Note:</span><br><span class="line">上游快捷方式当设置好跟踪分支后，可以通过简写 @&#123;upstream&#125; 或 @&#123;u&#125; 来引用它的上游分支。 所以在 master 分支时并且它正在跟踪 origin&#x2F;master 时，如果愿意的话可以使用 git merge @&#123;u&#125; 来取代 git merge origin&#x2F;master。</span><br></pre></td></tr></table></figure>



<p>如果想要查看设置的所有跟踪分支，可以使用 <code>git branch</code> 的 <code>-vv</code> 选项。 这会将所有的本地分支列出来并且包含更多的信息，如每一个分支正在跟踪哪个远程分支与本地分支是否是领先、落后或是都有。</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git branch -vv</span></span><br><span class="line">  iss53     7e424c3 [origin/iss53: ahead 2] forgot the brackets</span><br><span class="line">  master    1ae2a45 [origin/master] deploying index fix</span><br><span class="line">* serverfix f8674d9 [teamone/server-fix-good: ahead 3, behind 1] this should do it</span><br><span class="line">  testing   5ea463a trying something new</span><br></pre></td></tr></table></figure>

<p>这里可以看到 <code>iss53</code> 分支正在跟踪 <code>origin/iss53</code> 并且 “ahead” 是 2，意味着本地有两个提交还没有推送到服务器上。 也能看到 <code>master</code> 分支正在跟踪 <code>origin/master</code> 分支并且是最新的。 接下来可以看到 <code>serverfix</code> 分支正在跟踪 <code>teamone</code> 服务器上的 <code>server-fix-good</code> 分支并且领先 3 落后 1， 意味着服务器上有一次提交还没有合并入同时本地有三次提交还没有推送。 最后看到 <code>testing</code> 分支并没有跟踪任何远程分支。</p>
<p>需要重点注意的一点是这些数字的值来自于你从每个服务器上最后一次抓取的数据。 这个命令并没有连接服务器，它只会告诉你关于本地缓存的服务器数据。 如果想要统计最新的领先与落后数字，需要在运行此命令前抓取所有的远程仓库。 可以像这样做：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git fetch --all; git branch -vv</span></span><br></pre></td></tr></table></figure>

<h3 id="拉取"><a href="#拉取" class="headerlink" title="拉取"></a>拉取</h3><p>当 <code>git fetch</code> 命令从服务器上抓取本地没有的数据时，它并不会修改工作目录中的内容。 它只会获取数据然后让你自己合并。 然而，有一个命令叫作 <code>git pull</code> 在大多数情况下它的含义是一个 <code>git fetch</code> 紧接着一个 <code>git merge</code> 命令。 如果有一个像之前章节中演示的设置好的跟踪分支，不管它是显式地设置还是通过 <code>clone</code> 或 <code>checkout</code> 命令为你创建的，<code>git pull</code> 都会查找当前分支所跟踪的服务器与分支， 从服务器上抓取数据然后尝试合并入那个远程分支。</p>
<p>由于 <code>git pull</code> 的魔法经常令人困惑所以通常单独显式地使用 <code>fetch</code> 与 <code>merge</code> 命令会更好一些。</p>
<h3 id="删除远程分支"><a href="#删除远程分支" class="headerlink" title="删除远程分支"></a>删除远程分支</h3><p>假设你已经通过远程分支做完所有的工作了——也就是说你和你的协作者已经完成了一个特性， 并且将其合并到了远程仓库的 <code>master</code> 分支（或任何其他稳定代码分支）。 可以运行带有 <code>--delete</code> 选项的 <code>git push</code> 命令来删除一个远程分支。 如果想要从服务器上删除 <code>serverfix</code> 分支，运行下面的命令：</p>
<figure class="highlight console"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git push origin --delete serverfix</span></span><br><span class="line">To https://github.com/schacon/simplegit</span><br><span class="line"> - [deleted]         serverfix</span><br></pre></td></tr></table></figure>

<p>基本上这个命令做的只是从服务器上移除这个指针。 Git 服务器通常会保留数据一段时间直到垃圾回收运行，所以如果不小心删除掉了，通常是很容易恢复的。</p>
]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Git分支</tag>
        <tag>远程分支</tag>
      </tags>
  </entry>
  <entry>
    <title>go 运行错误expected &#39;package&#39;, found &#39;EOF&#39;解决</title>
    <url>/2020/09/16/go-%E8%BF%90%E8%A1%8C%E9%94%99%E8%AF%AFexpected-package-found-EOF-%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>一句话:只要把文件保存一下,再运行就可以了</p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>语言</tag>
      </tags>
  </entry>
</search>
