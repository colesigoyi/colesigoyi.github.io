<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hive2的启动</title>
    <url>/2020/06/26/hive2%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<ol>
<li>启动hive的服务<br>hive ‐‐service hiveserver2 &amp;</li>
<li>使用beeline，去连接thrift的服务<br>beeline -u jdbc:hive2://CentOS-01:10000</li>
</ol>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>服务</tag>
        <tag>beeline</tag>
      </tags>
  </entry>
  <entry>
    <title>数据质量检查</title>
    <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E6%A3%80%E6%9F%A5/</url>
    <content><![CDATA[<h2 id="数据质量检查"><a href="#数据质量检查" class="headerlink" title="数据质量检查"></a>数据质量检查</h2><p>是在完成宽表数据开发后进行的，主要包括四个方面：</p>
<ol>
<li><p>重复值检查</p>
</li>
<li><p>缺失值检查</p>
</li>
<li><p>数据倾斜问题</p>
</li>
<li><p>异常值检查</p>
<a id="more"></a>

</li>
</ol>
<h2 id="1-重复值检查"><a href="#1-重复值检查" class="headerlink" title="1. 重复值检查"></a>1. 重复值检查</h2><h3 id="1-1-什么是重复值"><a href="#1-1-什么是重复值" class="headerlink" title="1.1 什么是重复值"></a>1.1 什么是重复值</h3><p>重复值的检查首先要明确一点，即重复值的定义。对于一份二维表形式的数据集来说，什么是重复值？主要有两个层次：<br>① 关键字段出现重复记录，比如主索引字段出现重复；<br>② 所有字段出现重复记录。<br>第一个层次是否是重复，必须从这份数据的业务含义进行确定。比如一张表，从业务上讲，一个用户应该只会有一条记录，那么如果某个用户出现了超过一条的记录，那么这就是重复值。第二个层次，就一定是重复值了。</p>
<h3 id="1-2-重复值产生的原因"><a href="#1-2-重复值产生的原因" class="headerlink" title="1.2 重复值产生的原因"></a>1.2 重复值产生的原因</h3><p>重复值的产生主要有两个原因，一是上游源数据造成的，二是数据准备脚本中的数据关联造成的。从数据准备角度来看，首先检查数据准备的脚本，判断使用的源表是否有重复记录，同时检查关联语句的正确性和严谨性，比如关联条件是否合理、是否有限定数据周期等等。<br>比如：检查源表数据是否重复的SQL：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> MON_ID,<span class="keyword">COUNT</span>(*),<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> USER_ID)</span><br><span class="line"><span class="keyword">FROM</span> TABLE_NAME</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> MON_ID;</span><br></pre></td></tr></table></figure>

<p>如果是上游源数据出现重复，那么应该及时反映给上游进行修正；如果是脚本关联造成的，修改脚本，重新生成数据即可。<br>还有一份情况，这份数据集是一份单独的数据集，并不是在数据仓库中开发得到的数据，既没有上游源数据，也不存在生成数据的脚本，比如公开数据集，那么如何处理其中的重复值？一般的处理方式就是直接删除重复值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">dataset = pd.read_excel(<span class="string">"/labcenter/python/dataset.xlsx"</span>)</span><br><span class="line"><span class="comment">#判断重复数据</span></span><br><span class="line">dataset.duplicated()    <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.duplicated([<span class="string">'col2'</span>])    <span class="comment">#col2字段重复是重复数据</span></span><br><span class="line"><span class="comment">#删除重复数据</span></span><br><span class="line">dataset.drop_duplicates()     <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.drop_duplicates([<span class="string">'col2'</span>])   <span class="comment">#col2字段重复是重复数据</span></span><br></pre></td></tr></table></figure>

<h2 id="2-缺失值检查"><a href="#2-缺失值检查" class="headerlink" title="2. 缺失值检查"></a>2. 缺失值检查</h2><p>缺失值主要是指数据集中部分记录存在部分字段的信息缺失。</p>
<h3 id="2-1-缺失值出现的原因"><a href="#2-1-缺失值出现的原因" class="headerlink" title="2.1 缺失值出现的原因"></a>2.1 缺失值出现的原因</h3><p>出现趋势值主要有三种原因：<br>① 上游源系统因为技术或者成本原因无法完全获取到这一信息，比如对用户手机APP上网记录的解析；<br>② 从业务上讲，这一信息本来就不存在，比如一个学生的收入，一个未婚者的配偶姓名；<br>③ 数据准备脚本开发中的错误造成的。<br>第一种原因，短期内无法解决；第二种原因，数据的缺失并不是错误，无法避免；第三种原因，则只需通过查证修改脚本即可。<br>缺失值的存在既代表了某一部分信息的丢失，也影响了挖掘分析结论的可靠性与稳定性，因此，必须对缺失值进行处理。<br>如果缺失值记录数超过了全部记录数的50%，则应该从数据集中直接剔除掉该字段，尝试从业务上寻找替代字段；<br>如果缺失值记录数没有超过50%，则应该首先看这个字段在业务上是否有替代字段，如果有，则直接剔除掉该字段，如果没有，则必须对其进行处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看哪些字段有缺失值   </span></span><br><span class="line">dataset.isnull().any()    <span class="comment">#获取含有NaN的字段</span></span><br><span class="line"><span class="comment">#统计各字段的缺失值个数</span></span><br><span class="line">dataset.isnull().apply(pd.value_counts)</span><br><span class="line"><span class="comment">#删除含有缺失值的字段</span></span><br><span class="line">nan_col = dataset.isnull().any()</span><br><span class="line">dataset.drop(nan_col[nan_col].index,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-缺失值的处理"><a href="#2-2-缺失值的处理" class="headerlink" title="2.2 缺失值的处理"></a>2.2 缺失值的处理</h3><p>缺失值的处理主要有两种方式：过滤和填充。</p>
<h4 id="（1）缺失值的过滤"><a href="#（1）缺失值的过滤" class="headerlink" title="（1）缺失值的过滤"></a>（1）缺失值的过滤</h4><p>直接删除含有缺失值的记录，总体上会影响样本个数，如果删除样本过多或者数据集本来就是小数据集时，这种方式并不建议采用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除含有缺失值的记录</span></span><br><span class="line">dataset.dropna()</span><br></pre></td></tr></table></figure>

<h4 id="2）缺失值的填充"><a href="#2）缺失值的填充" class="headerlink" title="2）缺失值的填充"></a>2）缺失值的填充</h4><p>缺失值的填充主要三种方法：<br>① 方法一：使用特定值填充<br>使用缺失值字段的平均值、中位数、众数等统计量填充。<br>优点：简单、快速<br>缺点：容易产生数据倾斜<br>② 方法二：使用算法预测填充<br>将缺失值字段作为因变量，将没有缺失值字段作为自变量，使用决策树、随机森林、KNN、回归等预测算法进行缺失值的预测，用预测结果进行填充。<br>优点：相对精确<br>缺点：效率低，如果缺失值字段与其他字段相关性不大，预测效果差<br>③ 方法三：将缺失值单独作为一个分组，指定值进行填充<br>从业务上选择一个单独的值进行填充，使缺失值区别于其他值而作为一个分组，从而不影响算法计算。<br>优点：简单，实用<br>缺点：效率低</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用Pandas进行特定值填充</span></span><br><span class="line">dataset.fillna(<span class="number">0</span>)   <span class="comment">#不同字段的缺失值都用0填充</span></span><br><span class="line">dataset.fillna(&#123;<span class="string">'col2'</span>:<span class="number">20</span>,<span class="string">'col5'</span>:<span class="number">0</span>&#125;)    <span class="comment">#不同字段使用不同的填充值</span></span><br><span class="line">dataset.fillna(dataset.mean())   <span class="comment">#分别使用各字段的平均值填充</span></span><br><span class="line">dataset.fillna(dataset.median())     <span class="comment">#分别使用个字段的中位数填充</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用sklearn中的预处理方法进行缺失值填充(只适用于连续型字段)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">dataset2 = dataset.drop([<span class="string">'col4'</span>],axis=<span class="number">1</span>)</span><br><span class="line">colsets = dataset2.columns</span><br><span class="line">nan_rule1 = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)    <span class="comment">#创建填充规则(平均值填充)</span></span><br><span class="line">pd.DataFrame(nan_rule1.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule2 = Imputer(missing_values=<span class="string">'median'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>) <span class="comment">#创建填充规则(中位数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule2.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule3 = Imputer(missing_values=<span class="string">'most_frequent'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)  <span class="comment">#创建填充规则(众数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule3.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br></pre></td></tr></table></figure>

<h2 id="3-数据倾斜问题"><a href="#3-数据倾斜问题" class="headerlink" title="3. 数据倾斜问题"></a>3. 数据倾斜问题</h2><p>数据倾斜是指字段的取值分布主要集中在某个特定类别或者特定区间。</p>
<h3 id="3-1-数据倾斜问题的原因"><a href="#3-1-数据倾斜问题的原因" class="headerlink" title="3.1 数据倾斜问题的原因"></a>3.1 数据倾斜问题的原因</h3><p>出现这一问题的原因主要有三种：<br>① 上游源数据存在问题；<br>② 数据准备脚本的问题；<br>③ 数据本身的分布就是如此。<br>如果某个字段出现数据倾斜问题，必须首先排查上述第一、二种原因，如果都没有问题或者无法检查（如：单独的数据集），那么就要考虑这个字段对后续的分析建模是否有价值。一般来说，有严重的数据倾斜的字段对目标变量的区分能力很弱，对分析建模的价值不大，应该直接剔除掉。</p>
<h3 id="3-2-如何衡量数据的倾斜程度"><a href="#3-2-如何衡量数据的倾斜程度" class="headerlink" title="3.2 如何衡量数据的倾斜程度"></a>3.2 如何衡量数据的倾斜程度</h3><p>衡量数据的倾斜程度，主要采用频数分析方法，但因数据类别的不同而有所差异：<br>① 针对连续型字段，需要首先采用等宽分箱方式进行离散化，然后计算各分箱的记录数分布；<br>② 针对离散型字段，直接计算各类别的记录数分布。<br>一般来说，如果某个字段90%以上的记录数，主要集中在某个特定类别或者特定区间，那么这个字段就存在严重的数据倾斜问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对于连续型变量进行等宽分箱</span></span><br><span class="line">pd.value_counts(pd.cut(dataset[<span class="string">'col3'</span>],<span class="number">5</span>))  <span class="comment">#分成5箱</span></span><br><span class="line"><span class="comment">#对于离散型变量进行频数统计</span></span><br><span class="line">pd.value_counts(dataset[<span class="string">'col4'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="4-异常值检查"><a href="#4-异常值检查" class="headerlink" title="4. 异常值检查"></a>4. 异常值检查</h2><p>异常值是指数据中出现了处于特定分布、范围或者趋势之外的数据，这些数据一般会被成为异常值、离群点、噪音等。</p>
<h3 id="4-1-异常值产生的原因"><a href="#4-1-异常值产生的原因" class="headerlink" title="4.1 异常值产生的原因"></a>4.1 异常值产生的原因</h3><p>异常值的产生主要有两类原因：<br>① 数据采集、生成或者传递过程中发生的错误；<br>② 业务运营过程出现的一些特殊情况。<br>将第一种原因产生的异常值称为统计上的异常，这是错误带来的数据问题，需要解决；将第二种原因产生的异常值称为业务上的异常，反映了业务运营过程的某种特殊结果，它不是错误，但需要深究，在数据挖掘中的一种典型应用就是异常检测模型，比如信用卡欺诈，网络入侵检测、客户异动行为识别等等。</p>
<h3 id="4-2-异常值的识别方法"><a href="#4-2-异常值的识别方法" class="headerlink" title="4.2 异常值的识别方法"></a>4.2 异常值的识别方法</h3><p>异常值的识别方法主要有以下几种：</p>
<h4 id="（1）极值检查"><a href="#（1）极值检查" class="headerlink" title="（1）极值检查"></a>（1）极值检查</h4><p>主要检查字段的取值是否超出了合理的值域范围。<br>① 方法一：最大值最小值<br>使用最大值、最小值进行判断。比如客户年龄的最大值为199岁，客户账单的最小费用为-20，这些都明显存在异常。<br>② 方法二：3σ原则<br>如果数据服从正态分布，在3σ原则下，异常值被定义为与平均值的偏差超过了3倍标准差的值。这是因为，在正态分布的假设下，具体平均值3倍标准差之外的值出现的概率低于0.003，属于极个别的小概率事件。<br>③ 方法三：箱线图分析<br>箱线图提供了识别异常的标准：异常值被定义为小于下四分位-1.5倍的四分位间距，或者大于上四分位+1.5倍的四分位间距的值。<br>箱线图分析不要求数据服从任何分布，因此对异常值的识别比较客观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#计算相关统计指标</span></span><br><span class="line">statDF = dataset2.describe()  <span class="comment">#获取描述性统计量</span></span><br><span class="line">statDF.loc[<span class="string">'mean+3std'</span>] = statDF.loc[<span class="string">'mean'</span>] + <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值+3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'mean-3std'</span>] = statDF.loc[<span class="string">'mean'</span>] - <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值-3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'75%+1.5dist'</span>] = statDF.loc[<span class="string">'75%'</span>] + <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算上四分位+1.5倍的四分位间距</span></span><br><span class="line">statDF.loc[<span class="string">'25%-1.5dist'</span>] = statDF.loc[<span class="string">'25%'</span>] - <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算下四分位-1.5倍的四分位间距</span></span><br><span class="line"><span class="comment">#获取各字段最大值、最小值</span></span><br><span class="line">statDF.loc[[<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line"><span class="comment">#判断取值是否大于平均值+3倍标准差</span></span><br><span class="line">dataset3 = dataset2 - statDF.loc[<span class="string">'mean+3std'</span>]</span><br><span class="line">dataset3[dataset3&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于平均值-3倍标准差</span></span><br><span class="line">dataset4 = dataset2 - statDF.loc[<span class="string">'mean-3std'</span>]</span><br><span class="line">dataset4[dataset4&lt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否大于上四分位+1.5倍的四分位间距</span></span><br><span class="line">dataset5 = dataset2 - statDF.loc[<span class="string">'75%+1.5dist'</span>]</span><br><span class="line">dataset5[dataset5&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于下四分位-1.5倍的四分位间距</span></span><br><span class="line">dataset6 = dataset2 - statDF.loc[<span class="string">'25%-1.5dist'</span>]</span><br><span class="line">dataset6[dataset6&lt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="（2）记录数分布检查"><a href="#（2）记录数分布检查" class="headerlink" title="（2）记录数分布检查"></a>（2）记录数分布检查</h4><p>主要检查字段的记录数分布是否超出合理的分布范围，包括三个指标：零值记录数、正值记录数、负值记录数。</p>
<h4 id="（3）波动检查"><a href="#（3）波动检查" class="headerlink" title="（3）波动检查"></a>（3）波动检查</h4><p>波动检查主要适用于有监督的数据，用于检查随着自变量的变化，因变量是否发生明显的波动情况。<br>以上异常值的识别方法主要针对连续型的字段，而对于离散型的字段的异常识别主要通过检查类别出现是否出现了合理阈值外的数据，比如苹果终端型号字段，出现了“P20”的取值。</p>
<h3 id="4-3-异常值的处理"><a href="#4-3-异常值的处理" class="headerlink" title="4.3 异常值的处理"></a>4.3 异常值的处理</h3><p>对于统计上的异常值的处理，主要采取两种方式：剔除或者替换。剔除是指直接将被标记为异常值的记录从数据集中删除掉，而替换是指将异常值用一个非异常值进行替换，比如边界值，或者有监督情况下的目标变量表征相似的某个值。<br>对于业务上的异常值的处理，原则就是进行深入探索分析，查找出现这一特殊情况的根本原因。</p>
]]></content>
      <categories>
        <category>数据质量</category>
      </categories>
      <tags>
        <tag>质量检查</tag>
        <tag>数仓</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop从mysql导入数据到hive</title>
    <url>/2020/07/22/sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。</p>
<pre><code>把MySQL、Oracle等数据库中的数据导入到HDFS、Hive、HBase中。
把HDFS、Hive、HBase中的数据导出到MySQL、Oracle等数据库中。
1.4 为sqoop1, 1.9 为sqoop2 ，sqoop1与sqoop2是不兼容的。</code></pre><a id="more"></a>

<h3 id="实现需要"><a href="#实现需要" class="headerlink" title="实现需要"></a>实现需要</h3><p>数据库:</p>
<ul>
<li>driver</li>
<li>URL、username、password</li>
<li>database、table</li>
</ul>
<p>hadoop:</p>
<ul>
<li>type (hdfp、hive、hbase)</li>
<li>path 存储到哪里？</li>
<li>数据分隔符</li>
<li>mappers 数量，也就是使用多少线程。</li>
</ul>
<h2 id="二、命令"><a href="#二、命令" class="headerlink" title="二、命令"></a>二、命令</h2><h3 id="查看-sqoop-支持的命令"><a href="#查看-sqoop-支持的命令" class="headerlink" title="查看 sqoop 支持的命令"></a>查看 sqoop 支持的命令</h3><p>  sqoop help</p>
<h3 id="显示所有库名"><a href="#显示所有库名" class="headerlink" title="显示所有库名"></a>显示所有库名</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306 \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="显示某个数据库里所有表"><a href="#显示某个数据库里所有表" class="headerlink" title="显示某个数据库里所有表"></a>显示某个数据库里所有表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="MYSQL-导入数据到-HIVE"><a href="#MYSQL-导入数据到-HIVE" class="headerlink" title="MYSQL 导入数据到 HIVE"></a>MYSQL 导入数据到 HIVE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234 \</span></span><br><span class="line"><span class="comment">--table people</span></span><br><span class="line"><span class="comment">--hive-import </span></span><br><span class="line"><span class="comment">--create-hive-table </span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"</span></span><br><span class="line">-m 5</span><br></pre></td></tr></table></figure>

<h3 id="hive-参数"><a href="#hive-参数" class="headerlink" title="hive 参数"></a>hive 参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–hive-import 必须参数，指定导入hive</span><br><span class="line">–hive-database default hive库名</span><br><span class="line">–hive-table people hive表名</span><br><span class="line">–fields-terminated-by hive的分隔符</span><br><span class="line">–hive-overwrite 重写重复字段</span><br><span class="line">–create-hive-table 帮创建好 hive 表，但是表存在会出错。不建议使用这个参数，因为到导入的时候，会与我们的字段类型有出入。</span><br><span class="line">–hive-partition-key “dt” 指定分区表的字段</span><br><span class="line">–hive-partition-value “2018-08-08” 指定分区表的值</span><br></pre></td></tr></table></figure>

<h3 id="导出没有主键的表"><a href="#导出没有主键的表" class="headerlink" title="导出没有主键的表"></a>导出没有主键的表</h3><p>可以使用两种方式：</p>
<ul>
<li>–split-by 指定切分的字段</li>
<li>-m 1 : 设置只使用一个map进行数据迁移</li>
</ul>
<h3 id="过滤条件"><a href="#过滤条件" class="headerlink" title="过滤条件"></a>过滤条件</h3><p>–where “age&gt;18” 匹配条件<br>       –columns “name,age” 选择要导入的指定列<br>       –query ‘select * from people where age&gt;18 and $CONDITIONS’: sql语句查询的结果集<br>      不能 –table 一起使用<br>      需要指定 –target-dir 路径 </p>
<h3 id="当数据库中字符为空时的处理"><a href="#当数据库中字符为空时的处理" class="headerlink" title="当数据库中字符为空时的处理"></a>当数据库中字符为空时的处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–null-non-string ‘0’ 当不是字符串的数据为空的时候，用 0 替换</span><br><span class="line">–null-string ‘string’ 当字符串为空的时候，使用string 字符替换</span><br></pre></td></tr></table></figure>

<h3 id="提高传输速度"><a href="#提高传输速度" class="headerlink" title="提高传输速度"></a>提高传输速度</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–direct 提高数据库到hadoop的传输速度</span><br></pre></td></tr></table></figure>

<p>支持的数据库类型与版本：</p>
<ul>
<li>myslq 5.0 以上</li>
<li>oracle 10.2.0 以上</li>
</ul>
<h3 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h3><p>增量导入对应，首先需要知监控那一列，这列要从哪个值开始增量</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">* check-column id 用来指定一些列</span><br><span class="line">* 这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的,常用的是指定主键id.</span><br><span class="line">* –check-column 可以去指定多个列</span><br></pre></td></tr></table></figure>

<ul>
<li>last-value 10 从哪个值开始增量</li>
<li>incremental 增量的模式<ul>
<li>append id 是获取大于某一列的某个值。</li>
<li>lastmodified “2016-12-15 15:47:30” 获取某个时间后修改的所有数据<ul>
<li>–append 附加模式</li>
<li>–merge-key id 合并模式</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意：增量导入不能与 –delete-target-dir 一起使用，还有必须指定增量的模式</p>
]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Python时间操作</title>
    <url>/2020/07/22/python%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">print</span> time.time()	<span class="comment">#输出的结果是:1279578704.6725271</span></span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>但是这样是一连串的数字不是我们想要的结果，我们可以利用time模块的格式化时间的方法来处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.localtime(time.time())</span><br></pre></td></tr></table></figure>

<p>用time.localtime()方法，作用是格式化时间戳为本地的时间。<br>输出的结果是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.struct_time(tm_year=<span class="number">2010</span>, tm_mon=<span class="number">7</span>, tm_mday=<span class="number">19</span>, tm_hour=<span class="number">22</span>, tm_min=<span class="number">33</span>, tm_sec=<span class="number">39</span>, tm_wday=<span class="number">0</span>, tm_yday=<span class="number">200</span>, tm_isdst=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>现在看起来更有希望格式成我们想要的时间了。<br>time.strftime(‘%Y-%m-%d’,time.localtime(time.time()))</p>
<p>最后用time.strftime()方法，把刚才的一大串信息格式化成我们想要的东西，现在的结果是：<br>2010-07-19</p>
<p>time.strftime里面有很多参数，可以让你能够更随意的输出自己想要的东西：<br>下面是time.strftime的参数：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">strftime(format[, tuple]) -&gt; string</span><br></pre></td></tr></table></figure>

<p>将指定的struct_time(默认为当前时间)，根据指定的格式化字符串输出<br>python中时间日期格式化符号：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%y 两位数的年份表示（<span class="number">00</span><span class="number">-99</span>）</span><br><span class="line">%Y 四位数的年份表示（<span class="number">000</span><span class="number">-9999</span>）</span><br><span class="line">%m 月份（<span class="number">01</span><span class="number">-12</span>）</span><br><span class="line">%d 月内中的一天（<span class="number">0</span><span class="number">-31</span>）</span><br><span class="line">%H <span class="number">24</span>小时制小时数（<span class="number">0</span><span class="number">-23</span>）</span><br><span class="line">%I <span class="number">12</span>小时制小时数（<span class="number">01</span><span class="number">-12</span>） </span><br><span class="line">%M 分钟数（<span class="number">00</span>=<span class="number">59</span>）</span><br><span class="line">%S 秒（<span class="number">00</span><span class="number">-59</span>）</span><br><span class="line">%a 本地简化星期名称</span><br><span class="line">%A 本地完整星期名称</span><br><span class="line">%b 本地简化的月份名称</span><br><span class="line">%B 本地完整的月份名称</span><br><span class="line">%c 本地相应的日期表示和时间表示</span><br><span class="line">%j 年内的一天（<span class="number">001</span><span class="number">-366</span>）</span><br><span class="line">%p 本地A.M.或P.M.的等价符</span><br><span class="line">%U 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期天为星期的开始</span><br><span class="line">%w 星期（<span class="number">0</span><span class="number">-6</span>），星期天为星期的开始</span><br><span class="line">%W 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期一为星期的开始</span><br><span class="line">%x 本地相应的日期表示</span><br><span class="line">%X 本地相应的时间表示</span><br><span class="line">%Z 当前时区的名称</span><br><span class="line">%% %号本身</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive中Parquet格式的使用</title>
    <url>/2020/07/22/Hive%E4%B8%ADParquet%E6%A0%BC%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Hive中Parquet格式的使用"><a href="#Hive中Parquet格式的使用" class="headerlink" title="Hive中Parquet格式的使用"></a>Hive中Parquet格式的使用</h1><p><strong>#Hive建外部External表（外部表external table）：</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`table_name`</span>(</span><br><span class="line">  <span class="string">`column1`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column2`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column3`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`proc_date`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">'hdfs://hdfscluster/...'</span></span><br><span class="line">TBLPROPERTIES ( <span class="string">'orc.compress'</span>=<span class="string">'snappy'</span>);</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#添加分区并加载分区数据：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'...'</span>（不改变源数据存储位置）</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> exsit <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'hdfs://hdfscluster/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'...'</span> <span class="keyword">into</span> <span class="keyword">table</span> table_name <span class="keyword">partition</span>(proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>);（会将源数据切到hive表指定的路径下）</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除分区：alter table table_name drop if exists partition(proc_date='$&#123;hivevar:pdate&#125;');</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>Parquet列式存储格式</title>
    <url>/2020/07/22/parquet%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
    <content><![CDATA[<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目，最新的版本是1.8.0。</p>
<a id="more"></a>

<h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>列式存储和行式存储相比有哪些优势呢？</p>
<ol>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
</ol>
]]></content>
      <categories>
        <category>Parquet</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑主键,业务主键和复合主键</title>
    <url>/2020/07/21/%E9%80%BB%E8%BE%91%E4%B8%BB%E9%94%AE,%E4%B8%9A%E5%8A%A1%E4%B8%BB%E9%94%AE%E5%92%8C%E5%A4%8D%E5%90%88%E4%B8%BB%E9%94%AE/</url>
    <content><![CDATA[<p><strong>1.概念定义</strong></p>
<ul>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms191236(v=SQL.100).aspx" target="_blank" rel="noopener">主键(PRIMARY KEY)</a>：表通常具有包含唯一标识表中每一行的值的一列或一组列。这样的一列或多列称为表的主键 (PK)，用于强制表的实体完整性。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms175464(v=SQL.100).aspx" target="_blank" rel="noopener">外键(FOREIGN KEY)</a>：外键 (FK) 是用于建立和加强两个表数据之间的链接的一列或多列。在外键引用中，当一个表的列被引用作为另一个表的主键值的列时，就在两表之间创建了链接。这个列就成为第二个表的外键。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms190639(v=SQL.100).aspx" target="_blank" rel="noopener">聚集索引</a>：聚集索引基于数据行的键值在表内排序和存储这些数据行。每个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179325(v=SQL.100).aspx" target="_blank" rel="noopener">非聚集索引</a>：非聚集索引包含索引键值和指向表数据存储位置的行定位器。可以对表或索引视图创建多个非聚集索引。通常，设计非聚集索引是为改善经常使用的、没有建立聚集索引的查询的性能。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179413(v=SQL.100).aspx" target="_blank" rel="noopener">自动编号列和标识符列</a>：对于每个表，均可创建一个包含系统生成的序号值的标识符列，该序号值以唯一方式标识表中的每一行。</p>
</li>
<li><p>业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。</p>
</li>
<li><p>逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。</p>
</li>
<li><p>复合主键（联合主键）：通过两个或者多个字段的组合作为主键。</p>
<a id="more"></a>

<p><strong>2.原理分析</strong></p>
<p>​    使用逻辑主键的主要原因是，业务主键一旦改变则系统中关联该主键的部分的修改将会是不可避免的，并且引用越多改动越大。而使用逻辑主键则只需要修改相应的业务主键相关的业务逻辑即可，减少了因为业务主键相关改变对系统的影响范围。业务逻辑的改变是不可避免的，因为“永远不变的是变化”，没有任何一个公司是一成不变的，没有任何一个业务是永远不变的。最典型的例子就是身份证升位和驾驶执照号换用身份证号的业务变更。而且现实中也确实出现了<a href="http://zhidao.baidu.com/question/22152449" target="_blank" rel="noopener">身份证号码重复</a>的情况，这样如果用身份证号码作为主键也带来了难以处理的情况。当然应对改变，可以有很多解决方案，方案之一是做一新系统与时俱进，这对软件公司来说确实是件好事。</p>
<p>​    使用逻辑主键的另外一个原因是，业务主键过大，不利于传输、处理和存储。我认为一般如果业务主键超过8字节就应该考虑使用逻辑主键了，因为int是4字节的，bigint是8字节的，而业务主键一般是字符串，同样是 8 字节的 bigint 和 8 字节的字符串在传输和处理上自然是 bigint 效率更高一些。想象一下 code == “12345678” 和 id == 12345678 的汇编码的不同就知道了。当然逻辑主键不一定是 int 或者 bigint  ，而业务主键也不一定是字符串也可以是 int 或 datetime  等类型，同时传输的也不一定就是主键，这个就要具体分析了，但是原理类似，这里只是讨论通常情况。同时如果其他表需要引用该主键的话，也需要存储该主键，那么这个存储空间的开销也是不一样的。而且这些表的这个引用字段通常就是外键，或者通常也会建索引方便查找，这样也会造成存储空间的开销的不同，这也是需要具体分析的。</p>
<p>​    使用逻辑主键的再一个原因是，使用 int 或者 bigint 作为外键进行联接查询，性能会比以字符串作为外键进行联接查询快。原理和上面的类似，这里不再重复。</p>
<p>​    使用逻辑主键的再一个原因是，存在用户或维护人员误录入数据到业务主键中的问题。例如错把 RMB 录入为 RXB  ，相关的引用都是引用了错误的数据，一旦需要修改则非常麻烦。如果使用逻辑主键则问题很好解决，如果使用业务主键则会影响到其他表的外键数据，当然也可以通过级联更新方式解决，但是不是所有都能级联得了的。</p>
<p>​    使用业务主键的主要原因是，增加逻辑主键就是增加了一个业务无关的字段，而用户通常都是对于业务相关的字段进行查找（比如员工的工号，书本的 ISBN No.  ），这样我们除了为逻辑主键加索引，还必须为这些业务字段加索引，这样数据库的性能就会下降，而且也增加了存储空间的开销。所以对于业务上确实不常改变的基础数据而言，使用业务主键不失是一个比较好的选择。另一方面，对于基础数据而言，一般的增、删、改都比较少，所以这部分的开销也不会太多，而如果这时候对于业务逻辑的改变有担忧的话，也是可以考虑使用逻辑主键的，这就需要具体问题具体分析了。</p>
<p>​    使用业务主键的另外一个原因是，对于用户操作而言，都是通过业务字段进行的，所以在这些情况下，如果使用逻辑主键的话，必须要多做一次映射转换的动作。我认为这种担心是多余的，直接使用业务主键查询就能得到结果，根本不用管逻辑主键，除非业务主键本身就不唯一。另外，如果在设计的时候就考虑使用逻辑主键的话，编码的时候也是会以主键为主进行处理的，在系统内部传输、处理和存储都是相同的主键，不存在转换问题。除非现有系统是使用业务主键，要把现有系统改成使用逻辑主键，这种情况才会存在转换问题。暂时没有想到还有什么场景是存在这样的转换的。</p>
<p>​    使用业务主键的再一个原因是，对于银行系统而言安全性比性能更加重要，这时候就会考虑使用业务主键，既可以作为主键也可以作为冗余数据，避免因为使用逻辑主键带来的关联丢失问题。如果由于某种原因导致主表和子表关联关系丢失的话，银行可是会面临无法挽回的损失的。为了杜绝这种情况的发生，业务主键需要在重要的表中有冗余存在，这种情况最好的处理方式就是直接使用业务主键了。例如身份证号、存折号、卡号等。所以通常银行系统都要求使用业务主键，这个需求并不是出于性能的考虑而是出于安全性的考虑。</p>
<p>​    使用复合主键的主要原因和使用业务主键是相关的，通常业务主键只使用一个字段不能解决问题，那就只能使用多个字段了。例如使用姓名字段不够用了，再加个生日字段。这种使用复合主键方式效率非常低，主要原因和上面对于较大的业务主键的情况类似。另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。</p>
<p>​    使用复合主键的另外一个原因是，对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。</p>
<p>综合来说，网上大多数人是倾向于用逻辑主键的，而对于实体表用复合主键方式的应该没有多少人认同。支持业务主键的人通常有种误解，认为逻辑主键必须对用户来说有意义，其实逻辑主键只是系统内部使用的，对用户来说是无需知道的。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>主键</tag>
      </tags>
  </entry>
  <entry>
    <title>hive两个聚合函数的计算结果拼接成表并做进一步计算</title>
    <url>/2020/07/23/%E4%B8%A4%E4%B8%AA%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E6%8B%BC%E6%8E%A5/</url>
    <content><![CDATA[<h4 id="hive两个聚合函数的计算结果拼接成表并做进一步计算"><a href="#hive两个聚合函数的计算结果拼接成表并做进一步计算" class="headerlink" title="hive两个聚合函数的计算结果拼接成表并做进一步计算"></a>hive两个聚合函数的计算结果拼接成表并做进一步计算</h4><p>hive两个聚合函数的计算结果拼接成表让LZ头疼了很久，一度想到用python处理，或者新建两张临时表保存聚合函数的结果然后再取出数据进行计算，或者使用UDF, 但总觉得还有其他方法。经过一番探索，发现WITH AS 可以方便快捷解决此问题。</p>
<a id="more"></a>

<p>WITH AS短语，也叫做子查询部分（subquery factoring），可以让你做很多事情，定义一个SQL片断，该SQL片断会被整个SQL语句所用到。</p>
<p>需求：统计test1表满足某条件的记录数和test2表满足某条件的记录数然后做除法。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> c1</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*） <span class="keyword">as</span> aa <span class="keyword">from</span> test1 ),</span><br><span class="line">c2</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*）　<span class="keyword">as</span> bb <span class="keyword">from</span> test2)</span><br><span class="line"><span class="keyword">select</span> a.aa/b.bb <span class="keyword">from</span> c1 a, c2 b ;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title>hive on spark与spark on hive的区别</title>
    <url>/2020/07/23/Hive%20on%20Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h3 id="SparkSQL简介"><a href="#SparkSQL简介" class="headerlink" title="SparkSQL简介"></a>SparkSQL简介</h3><p>SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，hive应运而生，它是当时唯一运行在Hadoop上的SQL-on-hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，Shark应运而生，但又因为Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等),2014年spark团队停止对Shark的开发，将所有资源放SparkSQL项目上</p>
<a id="more"></a>

<h3 id="SparkSQL、Hive-on-Spark的关系"><a href="#SparkSQL、Hive-on-Spark的关系" class="headerlink" title="SparkSQL、Hive on Spark的关系"></a><strong>SparkSQL、Hive on Spark的关系</strong></h3><p><img src="https://i.loli.net/2020/07/23/UPVGS1qbLynB639.png" alt="image-20200515185457943.png"></p>
<p>由上图可以看出，SparkSQL之所以要从Shark中孵化出来，初衷就是为了剥离Shark对于Hive的太多依赖。SparkSQL作为Spark生态中独立的一员继续发展，不在受限于Hive，只是兼容Hive；而Hive on  Spark是Hive的发展计划，该计划将Spark作为Hive最底层的引擎之一，Hive不在受限于一个引擎（之前只支持map-reduce），可以采用map-reduce、Tez、Spark等计算引擎。</p>
<p>hive on  Spark是有Cloudera发起，有Intel、MapR等公司共同参与的开源项目，其目的就是将Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上面进行计算。通过该项目，可以提高Hive查询的性能，同事为已经部署了Hive或者Spark的用户提供了更加灵活地选择，从而进一步提高Hive和Spark的普及率。</p>
<p>hive on Spark和SparkSQL的结构类似，只是SQL引擎不同，但是计算引擎都是spark</p>
<p>sparkSQL通过sqlcontext来进行使用，hive on  spark通过hivecontext来使用。sqlcontext和hivecontext都是来自于同一个包，从这个层面上理解，其实hive on spark和sparkSQL并没有太大差别。</p>
<p>结构上来看，Hive on Spark和SparkSQL都是一个翻译曾，将SQL翻译成分布是可以执行的Spark程序。</p>
<p>SQLContext：spark处理结构化数据的入口，允许创建DataFrame以及sql查询。</p>
<p>HiveContext：Spark sql执行引擎，集成hive数据，读取在classpath的hive-site.xml配置文件配置hive。所以ye</p>
<h3 id="SparkSQL组件和运行架构"><a href="#SparkSQL组件和运行架构" class="headerlink" title="SparkSQL组件和运行架构"></a><strong>SparkSQL组件和运行架构</strong></h3><p>1-SQLContext：Spark SQL提供SQLContext封装Spark中的所有关系型功能。可以用之前的示例中的现有SparkContext创建SQLContext。<br>2-DataFrame：DataFrame是一个分布式的，按照命名列的形式组织的数据集合。DataFrame基于R语言中的data frame概念，与关系型数据库中的数据库表类似。通过调用将DataFrame的内容作为行RDD（RDD of  Rows）返回的rdd方法，可以将DataFrame转换成RDD。可以通过如下数据源创建DataFrame：已有的RDD、结构化数据文件、JSON数据集、Hive表、外部数据库。<br>了私语关系型数据库，SparkSQL中的SQL语句也是由Projection、Data source、Filter但部分组成，分别对应于sql查询过程中的Result、Data  source和Operation；SQL语句是按照Operation-》Data Source -》Result的次序来描述的。如下所示：</p>
<p><img src="https://i.loli.net/2020/07/23/XQZNzEvBbMxS6r1.png" alt="image-20200515185523352.png"></p>
<p>下面对上图中展示的SparkSQL语句的执行顺序进行详细解释：</p>
<p>1-对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范；<br>Projection：简单说就是select选择的列的集合，参考：SQL Projection<br>2-将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的；<br>3-一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）；<br>4-计划执行（Execute），按Operation–&gt;Data Source–&gt;Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。</p>
<h3 id="SQLContext和HiveContext"><a href="#SQLContext和HiveContext" class="headerlink" title="SQLContext和HiveContext"></a><strong>SQLContext和HiveContext</strong></h3><p>当使用SparkSQL时，根据是否要使用Hive，有两个不同的入口。推荐使用入口HiveContext，HiveContext继承自SQLContext。它可以提供HiveQL以及其他依赖于Hive的功能的支持。更为基础的SQLContext则仅仅支持SparlSQL功能的一个子集，子集中去掉了需要依赖Hive的功能。这种分离主要视为那些可能会因为引入Hive的全部依赖而陷入依赖冲突的用户而设计的。因为使用HiveContext的时候不需要事先部署好Hive。如果要把一个Spark  SQL链接到部署好的Hive上面，必须将hive-site.xml复制到Spark的配置文件目录中（$SPARK_HOME/conf）。即使没有部署好Hive，SparkSQL也可以运行，如果没有部署好Hive，但是还要使用HiveContext的话，那么SparkSQL将会在当前的工作目录中创建出自己的Hive元数据仓库，叫做metastore_db。，如果使用HiveQL中的CREATETABLE语句来创建表，那么这些表将会被放在默认的文件系统中的/user/hive/warehouse目录中，这里默认的文件系统视情况而定，如果配置了hdfs-site.xml那么就会存放在HDFS上面，否则就存放在本地文件系统中。</p>
<p>运行HiveContext的时候hive环境并不是必须，但是需要hive-site.xml配置文件。</p>
<hr>
<h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>hive on spark大体与SparkSQL结构类似，只是SQL引擎不同，但是计算引擎都是spark！</p>
<p>在pyspark中使用Hive on Spark是中怎么样的体验</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化Spark SQL</span></span><br><span class="line"><span class="comment">#导入Spark SQL</span></span><br><span class="line">from pyspark.sql import HiveContext,Row</span><br><span class="line"><span class="comment"># 当不能引入Hive依赖时</span></span><br><span class="line"><span class="comment"># from pyspark.sql import SQLContext,Row</span></span><br><span class="line"><span class="comment"># 注意，上面那一点才是关键的，他两来自于同一个包，你们区别能有多大</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hiveCtx = HiveContext(sc)   <span class="comment">#创建SQL上下文环境</span></span><br><span class="line">input = hiveCtx.jsonFile(inputFile)   <span class="comment">#基本查询示例</span></span><br><span class="line">input.registerTempTable("tweets")   <span class="comment">#注册输入的SchemaRDD（SchemaRDD在Spark 1.3版本后已经改为DataFrame）</span></span><br><span class="line"><span class="comment">#依据retweetCount(转发计数)选出推文</span></span><br><span class="line">topTweets = hiveCtx.sql("<span class="keyword">SELECT</span> <span class="built_in">text</span>,retweetCount <span class="keyword">FROM</span> tweets <span class="keyword">ORDER</span> <span class="keyword">BY</span> retweetCount <span class="keyword">LIMIT</span> <span class="number">10</span><span class="string">")</span></span><br></pre></td></tr></table></figure>

<p>我们可以看到，sqlcontext和hivecontext都是出自于pyspark.sql包，可以从这里理解的话，其实hive on spark和sparksql并没有太大差别</p>
<p>结构上Hive On Spark和SparkSQL都是一个翻译层，把一个SQL翻译成分布式可执行的Spark程序。而且大家的引擎都是spark</p>
<p>SparkSQL和Hive On  Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On  Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。—From <a href="http://blog.csdn.net/yeruby/article/details/51448188" target="_blank" rel="noopener">SparkSQL与Hive on Spark的比较</a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive on spark</tag>
        <tag>spark on hive</tag>
      </tags>
  </entry>
  <entry>
    <title>mac连接公司vpn的坑</title>
    <url>/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>1.出现IPSec共享密匙丢失</p>
<p>2.能访问公司内网,无法访问公网</p>
<a id="more"></a>

<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h3 id="1-出现IPSec共享密匙丢失"><a href="#1-出现IPSec共享密匙丢失" class="headerlink" title="1.出现IPSec共享密匙丢失"></a>1.出现IPSec共享密匙丢失</h3><p><img src="https://i.loli.net/2020/07/24/FEHZDY59BSnXNCP.png" alt="image-20200724171339894.png"></p>
<h4 id="原因"><a href="#原因" class="headerlink" title="原因:"></a>原因:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;etc&#x2F;ppp&#x2F;options</span><br><span class="line">包含缺省应用于系统中所有 PPP 链路的特征（例如，计算机是否要求对等点对其本身进行验证）的文件。如果不存在此文件，将禁止非超级用户使用 PPP。</span><br></pre></td></tr></table></figure>

<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>知其然，所以知其后然，这时候的解决方法就是在<code>/etc/ppp/</code>目录下建立options`这个配置文件，并且配置ppp链路l2tp不需要ipsec密钥。</p>
<p>下面就是vim命令操作，如果想系统学习相关命令可查看 <a href="https://link.jianshu.com?t=http%3A%2F%2Fwww.cnblogs.com%2Fpeida%2Farchive%2F2012%2F12%2F05%2F2803591.html" target="_blank" rel="noopener">每天一个linux命令目录</a>，这里不打算详细讲解，有兴趣同学可以另行学习。</p>
<p><strong>操作步骤</strong><br> （1）在终端任意路径下输入命令： <code>sudo vim /etc/ppp/options</code><br> 然后输入电脑密码后，显示vim操作界面后按键盘<code>i</code>进入插入模式，输入下面内容：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">plugin</span> <span class="selector-tag">L2TP</span><span class="selector-class">.ppp</span></span><br><span class="line"><span class="selector-tag">l2tpnoipsec</span></span><br></pre></td></tr></table></figure>

<p>（2）然后按<code>esc</code>键退出插入模式，最后输入<code>:wq!</code>，强制保存并退出vim模式。</p>
<h3 id="2-能访问公司内网-无法访问公网"><a href="#2-能访问公司内网-无法访问公网" class="headerlink" title="2.能访问公司内网,无法访问公网"></a>2.能访问公司内网,无法访问公网</h3><p>在vpn的高级里设置:</p>
<p>1.</p>
<p><img src="https://i.loli.net/2020/07/24/v3IlwSyEFjTxR95.png" alt="1595581415270-28ce9bf1-e286-40a9-b786-cc651df782a1.png"></p>
<p>2.</p>
<p><img src="https://i.loli.net/2020/07/24/fDRpcNP2a79kMUS.png" alt="1595581458710-f9efd0df-5447-4ac2-8784-e448e2f344b9.png"></p>
<p>即可</p>
]]></content>
      <categories>
        <category>公司vpn</category>
      </categories>
      <tags>
        <tag>远程访问</tag>
        <tag>公司vpn</tag>
      </tags>
  </entry>
  <entry>
    <title>同比与环比</title>
    <url>/2020/07/24/%E5%90%8C%E6%AF%94%E7%8E%AF%E6%AF%94/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>同比：本期与同期做对比。</p>
<p>环比：本期与上期做对比。</p>
<p>简单点说，同比和环比用于表示某一事物在对比时期内发展变化的方向和程度。以历史同期为基期，例如2016年2月份与2015年2月份、2016年上半年与2015年上半年的比较，就是同比。以前一个统计时间段为基期，例如2016年6月份与2016年5月份、2016年二季度与2016年一季度的比较，就是环比。</p>
<p><img src="https://i.loli.net/2020/07/24/c5EoSijy31QIDAx.png" alt="image-20200515173201858.png"></p>
<a id="more"></a>

<p>环比一般是用在月、日很少用在年上，主要是对比很短时间内涨幅程度，不过由于行业差异，比如旅游，会受到淡旺季影响。</p>
<p>同比一般用在相邻两年，相同时间段内，查看涨幅程度，一般用在两年相同月份，很少用在两月相同日期</p>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><h3 id="一、同比增长计算公式："><a href="#一、同比增长计算公式：" class="headerlink" title="一、同比增长计算公式："></a>一、同比增长计算公式：</h3><p>1、同比增长率=（本期数－同期数）÷同期数×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值300万，同比增长率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<p>2、当同期数为负值的情况，公式应当完善如下：</p>
<p>同比增长率=（本期数－同期数）÷ |同期数|×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值50万，同比增长率是多少？</p>
<p>本题中，同比增长率=(50W-(-100W))/|-100W||×100%=150%</p>
<h3 id="二、环比增长计算公式："><a href="#二、环比增长计算公式：" class="headerlink" title="二、环比增长计算公式："></a>二、<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>计算公式：</h3><p><a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率=（本期数-上期数）/上期数×100%。</p>
<p>例子：比如说今年3月的产值100万，2月的产值300万，<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>同比与环比的区别</p>
<p>同比、环比与定基比，都可以用百分数或倍数表示。定基比发展速度，也简称总速度，一般是指报告期水平与某一固定时期水平之比，表明这种现象在较长时期内总的发展速度。</p>
<p>同比发展速度，一般指是指本期发展水平与<a href="https://www.baidu.com/s?wd=上年同期&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">上年同期</a>发展水平对比，而达到的相对发展速度。环比发展速度，一般是指报告期水平与前一时期水平之比，表明现象逐期的发展速度。</p>
<p>同比和环比，这两者所反映的虽然都是变化速度，但由于采用基期的不同，其反映的内涵是完全不同的；一般来说，环比可以与环比相比较，而不能拿同比与环比相比较；而对于同一个地方，考虑时间纵向上发展趋势的反映，则往往要把同比与环比放在一起进行对照。</p>
]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>同比/环比</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习项目01-鸢尾花分类</title>
    <url>/2020/07/24/ML_flower/</url>
    <content><![CDATA[<p>机器学习项目之鸢尾花分类:</p>
<h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述:"></a>任务描述:</h3><p>构建一个模型，根据鸢尾花的花萼和花瓣大小将其分为三种不同的品种。</p>
<p><img src="https://ai-studio-static-online.cdn.bcebos.com/dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c" alt="img"></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> colors     </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm            </span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">'/Users/taoxuefeng/Desktop/jupyterlab/jupyterlab_python/MLData/flower_data/iris.data'</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iris_type</span><span class="params">(s)</span>:</span></span><br><span class="line">    it = &#123;<span class="string">b'Iris-setosa'</span>:<span class="number">0</span>, <span class="string">b'Iris-versicolor'</span>:<span class="number">1</span>, <span class="string">b'Iris-virginica'</span>:<span class="number">2</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> it[s]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.loadtxt(data_path,                <span class="comment">#数据文件路径</span></span><br><span class="line">                 dtype=float,               <span class="comment">#数据文件类型</span></span><br><span class="line">                 delimiter=<span class="string">','</span>,             <span class="comment">#数据分隔符</span></span><br><span class="line">                 converters=&#123;<span class="number">4</span>:iris_type&#125;)  <span class="comment">#将第5列使用函数iris_type进行转换</span></span><br><span class="line"><span class="comment">#print(data)</span></span><br><span class="line"><span class="comment">#print(data.shape)#(150, 5)</span></span><br><span class="line">x, y = np.split(data,     <span class="comment">#要切分的数组</span></span><br><span class="line">               (<span class="number">4</span>,),      <span class="comment">#沿轴切分的位置，第5列开始往后为y</span></span><br><span class="line">               axis = <span class="number">1</span>)  <span class="comment">#代表纵向分割，按列分割</span></span><br><span class="line">x = x[:, <span class="number">0</span>:<span class="number">2</span>]<span class="comment">#在X中我们取前两列作为特征，为了后面的可视化。x[:,0:2]代表第一维(行)全取，第二维(列)取0~2</span></span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line">x_train, x_test, y_train, y_test = model_selection.train_test_split(x,<span class="comment">#索要划分的样本特征集</span></span><br><span class="line">                                                                   y,<span class="comment">#所要划分的样本结果</span></span><br><span class="line">                                                                   random_state = <span class="number">1</span>,<span class="comment">#随机数种子</span></span><br><span class="line">                                                                   test_size = <span class="number">0.3</span>) <span class="comment">#测试样本占比</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**********************SVM分类器构建*************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#clf = svm.SVC(C=0.8,kernel='rbf', gamma=50,decision_function_shape='ovr')</span></span><br><span class="line">    clf = svm.SVC(C=<span class="number">0.5</span>,                         <span class="comment">#误差项惩罚系数,默认值是1</span></span><br><span class="line">                  kernel=<span class="string">'linear'</span>,               <span class="comment">#线性核 kenrel="rbf":高斯核</span></span><br><span class="line">                  decision_function_shape=<span class="string">'ovr'</span>) <span class="comment">#决策函数</span></span><br><span class="line">    <span class="keyword">return</span> clf</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.定义模型：SVM模型定义</span></span><br><span class="line">clf = classifier()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练SVM模型</span></span><br><span class="line">train(clf,x_train,y_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**************并判断a b是否相等，计算acc的均值*************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span><span class="params">(a, b, tip)</span>:</span></span><br><span class="line">    acc = a.ravel() == b.ravel()</span><br><span class="line">    print(<span class="string">'%s Accuracy:%.3f'</span> %(tip, np.mean(acc)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_accuracy</span><span class="params">(clf,x_train,y_train,x_test,y_test)</span>:</span></span><br><span class="line">    <span class="comment">#分别打印训练集和测试集的准确率  score(x_train,y_train):表示输出x_train,y_train在模型上的准确率</span></span><br><span class="line">    print(<span class="string">'trianing prediction:%.3f'</span> %(clf.score(x_train, y_train)))</span><br><span class="line">    print(<span class="string">'test data prediction:%.3f'</span> %(clf.score(x_test, y_test)))</span><br><span class="line">    <span class="comment">#原始结果与预测结果进行对比   predict()表示对x_train样本进行预测，返回样本类别</span></span><br><span class="line">    show_accuracy(clf.predict(x_train), y_train, <span class="string">'traing data'</span>)</span><br><span class="line">    show_accuracy(clf.predict(x_test), y_test, <span class="string">'testing data'</span>)</span><br><span class="line">    <span class="comment">#计算决策函数的值，表示x到各分割平面的距离</span></span><br><span class="line">    print(<span class="string">'decision_function:\n'</span>, clf.decision_function(x_train))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.模型评估</span></span><br><span class="line">print_accuracy(clf,x_train,y_train,x_test,y_test)</span><br></pre></td></tr></table></figure>

<pre><code>trianing prediction:0.819
test data prediction:0.778
traing data Accuracy:0.819
testing data Accuracy:0.778
decision_function:
 [[-0.30200388  1.26702365  2.28292526]
 [ 2.1831931  -0.19913458  1.06956422]
 [ 2.25424706  0.79489006 -0.20587224]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 0.95815482  2.18401419 -0.17375192]
 [ 2.23120771  0.84075865 -0.19144453]
 [ 2.17327158 -0.14884286  0.92795057]
 [-0.28667175  1.11372202  2.28302495]
 [-0.27989264  1.21274017  2.25881762]
 [-0.29313813  1.24442795  2.2732035 ]
 [-0.27008816  1.2272086   2.22682127]
 [-0.25981661  2.21998499  1.20479842]
 [-0.17071168  0.99542159  2.17180911]
 [-0.30018876  1.25829325  2.2829419 ]
 [-0.17539342  2.15368837  1.06772814]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.23988847  2.23286001  1.06656755]
 [-0.26915223  2.23333222  1.21679709]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 2.2530903   0.85932358 -0.2359772 ]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.26803658  0.80468578 -0.24299359]
 [-0.24030826  1.18556963  2.19011259]
 [-0.25881807  1.17240759  2.23535197]
 [-0.27273902  1.20332527  2.24866913]
 [-0.20956348  2.19674141  1.06726512]
 [-0.26556065  1.16490628  2.24871607]
 [-0.22965507  1.17870942  2.17146651]
 [ 2.25807657 -0.22526231  0.80881977]
 [-0.27322701  2.25917947  1.17077691]
 [-0.26638767  1.21631409  2.22685842]
 [-0.26740532  1.20784059  2.23528903]
 [-0.12135744  2.22922779  0.79343961]
 [-0.2365929   1.12219635  2.21706342]
 [-0.21558048  2.22640865  0.92573306]
 [ 2.22344499 -0.19955645  0.88288227]
 [ 2.22671228  0.93600592 -0.21794279]
 [ 2.26578978 -0.24701281  0.82742467]
 [-0.26556065  1.16490628  2.24871607]
 [ 2.26204658  0.89725133 -0.25453765]
 [-0.2518152   2.22343258  1.17120859]
 [-0.27340098  1.23624732  2.22678409]
 [-0.21624631  2.17118121  1.14723861]
 [ 2.22874494 -0.17513313  0.8269183 ]
 [ 2.2211989   0.87213971 -0.19151045]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.22671228  0.93600592 -0.21794279]
 [-0.29609931  1.25285329  2.27596663]
 [-0.25476857  1.20746943  2.20485252]
 [-0.29672783  1.24461331  2.28083131]
 [-0.27578664  1.21663499  2.24864564]
 [-0.28091389  2.25930846  1.21661886]
 [-0.21369288  1.05233452  2.20512234]
 [-0.27669555  1.12529292  2.27023906]
 [-0.16942442  2.17056098  0.99533295]
 [ 2.24933086 -0.25468768  1.0709247 ]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.18638944  1.20994285 -0.24936796]
 [-0.22656825  2.23557826  0.92551338]
 [-0.27989264  1.21274017  2.25881762]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.28390119  1.23920595  2.25400509]
 [ 2.24837463  0.81114157 -0.20592544]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.22765797  1.07419821  2.21710769]
 [-0.18996302  2.19089984  0.99497945]
 [-0.27357394  1.19278157  2.25408746]
 [ 2.23355717  0.86019975 -0.2060317 ]
 [ 2.25277813 -0.21394322  0.80875361]
 [-0.18611572  1.10670475  2.14746524]
 [ 2.25454797  0.88341904 -0.24307373]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.23794605  0.91585392 -0.22774264]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.0914977   1.20089769 -0.21820392]
 [ 2.25962348  0.84878847 -0.24304703]
 [-0.25213485  1.16423702  2.22696973]
 [ 2.26725005  0.88232062 -0.25923379]
 [-0.14201734  2.14344591  0.99568721]
 [ 2.25731     0.95572321 -0.25455798]
 [-0.22656825  2.23557826  0.92551338]
 [-0.19708433  2.25161696  0.79328185]
 [ 2.23957622  0.81769302 -0.19137855]
 [ 2.21575566  1.0173258  -0.21798639]
 [ 1.02668315  2.21468275 -0.21824732]
 [ 2.27472592  0.77777882 -0.24294008]
 [-0.21624631  2.17118121  1.14723861]
 [-0.24730284  1.20252603  2.19004536]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.27273902  1.20332527  2.24866913]
 [-0.19455078  2.17814555  1.06749683]
 [-0.28027257  2.2623408   1.20447285]
 [-0.28054312  1.20372124  2.26304729]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.17896853 -0.12686338  0.8824238 ]
 [ 2.19820639  1.04471124 -0.20619077]
 [-0.26313706  2.23602532  1.18984329]
 [-0.25331913  2.21599142  1.18997806]
 [-0.28966527  1.23403227  2.27016072]
 [-0.23157808  2.22314802  1.06680048]
 [-0.26533811  1.22371567  2.21684157]
 [-0.25751543  1.18608093  2.22693265]
 [-0.27562627  2.24825903  1.21670804]
 [-0.27273902  1.20332527  2.24866913]
 [ 2.22671228  0.93600592 -0.21794279]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(clf, x)</span>:</span></span><br><span class="line">    iris_feature = <span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>, <span class="string">'petal lenght'</span>, <span class="string">'petal width'</span></span><br><span class="line">    <span class="comment"># 开始画图</span></span><br><span class="line">    x1_min, x1_max = x[:, <span class="number">0</span>].min(), x[:, <span class="number">0</span>].max()               <span class="comment">#第0列的范围</span></span><br><span class="line">    x2_min, x2_max = x[:, <span class="number">1</span>].min(), x[:, <span class="number">1</span>].max()               <span class="comment">#第1列的范围</span></span><br><span class="line">    x1, x2 = np.mgrid[x1_min:x1_max:<span class="number">200j</span>, x2_min:x2_max:<span class="number">200j</span>]   <span class="comment">#生成网格采样点</span></span><br><span class="line">    grid_test = np.stack((x1.flat, x2.flat), axis=<span class="number">1</span>)            <span class="comment">#stack():沿着新的轴加入一系列数组</span></span><br><span class="line">    print(<span class="string">'grid_test:\n'</span>, grid_test)</span><br><span class="line">    <span class="comment"># 输出样本到决策面的距离</span></span><br><span class="line">    z = clf.decision_function(grid_test)</span><br><span class="line">    print(<span class="string">'the distance to decision plane:\n'</span>, z)</span><br><span class="line">    </span><br><span class="line">    grid_hat = clf.predict(grid_test)                           <span class="comment"># 预测分类值 得到【0,0.。。。2,2,2】</span></span><br><span class="line">    print(<span class="string">'grid_hat:\n'</span>, grid_hat)  </span><br><span class="line">    grid_hat = grid_hat.reshape(x1.shape)                       <span class="comment"># reshape grid_hat和x1形状一致</span></span><br><span class="line">                                                                <span class="comment">#若3*3矩阵e，则e.shape()为3*3,表示3行3列   </span></span><br><span class="line"> </span><br><span class="line">    cm_light = mpl.colors.ListedColormap([<span class="string">'#A0FFA0'</span>, <span class="string">'#FFA0A0'</span>, <span class="string">'#A0A0FF'</span>])</span><br><span class="line">    cm_dark = mpl.colors.ListedColormap([<span class="string">'g'</span>, <span class="string">'b'</span>, <span class="string">'r'</span>])</span><br><span class="line"> </span><br><span class="line">    plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)                                   <span class="comment"># pcolormesh(x,y,z,cmap)这里参数代入</span></span><br><span class="line">                                                                                      <span class="comment"># x1，x2，grid_hat，cmap=cm_light绘制的是背景。</span></span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=np.squeeze(y), edgecolor=<span class="string">'k'</span>, s=<span class="number">50</span>, cmap=cm_dark) <span class="comment"># 样本点</span></span><br><span class="line">    plt.scatter(x_test[:, <span class="number">0</span>], x_test[:, <span class="number">1</span>], s=<span class="number">120</span>, facecolor=<span class="string">'none'</span>, zorder=<span class="number">10</span>)       <span class="comment"># 测试点</span></span><br><span class="line">    plt.xlabel(iris_feature[<span class="number">0</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(iris_feature[<span class="number">1</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.title(<span class="string">'svm in iris data classification'</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.模型使用</span></span><br><span class="line">draw(clf,x)</span><br></pre></td></tr></table></figure>

<pre><code>grid_test:
 [[4.3       2.       ]
 [4.3       2.0120603]
 [4.3       2.0241206]
 ...
 [7.9       4.3758794]
 [7.9       4.3879397]
 [7.9       4.4      ]]
the distance to decision plane:
 [[ 2.17689921  1.23467171 -0.25941323]
 [ 2.17943684  1.23363096 -0.25941107]
 [ 2.18189345  1.23256802 -0.25940892]
 ...
 [-0.27958977  0.83621535  2.28683228]
 [-0.27928358  0.8332275   2.28683314]
 [-0.27897389  0.83034313  2.28683399]]
grid_hat:
 [0. 0. 0. ... 2. 2. 2.]


/Users/taoxuefeng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: MatplotlibDeprecationWarning: shading=&apos;flat&apos; when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading=&apos;auto&apos;, &apos;nearest&apos; or &apos;gouraud&apos;, or set rcParams[&apos;pcolor.shading&apos;].  This will become an error two minor releases later.</code></pre><p><img src="https://i.loli.net/2020/07/24/tRZPjJlhgFT5YAV.png" alt="output_13_2.png"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>鸢尾花分类</tag>
        <tag>机器学习</tag>
        <tag>Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>测试</title>
    <url>/2020/07/23/%E6%B5%8B%E8%AF%95/</url>
    <content><![CDATA[<p><img src="/2020/07/23/%E6%B5%8B%E8%AF%95/output_13_2.png" alt="output_13_2"></p>
]]></content>
  </entry>
</search>
