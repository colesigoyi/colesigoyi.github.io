<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hive2的启动</title>
    <url>/2020/06/26/hive2%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<ol>
<li>启动hive的服务<br>hive ‐‐service hiveserver2 &amp;</li>
<li>使用beeline，去连接thrift的服务<br>beeline -u jdbc:hive2://CentOS-01:10000</li>
</ol>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>服务</tag>
        <tag>beeline</tag>
      </tags>
  </entry>
  <entry>
    <title>数据质量检查</title>
    <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E6%A3%80%E6%9F%A5/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="1-数据质量检查"><a href="#1-数据质量检查" class="headerlink" title="1.数据质量检查"></a>1.数据质量检查</h1><p>是在完成宽表数据开发后进行的，主要包括四个方面：</p>
<ol>
<li><p>重复值检查</p>
</li>
<li><p>缺失值检查</p>
</li>
<li><p>数据倾斜问题</p>
</li>
<li><p>异常值检查</p>
<a id="more"></a>

</li>
</ol>
<h2 id="1-重复值检查"><a href="#1-重复值检查" class="headerlink" title="1. 重复值检查"></a>1. 重复值检查</h2><h3 id="1-1-什么是重复值"><a href="#1-1-什么是重复值" class="headerlink" title="1.1 什么是重复值"></a>1.1 什么是重复值</h3><p>重复值的检查首先要明确一点，即重复值的定义。对于一份二维表形式的数据集来说，什么是重复值？主要有两个层次：<br>① 关键字段出现重复记录，比如主索引字段出现重复；<br>② 所有字段出现重复记录。<br>第一个层次是否是重复，必须从这份数据的业务含义进行确定。比如一张表，从业务上讲，一个用户应该只会有一条记录，那么如果某个用户出现了超过一条的记录，那么这就是重复值。第二个层次，就一定是重复值了。</p>
<h3 id="1-2-重复值产生的原因"><a href="#1-2-重复值产生的原因" class="headerlink" title="1.2 重复值产生的原因"></a>1.2 重复值产生的原因</h3><p>重复值的产生主要有两个原因，一是上游源数据造成的，二是数据准备脚本中的数据关联造成的。从数据准备角度来看，首先检查数据准备的脚本，判断使用的源表是否有重复记录，同时检查关联语句的正确性和严谨性，比如关联条件是否合理、是否有限定数据周期等等。<br>比如：检查源表数据是否重复的SQL：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> MON_ID,<span class="keyword">COUNT</span>(*),<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> USER_ID)</span><br><span class="line"><span class="keyword">FROM</span> TABLE_NAME</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> MON_ID;</span><br></pre></td></tr></table></figure>

<p>如果是上游源数据出现重复，那么应该及时反映给上游进行修正；如果是脚本关联造成的，修改脚本，重新生成数据即可。<br>还有一份情况，这份数据集是一份单独的数据集，并不是在数据仓库中开发得到的数据，既没有上游源数据，也不存在生成数据的脚本，比如公开数据集，那么如何处理其中的重复值？一般的处理方式就是直接删除重复值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">dataset = pd.read_excel(<span class="string">"/labcenter/python/dataset.xlsx"</span>)</span><br><span class="line"><span class="comment">#判断重复数据</span></span><br><span class="line">dataset.duplicated()    <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.duplicated([<span class="string">'col2'</span>])    <span class="comment">#col2字段重复是重复数据</span></span><br><span class="line"><span class="comment">#删除重复数据</span></span><br><span class="line">dataset.drop_duplicates()     <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.drop_duplicates([<span class="string">'col2'</span>])   <span class="comment">#col2字段重复是重复数据</span></span><br></pre></td></tr></table></figure>

<h2 id="2-缺失值检查"><a href="#2-缺失值检查" class="headerlink" title="2. 缺失值检查"></a>2. 缺失值检查</h2><p>缺失值主要是指数据集中部分记录存在部分字段的信息缺失。</p>
<h3 id="2-1-缺失值出现的原因"><a href="#2-1-缺失值出现的原因" class="headerlink" title="2.1 缺失值出现的原因"></a>2.1 缺失值出现的原因</h3><p>出现趋势值主要有三种原因：<br>① 上游源系统因为技术或者成本原因无法完全获取到这一信息，比如对用户手机APP上网记录的解析；<br>② 从业务上讲，这一信息本来就不存在，比如一个学生的收入，一个未婚者的配偶姓名；<br>③ 数据准备脚本开发中的错误造成的。<br>第一种原因，短期内无法解决；第二种原因，数据的缺失并不是错误，无法避免；第三种原因，则只需通过查证修改脚本即可。<br>缺失值的存在既代表了某一部分信息的丢失，也影响了挖掘分析结论的可靠性与稳定性，因此，必须对缺失值进行处理。<br>如果缺失值记录数超过了全部记录数的50%，则应该从数据集中直接剔除掉该字段，尝试从业务上寻找替代字段；<br>如果缺失值记录数没有超过50%，则应该首先看这个字段在业务上是否有替代字段，如果有，则直接剔除掉该字段，如果没有，则必须对其进行处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看哪些字段有缺失值   </span></span><br><span class="line">dataset.isnull().any()    <span class="comment">#获取含有NaN的字段</span></span><br><span class="line"><span class="comment">#统计各字段的缺失值个数</span></span><br><span class="line">dataset.isnull().apply(pd.value_counts)</span><br><span class="line"><span class="comment">#删除含有缺失值的字段</span></span><br><span class="line">nan_col = dataset.isnull().any()</span><br><span class="line">dataset.drop(nan_col[nan_col].index,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-缺失值的处理"><a href="#2-2-缺失值的处理" class="headerlink" title="2.2 缺失值的处理"></a>2.2 缺失值的处理</h3><p>缺失值的处理主要有两种方式：过滤和填充。</p>
<h4 id="（1）缺失值的过滤"><a href="#（1）缺失值的过滤" class="headerlink" title="（1）缺失值的过滤"></a>（1）缺失值的过滤</h4><p>直接删除含有缺失值的记录，总体上会影响样本个数，如果删除样本过多或者数据集本来就是小数据集时，这种方式并不建议采用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除含有缺失值的记录</span></span><br><span class="line">dataset.dropna()</span><br></pre></td></tr></table></figure>

<h4 id="2）缺失值的填充"><a href="#2）缺失值的填充" class="headerlink" title="2）缺失值的填充"></a>2）缺失值的填充</h4><p>缺失值的填充主要三种方法：<br>① 方法一：使用特定值填充<br>使用缺失值字段的平均值、中位数、众数等统计量填充。<br>优点：简单、快速<br>缺点：容易产生数据倾斜<br>② 方法二：使用算法预测填充<br>将缺失值字段作为因变量，将没有缺失值字段作为自变量，使用决策树、随机森林、KNN、回归等预测算法进行缺失值的预测，用预测结果进行填充。<br>优点：相对精确<br>缺点：效率低，如果缺失值字段与其他字段相关性不大，预测效果差<br>③ 方法三：将缺失值单独作为一个分组，指定值进行填充<br>从业务上选择一个单独的值进行填充，使缺失值区别于其他值而作为一个分组，从而不影响算法计算。<br>优点：简单，实用<br>缺点：效率低</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用Pandas进行特定值填充</span></span><br><span class="line">dataset.fillna(<span class="number">0</span>)   <span class="comment">#不同字段的缺失值都用0填充</span></span><br><span class="line">dataset.fillna(&#123;<span class="string">'col2'</span>:<span class="number">20</span>,<span class="string">'col5'</span>:<span class="number">0</span>&#125;)    <span class="comment">#不同字段使用不同的填充值</span></span><br><span class="line">dataset.fillna(dataset.mean())   <span class="comment">#分别使用各字段的平均值填充</span></span><br><span class="line">dataset.fillna(dataset.median())     <span class="comment">#分别使用个字段的中位数填充</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用sklearn中的预处理方法进行缺失值填充(只适用于连续型字段)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">dataset2 = dataset.drop([<span class="string">'col4'</span>],axis=<span class="number">1</span>)</span><br><span class="line">colsets = dataset2.columns</span><br><span class="line">nan_rule1 = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)    <span class="comment">#创建填充规则(平均值填充)</span></span><br><span class="line">pd.DataFrame(nan_rule1.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule2 = Imputer(missing_values=<span class="string">'median'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>) <span class="comment">#创建填充规则(中位数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule2.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule3 = Imputer(missing_values=<span class="string">'most_frequent'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)  <span class="comment">#创建填充规则(众数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule3.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br></pre></td></tr></table></figure>

<h2 id="3-数据倾斜问题"><a href="#3-数据倾斜问题" class="headerlink" title="3. 数据倾斜问题"></a>3. 数据倾斜问题</h2><p>数据倾斜是指字段的取值分布主要集中在某个特定类别或者特定区间。</p>
<h3 id="3-1-数据倾斜问题的原因"><a href="#3-1-数据倾斜问题的原因" class="headerlink" title="3.1 数据倾斜问题的原因"></a>3.1 数据倾斜问题的原因</h3><p>出现这一问题的原因主要有三种：<br>① 上游源数据存在问题；<br>② 数据准备脚本的问题；<br>③ 数据本身的分布就是如此。<br>如果某个字段出现数据倾斜问题，必须首先排查上述第一、二种原因，如果都没有问题或者无法检查（如：单独的数据集），那么就要考虑这个字段对后续的分析建模是否有价值。一般来说，有严重的数据倾斜的字段对目标变量的区分能力很弱，对分析建模的价值不大，应该直接剔除掉。</p>
<h3 id="3-2-如何衡量数据的倾斜程度"><a href="#3-2-如何衡量数据的倾斜程度" class="headerlink" title="3.2 如何衡量数据的倾斜程度"></a>3.2 如何衡量数据的倾斜程度</h3><p>衡量数据的倾斜程度，主要采用频数分析方法，但因数据类别的不同而有所差异：<br>① 针对连续型字段，需要首先采用等宽分箱方式进行离散化，然后计算各分箱的记录数分布；<br>② 针对离散型字段，直接计算各类别的记录数分布。<br>一般来说，如果某个字段90%以上的记录数，主要集中在某个特定类别或者特定区间，那么这个字段就存在严重的数据倾斜问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对于连续型变量进行等宽分箱</span></span><br><span class="line">pd.value_counts(pd.cut(dataset[<span class="string">'col3'</span>],<span class="number">5</span>))  <span class="comment">#分成5箱</span></span><br><span class="line"><span class="comment">#对于离散型变量进行频数统计</span></span><br><span class="line">pd.value_counts(dataset[<span class="string">'col4'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="4-异常值检查"><a href="#4-异常值检查" class="headerlink" title="4. 异常值检查"></a>4. 异常值检查</h2><p>异常值是指数据中出现了处于特定分布、范围或者趋势之外的数据，这些数据一般会被成为异常值、离群点、噪音等。</p>
<h3 id="4-1-异常值产生的原因"><a href="#4-1-异常值产生的原因" class="headerlink" title="4.1 异常值产生的原因"></a>4.1 异常值产生的原因</h3><p>异常值的产生主要有两类原因：<br>① 数据采集、生成或者传递过程中发生的错误；<br>② 业务运营过程出现的一些特殊情况。<br>将第一种原因产生的异常值称为统计上的异常，这是错误带来的数据问题，需要解决；将第二种原因产生的异常值称为业务上的异常，反映了业务运营过程的某种特殊结果，它不是错误，但需要深究，在数据挖掘中的一种典型应用就是异常检测模型，比如信用卡欺诈，网络入侵检测、客户异动行为识别等等。</p>
<h3 id="4-2-异常值的识别方法"><a href="#4-2-异常值的识别方法" class="headerlink" title="4.2 异常值的识别方法"></a>4.2 异常值的识别方法</h3><p>异常值的识别方法主要有以下几种：</p>
<h4 id="（1）极值检查"><a href="#（1）极值检查" class="headerlink" title="（1）极值检查"></a>（1）极值检查</h4><p>主要检查字段的取值是否超出了合理的值域范围。<br>① 方法一：最大值最小值<br>使用最大值、最小值进行判断。比如客户年龄的最大值为199岁，客户账单的最小费用为-20，这些都明显存在异常。<br>② 方法二：3σ原则<br>如果数据服从正态分布，在3σ原则下，异常值被定义为与平均值的偏差超过了3倍标准差的值。这是因为，在正态分布的假设下，具体平均值3倍标准差之外的值出现的概率低于0.003，属于极个别的小概率事件。<br>③ 方法三：箱线图分析<br>箱线图提供了识别异常的标准：异常值被定义为小于下四分位-1.5倍的四分位间距，或者大于上四分位+1.5倍的四分位间距的值。<br>箱线图分析不要求数据服从任何分布，因此对异常值的识别比较客观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#计算相关统计指标</span></span><br><span class="line">statDF = dataset2.describe()  <span class="comment">#获取描述性统计量</span></span><br><span class="line">statDF.loc[<span class="string">'mean+3std'</span>] = statDF.loc[<span class="string">'mean'</span>] + <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值+3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'mean-3std'</span>] = statDF.loc[<span class="string">'mean'</span>] - <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值-3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'75%+1.5dist'</span>] = statDF.loc[<span class="string">'75%'</span>] + <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算上四分位+1.5倍的四分位间距</span></span><br><span class="line">statDF.loc[<span class="string">'25%-1.5dist'</span>] = statDF.loc[<span class="string">'25%'</span>] - <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算下四分位-1.5倍的四分位间距</span></span><br><span class="line"><span class="comment">#获取各字段最大值、最小值</span></span><br><span class="line">statDF.loc[[<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line"><span class="comment">#判断取值是否大于平均值+3倍标准差</span></span><br><span class="line">dataset3 = dataset2 - statDF.loc[<span class="string">'mean+3std'</span>]</span><br><span class="line">dataset3[dataset3&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于平均值-3倍标准差</span></span><br><span class="line">dataset4 = dataset2 - statDF.loc[<span class="string">'mean-3std'</span>]</span><br><span class="line">dataset4[dataset4&lt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否大于上四分位+1.5倍的四分位间距</span></span><br><span class="line">dataset5 = dataset2 - statDF.loc[<span class="string">'75%+1.5dist'</span>]</span><br><span class="line">dataset5[dataset5&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于下四分位-1.5倍的四分位间距</span></span><br><span class="line">dataset6 = dataset2 - statDF.loc[<span class="string">'25%-1.5dist'</span>]</span><br><span class="line">dataset6[dataset6&lt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="（2）记录数分布检查"><a href="#（2）记录数分布检查" class="headerlink" title="（2）记录数分布检查"></a>（2）记录数分布检查</h4><p>主要检查字段的记录数分布是否超出合理的分布范围，包括三个指标：零值记录数、正值记录数、负值记录数。</p>
<h4 id="（3）波动检查"><a href="#（3）波动检查" class="headerlink" title="（3）波动检查"></a>（3）波动检查</h4><p>波动检查主要适用于有监督的数据，用于检查随着自变量的变化，因变量是否发生明显的波动情况。<br>以上异常值的识别方法主要针对连续型的字段，而对于离散型的字段的异常识别主要通过检查类别出现是否出现了合理阈值外的数据，比如苹果终端型号字段，出现了“P20”的取值。</p>
<h3 id="4-3-异常值的处理"><a href="#4-3-异常值的处理" class="headerlink" title="4.3 异常值的处理"></a>4.3 异常值的处理</h3><p>对于统计上的异常值的处理，主要采取两种方式：剔除或者替换。剔除是指直接将被标记为异常值的记录从数据集中删除掉，而替换是指将异常值用一个非异常值进行替换，比如边界值，或者有监督情况下的目标变量表征相似的某个值。<br>对于业务上的异常值的处理，原则就是进行深入探索分析，查找出现这一特殊情况的根本原因。</p>
<h1 id="2-直接影响数据质量的问题"><a href="#2-直接影响数据质量的问题" class="headerlink" title="2.直接影响数据质量的问题"></a>2.直接影响数据质量的问题</h1><p><strong>孤立的数据。</strong> 又称“数据筒仓”，这些独立的数据组要么属于特定的业务单元，要么包含在特定的软件中。隔离数据的问题是，组织的其他部分无法访问它，因为该软件可能与任何其他内容不兼容，或者业务单元严格控制用户权限。虽然这些数据可能提供有用的，甚至是非常有价值的洞察力，因为它不容易被访问，但是业务不能对它形成一个完整的图景，更不用说从中受益了。</p>
<p><strong>过时的数据。</strong> 企业结构庞大而复杂，有多个团队和部门。因此，跨组织收集数据通常是一个缓慢而费力的过程。到收集所有数据时，其中一些-如果不是大多数-在相关性方面已经落后，因此大大降低了其对组织的价值。</p>
<p><strong>复杂的数据。</strong> 数据来自许多不同的来源和不同的形式。数据来自智能手机、笔记本电脑、网站、客户服务交互、销售和营销、数据库等。它可以是结构化的，也可以是非结构化的。理解输入的数据量和数据种类，并使其标准化供每个人使用是一个资源密集型的过程，许多组织没有足够的带宽或专门知识来跟上</p>
<h1 id="3-如何提高数据质量"><a href="#3-如何提高数据质量" class="headerlink" title="3.如何提高数据质量"></a>3.如何提高数据质量</h1><p>和任何有价值的商业活动一样，提高数据的质量和效用是一个多步骤、多方法的过程。以下是如何：</p>
<ol>
<li><p><strong>方法1：</strong> <a href="https://www.deployinc.com/software-development/choosing-a-scripting-language-for-big-data-processing/" target="_blank" rel="noopener"> 大数据脚本 </a> 获取大量数据，并使用脚本语言与其他现有语言进行通信和组合，以清理和处理数据以进行分析。虽然工程师欣赏脚本的灵活性，但它确实需要对需要合成的数据类型和数据存在的特定上下文有一个重要的理解，以便知道要使用哪种脚本语言。判断和执行中的错误会打乱整个过程。</p>
</li>
<li><p><strong>方法2：</strong> 传统的ETL(提取、加载、转换)工具集成了来自不同来源的数据，并将其加载到数据仓库中，然后准备进行分析。但是，通常需要一组技术熟练的内部数据科学家首先手动清除数据，以解决与源和目的地之间存在的模式和格式不兼容的问题。更不方便的是，这些工具通常是批量处理，而不是实时处理。传统的ETL需要基础设施的类型、现场的专业知识以及很少有组织愿意投资的时间承诺。</p>
</li>
<li><p><strong>方法3：</strong> 开放源码工具提供数据质量服务，如解除欺骗、标准化、充实和实时清理，以及快速注册和比其他解决方案更低的成本。然而，大多数开源工具在实现任何真正的好处之前仍然需要一定程度的定制。对于服务的启动和运行，支持可能是有限的，这意味着组织必须再次依靠他们现有的IT团队来使其工作。</p>
</li>
<li><p><strong>方法4：</strong> <a href="https://www.alooma.com/blog/what-is-data-integration" target="_blank" rel="noopener"> 现代数据集成 </a> 通过自动集成、清理和转换数据，然后将数据存储在数据仓库或数据湖中，从而消除了传统ETL工具的手工操作。组织定义数据类型和目的地，并可以根据需要使用更新的客户详细信息、IP地理定位数据或其他信息丰富数据流。转换过程将来自所有源和各种格式的数据标准化，使其可供组织中的任何人使用。而且，由于它实时处理数据，用户可以检查数据流并纠正正在发生的任何错误。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>质量检查</tag>
        <tag>数仓</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop从mysql导入数据到hive</title>
    <url>/2020/07/22/sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。</p>
<pre><code>把MySQL、Oracle等数据库中的数据导入到HDFS、Hive、HBase中。
把HDFS、Hive、HBase中的数据导出到MySQL、Oracle等数据库中。
1.4 为sqoop1, 1.9 为sqoop2 ，sqoop1与sqoop2是不兼容的。</code></pre><a id="more"></a>

<h3 id="实现需要"><a href="#实现需要" class="headerlink" title="实现需要"></a>实现需要</h3><p>数据库:</p>
<ul>
<li>driver</li>
<li>URL、username、password</li>
<li>database、table</li>
</ul>
<p>hadoop:</p>
<ul>
<li>type (hdfp、hive、hbase)</li>
<li>path 存储到哪里？</li>
<li>数据分隔符</li>
<li>mappers 数量，也就是使用多少线程。</li>
</ul>
<h2 id="二、命令"><a href="#二、命令" class="headerlink" title="二、命令"></a>二、命令</h2><h3 id="查看-sqoop-支持的命令"><a href="#查看-sqoop-支持的命令" class="headerlink" title="查看 sqoop 支持的命令"></a>查看 sqoop 支持的命令</h3><p>  sqoop help</p>
<h3 id="显示所有库名"><a href="#显示所有库名" class="headerlink" title="显示所有库名"></a>显示所有库名</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306 \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="显示某个数据库里所有表"><a href="#显示某个数据库里所有表" class="headerlink" title="显示某个数据库里所有表"></a>显示某个数据库里所有表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="MYSQL-导入数据到-HIVE"><a href="#MYSQL-导入数据到-HIVE" class="headerlink" title="MYSQL 导入数据到 HIVE"></a>MYSQL 导入数据到 HIVE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234 \</span></span><br><span class="line"><span class="comment">--table people</span></span><br><span class="line"><span class="comment">--hive-import </span></span><br><span class="line"><span class="comment">--create-hive-table </span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"</span></span><br><span class="line">-m 5</span><br></pre></td></tr></table></figure>

<h3 id="hive-参数"><a href="#hive-参数" class="headerlink" title="hive 参数"></a>hive 参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–hive-import 必须参数，指定导入hive</span><br><span class="line">–hive-database default hive库名</span><br><span class="line">–hive-table people hive表名</span><br><span class="line">–fields-terminated-by hive的分隔符</span><br><span class="line">–hive-overwrite 重写重复字段</span><br><span class="line">–create-hive-table 帮创建好 hive 表，但是表存在会出错。不建议使用这个参数，因为到导入的时候，会与我们的字段类型有出入。</span><br><span class="line">–hive-partition-key “dt” 指定分区表的字段</span><br><span class="line">–hive-partition-value “2018-08-08” 指定分区表的值</span><br></pre></td></tr></table></figure>

<h3 id="导出没有主键的表"><a href="#导出没有主键的表" class="headerlink" title="导出没有主键的表"></a>导出没有主键的表</h3><p>可以使用两种方式：</p>
<ul>
<li>–split-by 指定切分的字段</li>
<li>-m 1 : 设置只使用一个map进行数据迁移</li>
</ul>
<h3 id="过滤条件"><a href="#过滤条件" class="headerlink" title="过滤条件"></a>过滤条件</h3><p>–where “age&gt;18” 匹配条件<br>       –columns “name,age” 选择要导入的指定列<br>       –query ‘select * from people where age&gt;18 and $CONDITIONS’: sql语句查询的结果集<br>      不能 –table 一起使用<br>      需要指定 –target-dir 路径 </p>
<h3 id="当数据库中字符为空时的处理"><a href="#当数据库中字符为空时的处理" class="headerlink" title="当数据库中字符为空时的处理"></a>当数据库中字符为空时的处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–null-non-string ‘0’ 当不是字符串的数据为空的时候，用 0 替换</span><br><span class="line">–null-string ‘string’ 当字符串为空的时候，使用string 字符替换</span><br></pre></td></tr></table></figure>

<h3 id="提高传输速度"><a href="#提高传输速度" class="headerlink" title="提高传输速度"></a>提高传输速度</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–direct 提高数据库到hadoop的传输速度</span><br></pre></td></tr></table></figure>

<p>支持的数据库类型与版本：</p>
<ul>
<li>myslq 5.0 以上</li>
<li>oracle 10.2.0 以上</li>
</ul>
<h3 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h3><p>增量导入对应，首先需要知监控那一列，这列要从哪个值开始增量</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">* check-column id 用来指定一些列</span><br><span class="line">* 这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的,常用的是指定主键id.</span><br><span class="line">* –check-column 可以去指定多个列</span><br></pre></td></tr></table></figure>

<ul>
<li>last-value 10 从哪个值开始增量</li>
<li>incremental 增量的模式<ul>
<li>append id 是获取大于某一列的某个值。</li>
<li>lastmodified “2016-12-15 15:47:30” 获取某个时间后修改的所有数据<ul>
<li>–append 附加模式</li>
<li>–merge-key id 合并模式</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意：增量导入不能与 –delete-target-dir 一起使用，还有必须指定增量的模式</p>
]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Python时间操作</title>
    <url>/2020/07/22/python%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">print</span> time.time()	<span class="comment">#输出的结果是:1279578704.6725271</span></span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>但是这样是一连串的数字不是我们想要的结果，我们可以利用time模块的格式化时间的方法来处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.localtime(time.time())</span><br></pre></td></tr></table></figure>

<p>用time.localtime()方法，作用是格式化时间戳为本地的时间。<br>输出的结果是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.struct_time(tm_year=<span class="number">2010</span>, tm_mon=<span class="number">7</span>, tm_mday=<span class="number">19</span>, tm_hour=<span class="number">22</span>, tm_min=<span class="number">33</span>, tm_sec=<span class="number">39</span>, tm_wday=<span class="number">0</span>, tm_yday=<span class="number">200</span>, tm_isdst=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>现在看起来更有希望格式成我们想要的时间了。<br>time.strftime(‘%Y-%m-%d’,time.localtime(time.time()))</p>
<p>最后用time.strftime()方法，把刚才的一大串信息格式化成我们想要的东西，现在的结果是：<br>2010-07-19</p>
<p>time.strftime里面有很多参数，可以让你能够更随意的输出自己想要的东西：<br>下面是time.strftime的参数：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">strftime(format[, tuple]) -&gt; string</span><br></pre></td></tr></table></figure>

<p>将指定的struct_time(默认为当前时间)，根据指定的格式化字符串输出<br>python中时间日期格式化符号：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%y 两位数的年份表示（<span class="number">00</span><span class="number">-99</span>）</span><br><span class="line">%Y 四位数的年份表示（<span class="number">000</span><span class="number">-9999</span>）</span><br><span class="line">%m 月份（<span class="number">01</span><span class="number">-12</span>）</span><br><span class="line">%d 月内中的一天（<span class="number">0</span><span class="number">-31</span>）</span><br><span class="line">%H <span class="number">24</span>小时制小时数（<span class="number">0</span><span class="number">-23</span>）</span><br><span class="line">%I <span class="number">12</span>小时制小时数（<span class="number">01</span><span class="number">-12</span>） </span><br><span class="line">%M 分钟数（<span class="number">00</span>=<span class="number">59</span>）</span><br><span class="line">%S 秒（<span class="number">00</span><span class="number">-59</span>）</span><br><span class="line">%a 本地简化星期名称</span><br><span class="line">%A 本地完整星期名称</span><br><span class="line">%b 本地简化的月份名称</span><br><span class="line">%B 本地完整的月份名称</span><br><span class="line">%c 本地相应的日期表示和时间表示</span><br><span class="line">%j 年内的一天（<span class="number">001</span><span class="number">-366</span>）</span><br><span class="line">%p 本地A.M.或P.M.的等价符</span><br><span class="line">%U 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期天为星期的开始</span><br><span class="line">%w 星期（<span class="number">0</span><span class="number">-6</span>），星期天为星期的开始</span><br><span class="line">%W 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期一为星期的开始</span><br><span class="line">%x 本地相应的日期表示</span><br><span class="line">%X 本地相应的时间表示</span><br><span class="line">%Z 当前时区的名称</span><br><span class="line">%% %号本身</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive中Parquet格式的使用</title>
    <url>/2020/07/22/Hive%E4%B8%ADParquet%E6%A0%BC%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Hive中Parquet格式的使用"><a href="#Hive中Parquet格式的使用" class="headerlink" title="Hive中Parquet格式的使用"></a>Hive中Parquet格式的使用</h1><p><strong>#Hive建外部External表（外部表external table）：</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`table_name`</span>(</span><br><span class="line">  <span class="string">`column1`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column2`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column3`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`proc_date`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">'hdfs://hdfscluster/...'</span></span><br><span class="line">TBLPROPERTIES ( <span class="string">'orc.compress'</span>=<span class="string">'snappy'</span>);</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#添加分区并加载分区数据：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'...'</span>（不改变源数据存储位置）</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> exsit <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'hdfs://hdfscluster/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'...'</span> <span class="keyword">into</span> <span class="keyword">table</span> table_name <span class="keyword">partition</span>(proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>);（会将源数据切到hive表指定的路径下）</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除分区：alter table table_name drop if exists partition(proc_date='$&#123;hivevar:pdate&#125;');</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>Parquet列式存储格式</title>
    <url>/2020/07/22/parquet%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
    <content><![CDATA[<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目，最新的版本是1.8.0。</p>
<a id="more"></a>

<h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>列式存储和行式存储相比有哪些优势呢？</p>
<ol>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
</ol>
]]></content>
      <categories>
        <category>Parquet</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑主键,业务主键和复合主键</title>
    <url>/2020/07/21/%E9%80%BB%E8%BE%91%E4%B8%BB%E9%94%AE,%E4%B8%9A%E5%8A%A1%E4%B8%BB%E9%94%AE%E5%92%8C%E5%A4%8D%E5%90%88%E4%B8%BB%E9%94%AE/</url>
    <content><![CDATA[<p><strong>1.概念定义</strong></p>
<ul>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms191236(v=SQL.100).aspx" target="_blank" rel="noopener">主键(PRIMARY KEY)</a>：表通常具有包含唯一标识表中每一行的值的一列或一组列。这样的一列或多列称为表的主键 (PK)，用于强制表的实体完整性。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms175464(v=SQL.100).aspx" target="_blank" rel="noopener">外键(FOREIGN KEY)</a>：外键 (FK) 是用于建立和加强两个表数据之间的链接的一列或多列。在外键引用中，当一个表的列被引用作为另一个表的主键值的列时，就在两表之间创建了链接。这个列就成为第二个表的外键。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms190639(v=SQL.100).aspx" target="_blank" rel="noopener">聚集索引</a>：聚集索引基于数据行的键值在表内排序和存储这些数据行。每个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179325(v=SQL.100).aspx" target="_blank" rel="noopener">非聚集索引</a>：非聚集索引包含索引键值和指向表数据存储位置的行定位器。可以对表或索引视图创建多个非聚集索引。通常，设计非聚集索引是为改善经常使用的、没有建立聚集索引的查询的性能。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179413(v=SQL.100).aspx" target="_blank" rel="noopener">自动编号列和标识符列</a>：对于每个表，均可创建一个包含系统生成的序号值的标识符列，该序号值以唯一方式标识表中的每一行。</p>
</li>
<li><p>业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。</p>
</li>
<li><p>逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。</p>
</li>
<li><p>复合主键（联合主键）：通过两个或者多个字段的组合作为主键。</p>
<a id="more"></a>

<p><strong>2.原理分析</strong></p>
<p>​    使用逻辑主键的主要原因是，业务主键一旦改变则系统中关联该主键的部分的修改将会是不可避免的，并且引用越多改动越大。而使用逻辑主键则只需要修改相应的业务主键相关的业务逻辑即可，减少了因为业务主键相关改变对系统的影响范围。业务逻辑的改变是不可避免的，因为“永远不变的是变化”，没有任何一个公司是一成不变的，没有任何一个业务是永远不变的。最典型的例子就是身份证升位和驾驶执照号换用身份证号的业务变更。而且现实中也确实出现了<a href="http://zhidao.baidu.com/question/22152449" target="_blank" rel="noopener">身份证号码重复</a>的情况，这样如果用身份证号码作为主键也带来了难以处理的情况。当然应对改变，可以有很多解决方案，方案之一是做一新系统与时俱进，这对软件公司来说确实是件好事。</p>
<p>​    使用逻辑主键的另外一个原因是，业务主键过大，不利于传输、处理和存储。我认为一般如果业务主键超过8字节就应该考虑使用逻辑主键了，因为int是4字节的，bigint是8字节的，而业务主键一般是字符串，同样是 8 字节的 bigint 和 8 字节的字符串在传输和处理上自然是 bigint 效率更高一些。想象一下 code == “12345678” 和 id == 12345678 的汇编码的不同就知道了。当然逻辑主键不一定是 int 或者 bigint  ，而业务主键也不一定是字符串也可以是 int 或 datetime  等类型，同时传输的也不一定就是主键，这个就要具体分析了，但是原理类似，这里只是讨论通常情况。同时如果其他表需要引用该主键的话，也需要存储该主键，那么这个存储空间的开销也是不一样的。而且这些表的这个引用字段通常就是外键，或者通常也会建索引方便查找，这样也会造成存储空间的开销的不同，这也是需要具体分析的。</p>
<p>​    使用逻辑主键的再一个原因是，使用 int 或者 bigint 作为外键进行联接查询，性能会比以字符串作为外键进行联接查询快。原理和上面的类似，这里不再重复。</p>
<p>​    使用逻辑主键的再一个原因是，存在用户或维护人员误录入数据到业务主键中的问题。例如错把 RMB 录入为 RXB  ，相关的引用都是引用了错误的数据，一旦需要修改则非常麻烦。如果使用逻辑主键则问题很好解决，如果使用业务主键则会影响到其他表的外键数据，当然也可以通过级联更新方式解决，但是不是所有都能级联得了的。</p>
<p>​    使用业务主键的主要原因是，增加逻辑主键就是增加了一个业务无关的字段，而用户通常都是对于业务相关的字段进行查找（比如员工的工号，书本的 ISBN No.  ），这样我们除了为逻辑主键加索引，还必须为这些业务字段加索引，这样数据库的性能就会下降，而且也增加了存储空间的开销。所以对于业务上确实不常改变的基础数据而言，使用业务主键不失是一个比较好的选择。另一方面，对于基础数据而言，一般的增、删、改都比较少，所以这部分的开销也不会太多，而如果这时候对于业务逻辑的改变有担忧的话，也是可以考虑使用逻辑主键的，这就需要具体问题具体分析了。</p>
<p>​    使用业务主键的另外一个原因是，对于用户操作而言，都是通过业务字段进行的，所以在这些情况下，如果使用逻辑主键的话，必须要多做一次映射转换的动作。我认为这种担心是多余的，直接使用业务主键查询就能得到结果，根本不用管逻辑主键，除非业务主键本身就不唯一。另外，如果在设计的时候就考虑使用逻辑主键的话，编码的时候也是会以主键为主进行处理的，在系统内部传输、处理和存储都是相同的主键，不存在转换问题。除非现有系统是使用业务主键，要把现有系统改成使用逻辑主键，这种情况才会存在转换问题。暂时没有想到还有什么场景是存在这样的转换的。</p>
<p>​    使用业务主键的再一个原因是，对于银行系统而言安全性比性能更加重要，这时候就会考虑使用业务主键，既可以作为主键也可以作为冗余数据，避免因为使用逻辑主键带来的关联丢失问题。如果由于某种原因导致主表和子表关联关系丢失的话，银行可是会面临无法挽回的损失的。为了杜绝这种情况的发生，业务主键需要在重要的表中有冗余存在，这种情况最好的处理方式就是直接使用业务主键了。例如身份证号、存折号、卡号等。所以通常银行系统都要求使用业务主键，这个需求并不是出于性能的考虑而是出于安全性的考虑。</p>
<p>​    使用复合主键的主要原因和使用业务主键是相关的，通常业务主键只使用一个字段不能解决问题，那就只能使用多个字段了。例如使用姓名字段不够用了，再加个生日字段。这种使用复合主键方式效率非常低，主要原因和上面对于较大的业务主键的情况类似。另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。</p>
<p>​    使用复合主键的另外一个原因是，对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。</p>
<p>综合来说，网上大多数人是倾向于用逻辑主键的，而对于实体表用复合主键方式的应该没有多少人认同。支持业务主键的人通常有种误解，认为逻辑主键必须对用户来说有意义，其实逻辑主键只是系统内部使用的，对用户来说是无需知道的。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>主键</tag>
      </tags>
  </entry>
  <entry>
    <title>hive两个聚合函数的计算结果拼接成表并做进一步计算</title>
    <url>/2020/07/23/%E4%B8%A4%E4%B8%AA%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C%E8%BF%9B%E8%A1%8C%E6%8B%BC%E6%8E%A5/</url>
    <content><![CDATA[<h4 id="hive两个聚合函数的计算结果拼接成表并做进一步计算"><a href="#hive两个聚合函数的计算结果拼接成表并做进一步计算" class="headerlink" title="hive两个聚合函数的计算结果拼接成表并做进一步计算"></a>hive两个聚合函数的计算结果拼接成表并做进一步计算</h4><p>hive两个聚合函数的计算结果拼接成表让LZ头疼了很久，一度想到用python处理，或者新建两张临时表保存聚合函数的结果然后再取出数据进行计算，或者使用UDF, 但总觉得还有其他方法。经过一番探索，发现WITH AS 可以方便快捷解决此问题。</p>
<a id="more"></a>

<p>WITH AS短语，也叫做子查询部分（subquery factoring），可以让你做很多事情，定义一个SQL片断，该SQL片断会被整个SQL语句所用到。</p>
<p>需求：统计test1表满足某条件的记录数和test2表满足某条件的记录数然后做除法。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> c1</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*） <span class="keyword">as</span> aa <span class="keyword">from</span> test1 ),</span><br><span class="line">c2</span><br><span class="line"><span class="keyword">as</span> (<span class="keyword">select</span> <span class="keyword">count</span>（*）　<span class="keyword">as</span> bb <span class="keyword">from</span> test2)</span><br><span class="line"><span class="keyword">select</span> a.aa/b.bb <span class="keyword">from</span> c1 a, c2 b ;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>函数</tag>
      </tags>
  </entry>
  <entry>
    <title>mac连接公司vpn的坑</title>
    <url>/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/</url>
    <content><![CDATA[<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>1.出现IPSec共享密匙丢失</p>
<p>2.能访问公司内网,无法访问公网</p>
<a id="more"></a>

<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><h3 id="1-出现IPSec共享密匙丢失"><a href="#1-出现IPSec共享密匙丢失" class="headerlink" title="1.出现IPSec共享密匙丢失"></a>1.出现IPSec共享密匙丢失</h3><p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/FEHZDY59BSnXNCP.png" alt="FEHZDY59BSnXNCP"></p>
<h4 id="原因"><a href="#原因" class="headerlink" title="原因:"></a>原因:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;etc&#x2F;ppp&#x2F;options</span><br><span class="line">包含缺省应用于系统中所有 PPP 链路的特征（例如，计算机是否要求对等点对其本身进行验证）的文件。如果不存在此文件，将禁止非超级用户使用 PPP。</span><br></pre></td></tr></table></figure>

<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>知其然，所以知其后然，这时候的解决方法就是在<code>/etc/ppp/</code>目录下建立options`这个配置文件，并且配置ppp链路l2tp不需要ipsec密钥。</p>
<p>下面就是vim命令操作，如果想系统学习相关命令可查看 <a href="https://link.jianshu.com?t=http%3A%2F%2Fwww.cnblogs.com%2Fpeida%2Farchive%2F2012%2F12%2F05%2F2803591.html" target="_blank" rel="noopener">每天一个linux命令目录</a>，这里不打算详细讲解，有兴趣同学可以另行学习。</p>
<p><strong>操作步骤</strong><br> （1）在终端任意路径下输入命令： <code>sudo vim /etc/ppp/options</code><br> 然后输入电脑密码后，显示vim操作界面后按键盘<code>i</code>进入插入模式，输入下面内容：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">plugin</span> <span class="selector-tag">L2TP</span><span class="selector-class">.ppp</span></span><br><span class="line"><span class="selector-tag">l2tpnoipsec</span></span><br></pre></td></tr></table></figure>

<p>（2）然后按<code>esc</code>键退出插入模式，最后输入<code>:wq!</code>，强制保存并退出vim模式。</p>
<h3 id="2-能访问公司内网-无法访问公网"><a href="#2-能访问公司内网-无法访问公网" class="headerlink" title="2.能访问公司内网,无法访问公网"></a>2.能访问公司内网,无法访问公网</h3><p>在vpn的高级里设置:</p>
<p>1.</p>
<p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/v3IlwSyEFjTxR95-20200724191748915.png" alt="v3IlwSyEFjTxR95-20200724191748915"></p>
<p>2.</p>
<p><img src="/2020/07/24/mac%E8%BF%9E%E6%8E%A5%E5%85%AC%E5%8F%B8VPN%E7%9A%84%E5%9D%91/fDRpcNP2a79kMUS.png" alt="fDRpcNP2a79kMUS"></p>
<p>即可</p>
]]></content>
      <categories>
        <category>公司vpn</category>
      </categories>
      <tags>
        <tag>远程访问</tag>
        <tag>公司vpn</tag>
      </tags>
  </entry>
  <entry>
    <title>同比与环比</title>
    <url>/2020/07/24/%E5%90%8C%E6%AF%94%E7%8E%AF%E6%AF%94/</url>
    <content><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>同比：本期与同期做对比。</p>
<p>环比：本期与上期做对比。</p>
<p>简单点说，同比和环比用于表示某一事物在对比时期内发展变化的方向和程度。以历史同期为基期，例如2016年2月份与2015年2月份、2016年上半年与2015年上半年的比较，就是同比。以前一个统计时间段为基期，例如2016年6月份与2016年5月份、2016年二季度与2016年一季度的比较，就是环比。</p>
<p><img src="/2020/07/24/%E5%90%8C%E6%AF%94%E7%8E%AF%E6%AF%94/c5EoSijy31QIDAx.png" alt="c5EoSijy31QIDAx"></p>
<a id="more"></a>

<p>环比一般是用在月、日很少用在年上，主要是对比很短时间内涨幅程度，不过由于行业差异，比如旅游，会受到淡旺季影响。</p>
<p>同比一般用在相邻两年，相同时间段内，查看涨幅程度，一般用在两年相同月份，很少用在两月相同日期</p>
<h2 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h2><h3 id="一、同比增长计算公式："><a href="#一、同比增长计算公式：" class="headerlink" title="一、同比增长计算公式："></a>一、同比增长计算公式：</h3><p>1、同比增长率=（本期数－同期数）÷同期数×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值300万，同比增长率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<p>2、当同期数为负值的情况，公式应当完善如下：</p>
<p>同比增长率=（本期数－同期数）÷ |同期数|×100%</p>
<p>例子：比如说去年3月的产值100万，本年3月的产值50万，同比增长率是多少？</p>
<p>本题中，同比增长率=(50W-(-100W))/|-100W||×100%=150%</p>
<h3 id="二、环比增长计算公式："><a href="#二、环比增长计算公式：" class="headerlink" title="二、环比增长计算公式："></a>二、<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>计算公式：</h3><p><a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率=（本期数-上期数）/上期数×100%。</p>
<p>例子：比如说今年3月的产值100万，2月的产值300万，<a href="https://www.baidu.com/s?wd=环比增长&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">环比增长</a>率是多少？</p>
<p>本题中，同比增长率=（300－100）÷100=200%</p>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>同比与环比的区别</p>
<p>同比、环比与定基比，都可以用百分数或倍数表示。定基比发展速度，也简称总速度，一般是指报告期水平与某一固定时期水平之比，表明这种现象在较长时期内总的发展速度。</p>
<p>同比发展速度，一般指是指本期发展水平与<a href="https://www.baidu.com/s?wd=上年同期&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao" target="_blank" rel="noopener">上年同期</a>发展水平对比，而达到的相对发展速度。环比发展速度，一般是指报告期水平与前一时期水平之比，表明现象逐期的发展速度。</p>
<p>同比和环比，这两者所反映的虽然都是变化速度，但由于采用基期的不同，其反映的内涵是完全不同的；一般来说，环比可以与环比相比较，而不能拿同比与环比相比较；而对于同一个地方，考虑时间纵向上发展趋势的反映，则往往要把同比与环比放在一起进行对照。</p>
]]></content>
      <categories>
        <category>数据</category>
      </categories>
      <tags>
        <tag>同比/环比</tag>
      </tags>
  </entry>
  <entry>
    <title>hive on spark与spark on hive的区别</title>
    <url>/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h3 id="SparkSQL简介"><a href="#SparkSQL简介" class="headerlink" title="SparkSQL简介"></a>SparkSQL简介</h3><p>SparkSQL的前身是Shark，给熟悉RDBMS但又不理解MapReduce的技术人员提供快速上手的工具，hive应运而生，它是当时唯一运行在Hadoop上的SQL-on-hadoop工具。但是MapReduce计算过程中大量的中间磁盘落地过程消耗了大量的I/O，降低的运行效率，为了提高SQL-on-Hadoop的效率，Shark应运而生，但又因为Shark对于Hive的太多依赖（如采用Hive的语法解析器、查询优化器等等),2014年spark团队停止对Shark的开发，将所有资源放SparkSQL项目上</p>
<a id="more"></a>

<h3 id="SparkSQL、Hive-on-Spark的关系"><a href="#SparkSQL、Hive-on-Spark的关系" class="headerlink" title="SparkSQL、Hive on Spark的关系"></a><strong>SparkSQL、Hive on Spark的关系</strong></h3><p><img src="/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/UPVGS1qbLynB639.png" alt="UPVGS1qbLynB639"></p>
<p>由上图可以看出，SparkSQL之所以要从Shark中孵化出来，初衷就是为了剥离Shark对于Hive的太多依赖。SparkSQL作为Spark生态中独立的一员继续发展，不在受限于Hive，只是兼容Hive；而Hive on  Spark是Hive的发展计划，该计划将Spark作为Hive最底层的引擎之一，Hive不在受限于一个引擎（之前只支持map-reduce），可以采用map-reduce、Tez、Spark等计算引擎。</p>
<p>hive on  Spark是有Cloudera发起，有Intel、MapR等公司共同参与的开源项目，其目的就是将Spark作为Hive的一个计算引擎，将Hive的查询作为Spark的任务提交到Spark集群上面进行计算。通过该项目，可以提高Hive查询的性能，同事为已经部署了Hive或者Spark的用户提供了更加灵活地选择，从而进一步提高Hive和Spark的普及率。</p>
<p>hive on Spark和SparkSQL的结构类似，只是SQL引擎不同，但是计算引擎都是spark</p>
<p>sparkSQL通过sqlcontext来进行使用，hive on  spark通过hivecontext来使用。sqlcontext和hivecontext都是来自于同一个包，从这个层面上理解，其实hive on spark和sparkSQL并没有太大差别。</p>
<p>结构上来看，Hive on Spark和SparkSQL都是一个翻译曾，将SQL翻译成分布是可以执行的Spark程序。</p>
<p>SQLContext：spark处理结构化数据的入口，允许创建DataFrame以及sql查询。</p>
<p>HiveContext：Spark sql执行引擎，集成hive数据，读取在classpath的hive-site.xml配置文件配置hive。所以ye</p>
<h3 id="SparkSQL组件和运行架构"><a href="#SparkSQL组件和运行架构" class="headerlink" title="SparkSQL组件和运行架构"></a><strong>SparkSQL组件和运行架构</strong></h3><p>1-SQLContext：Spark SQL提供SQLContext封装Spark中的所有关系型功能。可以用之前的示例中的现有SparkContext创建SQLContext。<br>2-DataFrame：DataFrame是一个分布式的，按照命名列的形式组织的数据集合。DataFrame基于R语言中的data frame概念，与关系型数据库中的数据库表类似。通过调用将DataFrame的内容作为行RDD（RDD of  Rows）返回的rdd方法，可以将DataFrame转换成RDD。可以通过如下数据源创建DataFrame：已有的RDD、结构化数据文件、JSON数据集、Hive表、外部数据库。<br>了私语关系型数据库，SparkSQL中的SQL语句也是由Projection、Data source、Filter但部分组成，分别对应于sql查询过程中的Result、Data  source和Operation；SQL语句是按照Operation-》Data Source -》Result的次序来描述的。如下所示：</p>
<p><img src="/2020/07/23/Hive-on-Spark%E4%B8%8ESparkSql%E7%9A%84%E5%8C%BA%E5%88%AB/XQZNzEvBbMxS6r1.png" alt="XQZNzEvBbMxS6r1"></p>
<p>下面对上图中展示的SparkSQL语句的执行顺序进行详细解释：</p>
<p>1-对读入的SQL语句进行解析（Parse），分辨出SQL语句中哪些词是关键词（如SELECT、FROM、WHERE），哪些是表达式、哪些是Projection、哪些是Data Source等，从而判断SQL语句是否规范；<br>Projection：简单说就是select选择的列的集合，参考：SQL Projection<br>2-将SQL语句和数据库的数据字典（列、表、视图等等）进行绑定（Bind），如果相关的Projection、Data Source等都是存在的话，就表示这个SQL语句是可以执行的；<br>3-一般的数据库会提供几个执行计划，这些计划一般都有运行统计数据，数据库会在这些计划中选择一个最优计划（Optimize）；<br>4-计划执行（Execute），按Operation–&gt;Data Source–&gt;Result的次序来进行的，在执行过程有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，可能直接从数据库的缓冲池中获取返回结果。</p>
<h3 id="SQLContext和HiveContext"><a href="#SQLContext和HiveContext" class="headerlink" title="SQLContext和HiveContext"></a><strong>SQLContext和HiveContext</strong></h3><p>当使用SparkSQL时，根据是否要使用Hive，有两个不同的入口。推荐使用入口HiveContext，HiveContext继承自SQLContext。它可以提供HiveQL以及其他依赖于Hive的功能的支持。更为基础的SQLContext则仅仅支持SparlSQL功能的一个子集，子集中去掉了需要依赖Hive的功能。这种分离主要视为那些可能会因为引入Hive的全部依赖而陷入依赖冲突的用户而设计的。因为使用HiveContext的时候不需要事先部署好Hive。如果要把一个Spark  SQL链接到部署好的Hive上面，必须将hive-site.xml复制到Spark的配置文件目录中（$SPARK_HOME/conf）。即使没有部署好Hive，SparkSQL也可以运行，如果没有部署好Hive，但是还要使用HiveContext的话，那么SparkSQL将会在当前的工作目录中创建出自己的Hive元数据仓库，叫做metastore_db。，如果使用HiveQL中的CREATETABLE语句来创建表，那么这些表将会被放在默认的文件系统中的/user/hive/warehouse目录中，这里默认的文件系统视情况而定，如果配置了hdfs-site.xml那么就会存放在HDFS上面，否则就存放在本地文件系统中。</p>
<p>运行HiveContext的时候hive环境并不是必须，但是需要hive-site.xml配置文件。</p>
<hr>
<h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>hive on spark大体与SparkSQL结构类似，只是SQL引擎不同，但是计算引擎都是spark！</p>
<p>在pyspark中使用Hive on Spark是中怎么样的体验</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化Spark SQL</span></span><br><span class="line"><span class="comment">#导入Spark SQL</span></span><br><span class="line">from pyspark.sql import HiveContext,Row</span><br><span class="line"><span class="comment"># 当不能引入Hive依赖时</span></span><br><span class="line"><span class="comment"># from pyspark.sql import SQLContext,Row</span></span><br><span class="line"><span class="comment"># 注意，上面那一点才是关键的，他两来自于同一个包，你们区别能有多大</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hiveCtx = HiveContext(sc)   <span class="comment">#创建SQL上下文环境</span></span><br><span class="line">input = hiveCtx.jsonFile(inputFile)   <span class="comment">#基本查询示例</span></span><br><span class="line">input.registerTempTable("tweets")   <span class="comment">#注册输入的SchemaRDD（SchemaRDD在Spark 1.3版本后已经改为DataFrame）</span></span><br><span class="line"><span class="comment">#依据retweetCount(转发计数)选出推文</span></span><br><span class="line">topTweets = hiveCtx.sql("<span class="keyword">SELECT</span> <span class="built_in">text</span>,retweetCount <span class="keyword">FROM</span> tweets <span class="keyword">ORDER</span> <span class="keyword">BY</span> retweetCount <span class="keyword">LIMIT</span> <span class="number">10</span><span class="string">")</span></span><br></pre></td></tr></table></figure>

<p>我们可以看到，sqlcontext和hivecontext都是出自于pyspark.sql包，可以从这里理解的话，其实hive on spark和sparksql并没有太大差别</p>
<p>结构上Hive On Spark和SparkSQL都是一个翻译层，把一个SQL翻译成分布式可执行的Spark程序。而且大家的引擎都是spark</p>
<p>SparkSQL和Hive On  Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On  Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。—From <a href="http://blog.csdn.net/yeruby/article/details/51448188" target="_blank" rel="noopener">SparkSQL与Hive on Spark的比较</a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive on spark</tag>
        <tag>spark on hive</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习项目01-鸢尾花分类</title>
    <url>/2020/07/24/ML-flower/</url>
    <content><![CDATA[<p>机器学习项目之鸢尾花分类:</p>
<h3 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述:"></a>任务描述:</h3><p>构建一个模型，根据鸢尾花的花萼和花瓣大小将其分为三种不同的品种。</p>
<p><img src="/2020/07/24/ML-flower/dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c.png" alt="dd74666475b549fcae99ac2aff67488f015cdd76569d4d208909983bcf40fe3c"></p>
<a id="more"></a>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np                </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> colors     </span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm            </span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_path = <span class="string">'/Users/taoxuefeng/Desktop/jupyterlab/jupyterlab_python/MLData/flower_data/iris.data'</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iris_type</span><span class="params">(s)</span>:</span></span><br><span class="line">    it = &#123;<span class="string">b'Iris-setosa'</span>:<span class="number">0</span>, <span class="string">b'Iris-versicolor'</span>:<span class="number">1</span>, <span class="string">b'Iris-virginica'</span>:<span class="number">2</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> it[s]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = np.loadtxt(data_path,                <span class="comment">#数据文件路径</span></span><br><span class="line">                 dtype=float,               <span class="comment">#数据文件类型</span></span><br><span class="line">                 delimiter=<span class="string">','</span>,             <span class="comment">#数据分隔符</span></span><br><span class="line">                 converters=&#123;<span class="number">4</span>:iris_type&#125;)  <span class="comment">#将第5列使用函数iris_type进行转换</span></span><br><span class="line"><span class="comment">#print(data)</span></span><br><span class="line"><span class="comment">#print(data.shape)#(150, 5)</span></span><br><span class="line">x, y = np.split(data,     <span class="comment">#要切分的数组</span></span><br><span class="line">               (<span class="number">4</span>,),      <span class="comment">#沿轴切分的位置，第5列开始往后为y</span></span><br><span class="line">               axis = <span class="number">1</span>)  <span class="comment">#代表纵向分割，按列分割</span></span><br><span class="line">x = x[:, <span class="number">0</span>:<span class="number">2</span>]<span class="comment">#在X中我们取前两列作为特征，为了后面的可视化。x[:,0:2]代表第一维(行)全取，第二维(列)取0~2</span></span><br><span class="line"><span class="comment">#print(x)</span></span><br><span class="line">x_train, x_test, y_train, y_test = model_selection.train_test_split(x,<span class="comment">#索要划分的样本特征集</span></span><br><span class="line">                                                                   y,<span class="comment">#所要划分的样本结果</span></span><br><span class="line">                                                                   random_state = <span class="number">1</span>,<span class="comment">#随机数种子</span></span><br><span class="line">                                                                   test_size = <span class="number">0.3</span>) <span class="comment">#测试样本占比</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**********************SVM分类器构建*************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classifier</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#clf = svm.SVC(C=0.8,kernel='rbf', gamma=50,decision_function_shape='ovr')</span></span><br><span class="line">    clf = svm.SVC(C=<span class="number">0.5</span>,                         <span class="comment">#误差项惩罚系数,默认值是1</span></span><br><span class="line">                  kernel=<span class="string">'linear'</span>,               <span class="comment">#线性核 kenrel="rbf":高斯核</span></span><br><span class="line">                  decision_function_shape=<span class="string">'ovr'</span>) <span class="comment">#决策函数</span></span><br><span class="line">    <span class="keyword">return</span> clf</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.定义模型：SVM模型定义</span></span><br><span class="line">clf = classifier()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#***********************训练模型*****************************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(clf,x_train,y_train)</span>:</span></span><br><span class="line">    clf.fit(x_train,         <span class="comment">#训练集特征向量</span></span><br><span class="line">            y_train.ravel()) <span class="comment">#训练集目标值</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 3.训练SVM模型</span></span><br><span class="line">train(clf,x_train,y_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#**************并判断a b是否相等，计算acc的均值*************</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_accuracy</span><span class="params">(a, b, tip)</span>:</span></span><br><span class="line">    acc = a.ravel() == b.ravel()</span><br><span class="line">    print(<span class="string">'%s Accuracy:%.3f'</span> %(tip, np.mean(acc)))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_accuracy</span><span class="params">(clf,x_train,y_train,x_test,y_test)</span>:</span></span><br><span class="line">    <span class="comment">#分别打印训练集和测试集的准确率  score(x_train,y_train):表示输出x_train,y_train在模型上的准确率</span></span><br><span class="line">    print(<span class="string">'trianing prediction:%.3f'</span> %(clf.score(x_train, y_train)))</span><br><span class="line">    print(<span class="string">'test data prediction:%.3f'</span> %(clf.score(x_test, y_test)))</span><br><span class="line">    <span class="comment">#原始结果与预测结果进行对比   predict()表示对x_train样本进行预测，返回样本类别</span></span><br><span class="line">    show_accuracy(clf.predict(x_train), y_train, <span class="string">'traing data'</span>)</span><br><span class="line">    show_accuracy(clf.predict(x_test), y_test, <span class="string">'testing data'</span>)</span><br><span class="line">    <span class="comment">#计算决策函数的值，表示x到各分割平面的距离</span></span><br><span class="line">    print(<span class="string">'decision_function:\n'</span>, clf.decision_function(x_train))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 4.模型评估</span></span><br><span class="line">print_accuracy(clf,x_train,y_train,x_test,y_test)</span><br></pre></td></tr></table></figure>

<pre><code>trianing prediction:0.819
test data prediction:0.778
traing data Accuracy:0.819
testing data Accuracy:0.778
decision_function:
 [[-0.30200388  1.26702365  2.28292526]
 [ 2.1831931  -0.19913458  1.06956422]
 [ 2.25424706  0.79489006 -0.20587224]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 0.95815482  2.18401419 -0.17375192]
 [ 2.23120771  0.84075865 -0.19144453]
 [ 2.17327158 -0.14884286  0.92795057]
 [-0.28667175  1.11372202  2.28302495]
 [-0.27989264  1.21274017  2.25881762]
 [-0.29313813  1.24442795  2.2732035 ]
 [-0.27008816  1.2272086   2.22682127]
 [-0.25981661  2.21998499  1.20479842]
 [-0.17071168  0.99542159  2.17180911]
 [-0.30018876  1.25829325  2.2829419 ]
 [-0.17539342  2.15368837  1.06772814]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.23988847  2.23286001  1.06656755]
 [-0.26915223  2.23333222  1.21679709]
 [ 2.22927055  0.98556708 -0.22777916]
 [ 2.2530903   0.85932358 -0.2359772 ]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.26803658  0.80468578 -0.24299359]
 [-0.24030826  1.18556963  2.19011259]
 [-0.25881807  1.17240759  2.23535197]
 [-0.27273902  1.20332527  2.24866913]
 [-0.20956348  2.19674141  1.06726512]
 [-0.26556065  1.16490628  2.24871607]
 [-0.22965507  1.17870942  2.17146651]
 [ 2.25807657 -0.22526231  0.80881977]
 [-0.27322701  2.25917947  1.17077691]
 [-0.26638767  1.21631409  2.22685842]
 [-0.26740532  1.20784059  2.23528903]
 [-0.12135744  2.22922779  0.79343961]
 [-0.2365929   1.12219635  2.21706342]
 [-0.21558048  2.22640865  0.92573306]
 [ 2.22344499 -0.19955645  0.88288227]
 [ 2.22671228  0.93600592 -0.21794279]
 [ 2.26578978 -0.24701281  0.82742467]
 [-0.26556065  1.16490628  2.24871607]
 [ 2.26204658  0.89725133 -0.25453765]
 [-0.2518152   2.22343258  1.17120859]
 [-0.27340098  1.23624732  2.22678409]
 [-0.21624631  2.17118121  1.14723861]
 [ 2.22874494 -0.17513313  0.8269183 ]
 [ 2.2211989   0.87213971 -0.19151045]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.22671228  0.93600592 -0.21794279]
 [-0.29609931  1.25285329  2.27596663]
 [-0.25476857  1.20746943  2.20485252]
 [-0.29672783  1.24461331  2.28083131]
 [-0.27578664  1.21663499  2.24864564]
 [-0.28091389  2.25930846  1.21661886]
 [-0.21369288  1.05233452  2.20512234]
 [-0.27669555  1.12529292  2.27023906]
 [-0.16942442  2.17056098  0.99533295]
 [ 2.24933086 -0.25468768  1.0709247 ]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.18638944  1.20994285 -0.24936796]
 [-0.22656825  2.23557826  0.92551338]
 [-0.27989264  1.21274017  2.25881762]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.28390119  1.23920595  2.25400509]
 [ 2.24837463  0.81114157 -0.20592544]
 [ 2.25702986  0.81715893 -0.22763295]
 [-0.22765797  1.07419821  2.21710769]
 [-0.18996302  2.19089984  0.99497945]
 [-0.27357394  1.19278157  2.25408746]
 [ 2.23355717  0.86019975 -0.2060317 ]
 [ 2.25277813 -0.21394322  0.80875361]
 [-0.18611572  1.10670475  2.14746524]
 [ 2.25454797  0.88341904 -0.24307373]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.23794605  0.91585392 -0.22774264]
 [-0.26740532  1.20784059  2.23528903]
 [ 2.0914977   1.20089769 -0.21820392]
 [ 2.25962348  0.84878847 -0.24304703]
 [-0.25213485  1.16423702  2.22696973]
 [ 2.26725005  0.88232062 -0.25923379]
 [-0.14201734  2.14344591  0.99568721]
 [ 2.25731     0.95572321 -0.25455798]
 [-0.22656825  2.23557826  0.92551338]
 [-0.19708433  2.25161696  0.79328185]
 [ 2.23957622  0.81769302 -0.19137855]
 [ 2.21575566  1.0173258  -0.21798639]
 [ 1.02668315  2.21468275 -0.21824732]
 [ 2.27472592  0.77777882 -0.24294008]
 [-0.21624631  2.17118121  1.14723861]
 [-0.24730284  1.20252603  2.19004536]
 [ 2.24156015  0.83211053 -0.20597859]
 [-0.27273902  1.20332527  2.24866913]
 [-0.19455078  2.17814555  1.06749683]
 [-0.28027257  2.2623408   1.20447285]
 [-0.28054312  1.20372124  2.26304729]
 [-0.23391072  2.21566697  1.11400955]
 [ 2.17896853 -0.12686338  0.8824238 ]
 [ 2.19820639  1.04471124 -0.20619077]
 [-0.26313706  2.23602532  1.18984329]
 [-0.25331913  2.21599142  1.18997806]
 [-0.28966527  1.23403227  2.27016072]
 [-0.23157808  2.22314802  1.06680048]
 [-0.26533811  1.22371567  2.21684157]
 [-0.25751543  1.18608093  2.22693265]
 [-0.27562627  2.24825903  1.21670804]
 [-0.27273902  1.20332527  2.24866913]
 [ 2.22671228  0.93600592 -0.21794279]]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(clf, x)</span>:</span></span><br><span class="line">    iris_feature = <span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>, <span class="string">'petal lenght'</span>, <span class="string">'petal width'</span></span><br><span class="line">    <span class="comment"># 开始画图</span></span><br><span class="line">    x1_min, x1_max = x[:, <span class="number">0</span>].min(), x[:, <span class="number">0</span>].max()               <span class="comment">#第0列的范围</span></span><br><span class="line">    x2_min, x2_max = x[:, <span class="number">1</span>].min(), x[:, <span class="number">1</span>].max()               <span class="comment">#第1列的范围</span></span><br><span class="line">    x1, x2 = np.mgrid[x1_min:x1_max:<span class="number">200j</span>, x2_min:x2_max:<span class="number">200j</span>]   <span class="comment">#生成网格采样点</span></span><br><span class="line">    grid_test = np.stack((x1.flat, x2.flat), axis=<span class="number">1</span>)            <span class="comment">#stack():沿着新的轴加入一系列数组</span></span><br><span class="line">    print(<span class="string">'grid_test:\n'</span>, grid_test)</span><br><span class="line">    <span class="comment"># 输出样本到决策面的距离</span></span><br><span class="line">    z = clf.decision_function(grid_test)</span><br><span class="line">    print(<span class="string">'the distance to decision plane:\n'</span>, z)</span><br><span class="line">    </span><br><span class="line">    grid_hat = clf.predict(grid_test)                           <span class="comment"># 预测分类值 得到【0,0.。。。2,2,2】</span></span><br><span class="line">    print(<span class="string">'grid_hat:\n'</span>, grid_hat)  </span><br><span class="line">    grid_hat = grid_hat.reshape(x1.shape)                       <span class="comment"># reshape grid_hat和x1形状一致</span></span><br><span class="line">                                                                <span class="comment">#若3*3矩阵e，则e.shape()为3*3,表示3行3列   </span></span><br><span class="line"> </span><br><span class="line">    cm_light = mpl.colors.ListedColormap([<span class="string">'#A0FFA0'</span>, <span class="string">'#FFA0A0'</span>, <span class="string">'#A0A0FF'</span>])</span><br><span class="line">    cm_dark = mpl.colors.ListedColormap([<span class="string">'g'</span>, <span class="string">'b'</span>, <span class="string">'r'</span>])</span><br><span class="line"> </span><br><span class="line">    plt.pcolormesh(x1, x2, grid_hat, cmap=cm_light)                                   <span class="comment"># pcolormesh(x,y,z,cmap)这里参数代入</span></span><br><span class="line">                                                                                      <span class="comment"># x1，x2，grid_hat，cmap=cm_light绘制的是背景。</span></span><br><span class="line">    plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=np.squeeze(y), edgecolor=<span class="string">'k'</span>, s=<span class="number">50</span>, cmap=cm_dark) <span class="comment"># 样本点</span></span><br><span class="line">    plt.scatter(x_test[:, <span class="number">0</span>], x_test[:, <span class="number">1</span>], s=<span class="number">120</span>, facecolor=<span class="string">'none'</span>, zorder=<span class="number">10</span>)       <span class="comment"># 测试点</span></span><br><span class="line">    plt.xlabel(iris_feature[<span class="number">0</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.ylabel(iris_feature[<span class="number">1</span>], fontsize=<span class="number">20</span>)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.title(<span class="string">'svm in iris data classification'</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 5.模型使用</span></span><br><span class="line">draw(clf,x)</span><br></pre></td></tr></table></figure>

<pre><code>grid_test:
 [[4.3       2.       ]
 [4.3       2.0120603]
 [4.3       2.0241206]
 ...
 [7.9       4.3758794]
 [7.9       4.3879397]
 [7.9       4.4      ]]
the distance to decision plane:
 [[ 2.17689921  1.23467171 -0.25941323]
 [ 2.17943684  1.23363096 -0.25941107]
 [ 2.18189345  1.23256802 -0.25940892]
 ...
 [-0.27958977  0.83621535  2.28683228]
 [-0.27928358  0.8332275   2.28683314]
 [-0.27897389  0.83034313  2.28683399]]
grid_hat:
 [0. 0. 0. ... 2. 2. 2.]


/Users/taoxuefeng/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: MatplotlibDeprecationWarning: shading=&apos;flat&apos; when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading=&apos;auto&apos;, &apos;nearest&apos; or &apos;gouraud&apos;, or set rcParams[&apos;pcolor.shading&apos;].  This will become an error two minor releases later.</code></pre><p><img src="/2020/07/24/ML-flower/tRZPjJlhgFT5YAV.png" alt="tRZPjJlhgFT5YAV"></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>鸢尾花分类</tag>
        <tag>机器学习</tag>
        <tag>Notebook</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce小文件优化问题</title>
    <url>/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>默认情况下，TextInputformat对任务的切片机智是按文件规划切片，不管文件多小，都会是一个单独的切片，都是交给一个maptask，如果有多个小文件，就会产生大量的maptask，处理效率底下。</p>
<a id="more"></a>

<p>解决办法：</p>
<pre><code>1.从源头上解决，将文件合并后再上传到HDFS处理。
2.如果小文件已经在HDFS中，可以先写一个MapReduce程序对小文件合并
3.可以用另一种InputFormat：CombineInputFormat（它可以将多个文件划分到一个切片中），这样就可以交给一个maptask处理。</code></pre><p> 使用默认的InputFormat</p>
<p><img src="/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3l1bmZhbjY=,size_16,color_FFFFFF,t_70.png" alt="watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3l1bmZhbjY=,size_16,color_FFFFFF,t_70"></p>
<p> 使用CombineTextInputFormat</p>
<pre><code>job.setInputFormatClass(CombineTextInputFormat.class);
CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);
CombineTextInputFormat.setMinInputSplitSize(job, 2097152);</code></pre><p><img src="/2020/08/04/MapReduce%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98/2019080810591773.png" alt="2019080810591773"></p>
]]></content>
      <categories>
        <category>MapReduce</category>
      </categories>
      <tags>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase学习笔记-1</title>
    <url>/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一-Hadoop的局限性"><a href="#一-Hadoop的局限性" class="headerlink" title="一.Hadoop的局限性"></a>一.Hadoop的局限性</h2><p>Hbase是一个构建在Hadoop文件系统之上的面向列(列族)的数据库管理系统</p>
<img src="/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/image-20200804194929543.png" alt="image-20200804194929543" style="zoom:50%;">

<a id="more"></a>

<p>Hadoop存在的限制:</p>
<p>Hadoop可以通过HDFS来存储结构化,半结构化甚至非结构化的数据,是传统数据库的补充,是海量数据存储的最佳方法,针对大文件的存储,批量访问和流式访问都做了优化,同时也通过多副本解决容灾问题,使得数据更加安全.</p>
<p>但是Hadoop的缺陷在于它只能执行批处理,并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。</p>
<blockquote>
<p>  注：数据结构分类：</p>
<ul>
<li>结构化数据：即以关系型数据库表形式管理的数据；</li>
<li>半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等；</li>
<li>非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。</li>
</ul>
</blockquote>
<h2 id="二-Hbase简介"><a href="#二-Hbase简介" class="headerlink" title="二. Hbase简介"></a>二. Hbase简介</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p>
<p>HBase 是一种类似于 <code>Google’s Big Table</code> 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性：</p>
<ul>
<li>不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的；</li>
<li>由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储；</li>
<li>支持通过增加机器进行横向扩展；</li>
<li>支持数据分片；</li>
<li>支持 RegionServers 之间的自动故障转移；</li>
<li>易于使用的 Java 客户端 API；</li>
<li>支持 BlockCache 和布隆过滤器；</li>
<li>过滤器支持谓词下推。</li>
</ul>
<h2 id="三-Hbase-Table"><a href="#三-Hbase-Table" class="headerlink" title="三. Hbase Table"></a>三. Hbase Table</h2><p>HBase 是一个面向 <code>列</code> 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 <code>列族</code> 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。</p>
<p>下图为 HBase 中一张表的：</p>
<ul>
<li>RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序；</li>
<li>该表具有两个列族，分别是 personal 和 office;</li>
<li>其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。</li>
</ul>
<img src="/2020/08/04/Hbase%E7%AC%94%E8%AE%B0-1/image-20200807172505289.png" alt="image-20200807172505289" style="zoom:50%;">

<p>Hbase 的表具有以下特点：</p>
<ul>
<li>容量大：一个表可以有数十亿行，上百万列；</li>
<li>面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担；</li>
<li>稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏 ；</li>
<li>数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面；</li>
<li>存储类型：所有数据的底层存储格式都是字节数组 (byte[])。</li>
</ul>
<h2 id="四-Phoenix"><a href="#四-Phoenix" class="headerlink" title="四. Phoenix"></a>四. Phoenix</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p>
<p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>github.com/heibaiying/BigData-Notes</p>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>实时数仓</title>
    <url>/2020/08/11/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="0x00-数仓为什么要实时"><a href="#0x00-数仓为什么要实时" class="headerlink" title="|0x00 数仓为什么要实时"></a>|0x00 数仓为什么要实时</h3><p>去年开始，<font color="red">实时数仓</font> 的概念突然火了。也许是<strong>传统的离线数仓搞了很多年</strong>，技术相对成熟了，因此大家都把注意力放到了<strong>挑战性更高</strong>的实时上来；也许是随着<strong>存量市场竞争</strong>的到来，对于速度的要求越来越快，<strong>T+1已经不能满足数据的获取要求了</strong>，实时的构建需求也就应运而生了。 </p>
<p><font color="red">总之，时效性开始大于分析性</font> 。</p>
<p>文本简单介绍实时数仓的一些基础理论，更系统性的理论，仍然行业需要更大范围的应用和总结。总之，这是一块<strong>有前景</strong>的<strong>新领域</strong>，值得探索。</p>
<a id="more"></a>

<h3 id="0x01-实时数仓的技术要求"><a href="#0x01-实时数仓的技术要求" class="headerlink" title="|0x01 实时数仓的技术要求"></a>|0x01 实时数仓的技术要求</h3><ol>
<li><font color="red">高并发性</font>：未来的实时数据一定不是仅仅给几个运营或管理层人员使用，会更多的面向商户、用户，那么用户增加的情况下一定会带来并发量的提升，因此实时数仓一定要具备<strong>提供高并发数据服务的能力</strong>。</li>
<li><font color="red">查询速度</font>：目前很多实时指标的应用场景在移动端，移动端对于数据的响应要求远高于PC端，对于大多数数据使用场景希望能够毫秒级返回，未来实时标签如果应用到用户推荐上，<strong>对于响应的速度要求更高</strong>。</li>
<li><font color="red">处理速度</font>：要保证大促期间，在流量峰值的情况下有极强的处理能力，并且<strong>具备消费低延迟性甚至0延迟</strong>。</li>
</ol>
<h3 id="0x02-实时数仓的技术基础：流式技术架构"><a href="#0x02-实时数仓的技术基础：流式技术架构" class="headerlink" title="|0x02 实时数仓的技术基础：流式技术架构"></a>|0x02 实时数仓的技术基础：流式技术架构</h3><p>目前<font color="red">流式计算框架相对成熟</font>，以<strong>Storm、Spark Streaming、Flink</strong>为代表的开源组件也被广泛应用。流式数据处理，简单来讲，就是<strong>系统每产生一条数据，都会被立刻采集并发送到流式任务中心进行处理，不需要额外的定时调度来执行任务</strong>。目前在业界比较广泛采用的框架有Twitter的Storm、Apache的Spark Streaming，以及最近几年流行的Flink。它们整体架构大同小异，但在实现细节上有诸多的不同，需要根据业务场景的特征来灵活选择框架。</p>
<p>流式框架具备以下优点：</p>
<ol>
<li><font color="red">时效性高</font>：延迟粒度通常在<strong>秒级</strong>；</li>
<li><font color="red">任务常驻</font>：流式任务一旦启动，就会一直运行，直到人为终止，且数据源是<strong>无界</strong>的；</li>
<li><font color="red">处理性能高</font>：流式计算通常会采用<strong>较好的服务器</strong>来运行任务，因为一旦处理吞吐量赶不上采集吞吐量，就会出现数据计算延迟；</li>
<li><font color="red">逻辑简单</font>：由于流式计算通常是针对单条数据做处理，缺乏数据之间的关联运算能力，所以在支持的<strong>业务逻辑上相对简单</strong>，并且处理结果会与离线存在一定的差异。</li>
</ol>
<h3 id="0x03-实时数仓的两个常见架构"><a href="#0x03-实时数仓的两个常见架构" class="headerlink" title="|0x03 实时数仓的两个常见架构"></a>|0x03 实时数仓的两个常见架构</h3><p><font color="red">Lambda架构</font>：Lambda架构的核心理念是“<font color="red">流批一体化</font>”，因为随着机器性能和数据框架的不断完善，用户其实不关心底层是如何运行的，批处理也好，流式处理也罢，<strong>能按照统一的模型返回结果就可以了</strong>，这就是Lambda架构诞生的原因。现在很多应用，例如Spark和Flink，都支持这种结构，也就是数据进入平台后，可以选择批处理运行，也可以选择流式处理运行，但不管怎样，<strong>一致性都是相同的</strong>。</p>
<p><font color="red">Kappa架构</font>：Lambda架构虽然理念很好，但用多了会有一个问题：<strong>数据复杂性大大增加</strong>。为了解决复杂性的问题，有人提出了用一套架构解决所有问题的设想，而流行的做法就是基于流计算来做。通过<font color="red">加大流计算的“时间窗口”</font>，来实现逻辑意义上的批处理操作。</p>
<h3 id="0x04-实时数仓的查询引擎"><a href="#0x04-实时数仓的查询引擎" class="headerlink" title="|0x04 实时数仓的查询引擎"></a>|0x04 实时数仓的查询引擎</h3><p>实时数仓的查询依赖于<font color="red">交互式查询引擎</font>，常见于OLAP场景，根据存储数据方式的不同可以分为<strong>ROLAP、MOLAP和HOLAP</strong>：</p>
<p><font color="red">ROLAP</font>：在大数据生态圈中，主流的应用于ROLAP场景的交互式计算引擎包括<strong>Impala和Presto</strong>。基于关系数据库OLAP实现（Relational OLAP），它以关系数据库为核心，以<strong>关系型结构</strong>进行多维数据的表示和存储。它将多维结构划分成两类表：一类是<strong>事实表</strong>，迎来存储数据和维度关键字；另一类是<strong>维度表</strong>，即对每个维度至少使用一个表来存放维度层次、成员类别等维度描述信息。ROLAP的最大好处是可以实时地从源数据中获得最新数据更新，以保持数据实时性，缺点在于运算效率比较低，用户等待时间比较长。</p>
<p><font color="red">MOLAP</font>：MOLAP是一种通过<font color="red">预计算Cube方式</font>加速查询的OLAP引擎，它的核心思想是“<font color="red">空间换时间</font>”，典型的代表包括<strong>Druid和Kylin</strong>。基于多维数据组织的OLAP实现（Multidimensional OLAP），它以多维数据组织方式为核心，使用多维数组存储数据。多维数据在存储系统中形成“<strong>数据立方体（Cube）</strong>”结构，此结构是高度优化的，可以最大程度提高查询性能。MOLAP的优势在于借助数据多位预处理显著提高运算效率，主要缺陷在于占用存储空间大和数据更新有一定延迟。</p>
<p><font color="red">HOLAP</font>：基于混合数据组织的OLAP实现（Hybrid OLAP），用户可以根据自己的业务需求，选择哪些模型采用ROLAP、哪些采用MOLAP。一般来说，<strong>将不常用或需要灵活定义的分析使用ROLAP，而常用、常规模型采用MOLAP实现</strong>。</p>
<table>
<thead>
<tr>
<th><strong>名称</strong></th>
<th><strong>描述</strong></th>
<th><strong>细节数据存储位置</strong></th>
<th><strong>聚合后的数据存储位置</strong></th>
</tr>
</thead>
<tbody><tr>
<td>ROLAP(Relational OLAP)</td>
<td>基于关系数据库的OLAP实现</td>
<td>关系型数据库</td>
<td>关系型数据库</td>
</tr>
<tr>
<td>MOLAP(Multidimensional OLAP)</td>
<td>基于多维数据组织的OLAP实现</td>
<td>数据立方体</td>
<td>数据立方体</td>
</tr>
<tr>
<td>HOLAP(Hybrid OLAP)</td>
<td>基于混合数据组织的OLAP实现</td>
<td>关系型数据库</td>
<td>数据立方体</td>
</tr>
</tbody></table>
<h3 id="0x05-实时数仓的分层模型"><a href="#0x05-实时数仓的分层模型" class="headerlink" title="|0x05 实时数仓的分层模型"></a>|0x05 实时数仓的分层模型</h3><p>实时数仓的分层思路<strong>沿用</strong>了离线的思想。</p>
<ul>
<li><font color="red">CDM层（明细数据层)</font>: CDM层根据业务场景的不同，分为<strong>各个主题域</strong>。</li>
<li><font color="red">DWS层（汇总数据层)</font>：DWS层对各个域进行了<strong>适度汇总</strong>。</li>
<li><font color="red">ADS层 （应用数据层)</font>：ADS层并不完全根据需求一对一建设，而是结合不同的需求对这一层进行统一设计，以快速支撑<strong>更多的需求场景</strong>。</li>
</ul>
<h3 id="0x06-实时技术中的幂等机制"><a href="#0x06-实时技术中的幂等机制" class="headerlink" title="|0x06 实时技术中的幂等机制"></a>|0x06 实时技术中的幂等机制</h3><p><font color="red">幂等</font>是一个数学概念，<strong>特点是任意多次执行所产生的影响均与一次执行的影响相同</strong>，例如setTrue()函数就是一个幂等函数,无论多次执行，其结果都是一样的。在很多复杂情况下，例如网络波动、Storm重启等，会出现重复数据，因此<strong>并不是所有操作都是幂等的</strong>。在幂等的概念下，我们需要了解消息传输保障的三种机制：At most once、At least once和Exactly once。</p>
<ul>
<li><font color="red">At most once</font>：消息传输机制是每条消息传输<strong>零次或者一次</strong>，即消息可能会丢失；</li>
<li><font color="red">At least once</font>：意味着每条消息会进行<strong>多次传输尝试</strong>，至少一次成功，即消息传输可能重复但不会丢失；</li>
<li><font color="red">Exactly once</font>：消息传输机制是<strong>每条消息有且只有一次</strong>，即消息传输既不会丢失也不会重复。</li>
</ul>
<h3 id="0x07-实时数仓中的多表关联"><a href="#0x07-实时数仓中的多表关联" class="headerlink" title="|0x07 实时数仓中的多表关联"></a>|0x07 实时数仓中的多表关联</h3><p>在流式数据处理中，数据的计算是以<font color="red">计算增量</font>为基础的，所以各个环节到达的时间和顺序，<strong>既是不确定的，也是无序的</strong>。在这种情况下，如果两表要关联，势必需要将数据在内存中进行存储，当一条数据到达后，需要去另一个表中查找数据，如果能够查到，则关联成功，写入下游；如果无法查到，可以被分到未分配的数据集合中进行等待。在实际处理中，考虑到数据查找的性能，会把数据按照关联主键进行<font color="red">分桶处理</font>，以<strong>降低查找的数据量，提高性能</strong>。</p>
<h3 id="0x08-实时技术中的洪峰挑战"><a href="#0x08-实时技术中的洪峰挑战" class="headerlink" title="|0x08 实时技术中的洪峰挑战"></a>|0x08 实时技术中的洪峰挑战</h3><p>主要解决思路有如下几种：</p>
<ol>
<li><font color="red">独占资源与共享资源合理分配</font>：在一台机器中，<strong>共享资源池可以被多个实时任务抢占</strong>，如果一个任务80%的时间都需要抢资源，可以考虑分配更多的独占资源；</li>
<li><font color="red">合理设置缓存机制</font>：虽然内存的读写性能是最好的，但很多数据依然需要读库更新，可以考虑将<strong>热门数据尽量多的留在内存中</strong>，通过异步的方式来更新缓存；</li>
<li><font color="red">计算合并单元</font>：在流式计算框架中，拓扑结构的层级越深，性能越差，考虑<strong>合并计算单元</strong>，可以有效降低数据的传输、序列化等时间；</li>
<li><font color="red">内存共享</font>：在海量数据的处理中，大部分的对象都是以字符串形式存在的，在<strong>不同线程间合理共享对象</strong>，可以大幅度降低字符拷贝带来的性能消耗；</li>
<li><font color="red">在高吞吐与低延迟间寻求平衡</font>：高吞吐与低延迟天生就是一对矛盾体，将<strong>多个读写库的操作或者ACK操作合并</strong>时，可以有效降低数据吞吐，但也会增加延迟，可以进行业务上的取舍。</li>
</ol>
<h3 id="0xFF-总结"><a href="#0xFF-总结" class="headerlink" title="|0xFF 总结"></a>|0xFF 总结</h3><p>在整个实时数仓的建设中，业界已经有了常用的方案选型。整体架构设计<strong>通过分层设计为OLAP查询分担压力，让出计算空间，复杂的计算统一在实时计算层处理掉，避免给OLAP查询带来过大的压力</strong>。汇总计算交给OLAP数据库进行。因此，在整个架构中实时计算一般是<font color="red">Spark+Flink</font>配合，<font color="red">消息队列Kafka</font>一家独大，在整个大数据领域消息队列的应用中仍然处于垄断地位，Hbase、Redis和MySQL都在特定场景下有一席之地。</p>
<p>目前还<font color="red">没有一个</font>OLAP系统能够满足各种场景的查询需求。其本质原因是，<font color="red">没有一个系统能同时在数据量、性能、和灵活性三个方面做到完美</font>，每个系统在设计时都需要在这三者间做出取舍。</p>
<p><strong>从长远看，Spark和Flink更有可能成为下一代的Hadoop，值得投入大量时间学习</strong>。</p>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
      <tags>
        <tag>数仓</tag>
        <tag>实时</tag>
      </tags>
  </entry>
  <entry>
    <title>学习知乎实时数仓实践及架构演进</title>
    <url>/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>“数据智能” (Data Intelligence) 有一个必须且基础的环节，就是数据仓库的建设，同时，数据仓库也是公司数据发展到一定规模后必然会提供的一种基础服务。从智能商业的角度来讲，数据的结果代表了用户的反馈，获取结果的及时性就显得尤为重要，快速的获取数据反馈能够帮助公司更快的做出决策，更好的进行产品迭代，实时数仓在这一过程中起到了不可替代的作用。</p>
<p>本文主要讲述知乎的实时数仓实践以及架构的演进，这包括以下几个方面</p>
<ol>
<li><p>实时数仓 1.0 版本，主题： ETL 逻辑实时化，技术方案：Spark Streaming。</p>
</li>
<li><p>实时数仓 2.0 版本，主题：数据分层，指标计算实时化，技术方案：Flink Streaming。</p>
</li>
<li><p>实时数仓未来展望：Streaming SQL 平台化，元信息管理系统化，结果验收自动化。</p>
<a id="more"></a>

</li>
</ol>
<h1 id="实时数仓架构"><a href="#实时数仓架构" class="headerlink" title="实时数仓架构"></a>实时数仓架构</h1><h3 id="实时数仓-1-0-版本"><a href="#实时数仓-1-0-版本" class="headerlink" title="实时数仓 1.0 版本"></a>实时数仓 1.0 版本</h3><p>1.0 版本的实时数仓主要是对流量数据做实时 ETL，并不计算实时指标，也未建立起实时数仓体系，实时场景比较单一，对实时数据流的处理主要是为了提升数据平台的服务能力。实时数据的处理向上依赖数据的收集，向下关系到数据的查询和可视化，下图是实时数仓 1.0 版本的整体数据架构图。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-630f5d70d2cc1c85be4ea020f99c0554_1440w.jpg" alt="v2-630f5d70d2cc1c85be4ea020f99c0554_1440w"></p>
<p>第一部分是数据采集，由三端 SDK 采集数据并通过 Log Collector Server 发送到 Kafka。第二部分是数据 ETL，主要完成对原始数据的清洗和加工并分实时和离线导入 Druid。第三部分是数据可视化，由 Druid 负责计算指标并通过 Web Server 配合前端完成数据可视化。</p>
<p>其中第一、三部分的相关内容请分别参考：<a href="https://zhuanlan.zhihu.com/p/40985361" target="_blank" rel="noopener">知乎客户端埋点流程、模型和平台技术</a>，<a href="https://zhuanlan.zhihu.com/p/48046671" target="_blank" rel="noopener">Druid 与知乎数据分析平台</a>，此处我们详细介绍第二部分。由于实时数据流的稳定性不如离线数据流，当实时流出现问题后需要离线数据重刷历史数据，因此实时处理部分我们采用了 lambda 架构。</p>
<p>Lambda 架构有高容错、低延时和可扩展的特点，为了实现这一设计，我们将 ETL 工作分为两部分：Streaming ETL 和 Batch ETL。</p>
<h3 id="Streaming-ETL"><a href="#Streaming-ETL" class="headerlink" title="Streaming ETL"></a>Streaming ETL</h3><p>这一部分我会介绍实时计算框架的选择、数据正确性的保证、以及 Streaming 中一些通用的 ETL 逻辑，最后还会介绍 Spark Streaming 在实时 ETL 中的稳定性实践。</p>
<h3 id="计算框架选择"><a href="#计算框架选择" class="headerlink" title="计算框架选择"></a>计算框架选择</h3><p>在 2016 年年初，业界用的比较多的实时计算框架有 Storm 和 Spark Streaming。Storm 是纯流式框架，Spark Streaming 用 Micro Batch 模拟流式计算，前者比后者更实时，后者比前者吞吐量大且生态系统更完善，考虑到知乎的日志量以及初期对实时性的要求，我们选择了 Spark Streaming 作为实时数据的处理框架。</p>
<h3 id="数据正确性保证"><a href="#数据正确性保证" class="headerlink" title="数据正确性保证"></a>数据正确性保证</h3><p>Spark Streaming 的端到端 Exactly-once 需要下游支持幂等、上游支持流量重放，这里我们在 Spark Streaming 这一层做到了 At-least-once，正常情况下数据不重不少，但在程序重启时可能会重发部分数据，为了实现全局的 Exactly-once，我们在下游做了去重逻辑，关于如何去重后面我会讲到。</p>
<h3 id="通用-ETL-逻辑"><a href="#通用-ETL-逻辑" class="headerlink" title="通用 ETL 逻辑"></a>通用 ETL 逻辑</h3><p>ETL 逻辑和埋点的数据结构息息相关，我们所有的埋点共用同一套 Proto Buffer Schema，大致如下所示。</p>
<figure class="highlight protobuf"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">LogEntry</span> </span>&#123;</span><br><span class="line">  <span class="keyword">optional</span> BaseInfo base = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">optional</span> DetailInfo detail = <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">optional</span> ExtraInfo extra = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>BaseInfo：日志中最基本的信息，包括用户信息、客户端信息、时间信息、网络信息等日志发送时的必要信息。DetailInfo：日志中的视图信息，包括当前视图、上一个视图等用于定位用户所在位置的信息。ExtraInfo：日志中与特定业务相关的额外信息。</p>
<p>针对上述三种信息我们将 ETL 逻辑分为通用和非通用两类，通用逻辑和各个业务相关，主要应用于 Base 和 Detail 信息，非通用逻辑则是由需求方针对某次需求提出，主要应用于 Extra 信息。这里我们列举 3 个通用逻辑进行介绍，这包括：动态配置 Streaming、UTM 参数解析、新老用户识别。</p>
<h3 id="动态配置-Streaming"><a href="#动态配置-Streaming" class="headerlink" title="动态配置 Streaming"></a>动态配置 Streaming</h3><p>由于 Streaming 任务需要 7 * 24 小时运行，但有些业务逻辑，比如：存在一个元数据信息中心，当这个元数据发生变化时，需要将这种变化映射到数据流上方便下游使用数据，这种变化可能需要停止 Streaming 任务以更新业务逻辑，但元数据变化的频率非常高，且在元数据变化后如何及时通知程序的维护者也很难。动态配置 Streaming 为我们提供了一个解决方案，该方案如下图所示。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-af21d0b60e548a1b56317ab42fa5d083_1440w.jpg" alt="img"></p>
<p>我们可以把经常变化的元数据作为 Streaming Broadcast 变量，该变量扮演的角色类似于只读缓存，同时针对该变量可设置 TTL，缓存过期后 Executor 节点会重新向 Driver 请求最新的变量。通过这种机制可以非常自然的将元数据的变化映射到数据流上，无需重启任务也无需通知程序的维护者。</p>
<h3 id="UTM-参数解析"><a href="#UTM-参数解析" class="headerlink" title="UTM 参数解析"></a>UTM 参数解析</h3><p>UTM 的全称是 Urchin Tracking Module，是用于追踪网站流量来源的利器，关于 UTM 背景知识介绍可以参考网上其他内容，这里不再赘述。下图是我们解析 UTM 信息的完整逻辑。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-c988423536bd9ca342ee5aa21688d7e3_1440w.jpg" alt="img"></p>
<p>流量数据通过 UTM 参数解析后，我们可以很容易满足以下需求</p>
<ol>
<li>查看各搜索引擎导流情况以及这些流量来自于哪些热门搜索词。</li>
<li>市场部某次活动带来的流量大小，如：页面浏览数、独立访问用户数等。</li>
<li>从站内分享出去的链接在各分享平台（如：微信、微博）被浏览的情况。</li>
</ol>
<h3 id="新老用户识别"><a href="#新老用户识别" class="headerlink" title="新老用户识别"></a>新老用户识别</h3><p>对于互联网公司而言，增长是一个永恒的话题，实时拿到新增用户量，对于增长运营十分重要。例如：一次投放 n 个渠道，如果能拿到每个渠道的实时新增用户数，就可以快速判断出那些渠道更有价值。我们用下图来表达 Streaming ETL 中是如何识别新老用户的。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-e073c2397c34f614cc26fac279f2863a_1440w.jpg" alt="img"></p>
<p>判断一个用户是不是新用户，最简单的办法就是维护一个历史用户池，对每条日志判断该用户是否存在于用户池中。 由于日志量巨大，为了不影响 Streaming 任务的处理速度，我们设计了两层缓存：Thread Local Cache 和 Redis Cache，同时用 HBase 做持久化存储以保存历史用户。访问速度：本地内存 &gt; 远端内存 &gt; 远端磁盘，对于我们这个任务来说，只有 1% 左右的请求会打到 HBase，日志高峰期 26w/s，完全不会影响任务的实时性。当然本地缓存 LruCache 的容量大小和 Redis 的性能也是影响实时性的两个因素。</p>
<p>Streaming ETL 除了上述几个通用场景外，还有一些其他逻辑，这些逻辑的存在有的是为了满足下游更方便的使用数据的需求，有的是对某些错误埋点的修复，总之 Streaming ETL 在整个实时数仓中处于指标计算的上游，有着不可替代的作用。</p>
<h3 id="Spark-Streaming-稳定性实践"><a href="#Spark-Streaming-稳定性实践" class="headerlink" title="Spark Streaming 稳定性实践"></a>Spark Streaming 稳定性实践</h3><h4 id="在实时数仓-1-0-中的"><a href="#在实时数仓-1-0-中的" class="headerlink" title="在实时数仓 1.0 中的"></a>在实时数仓 1.0 中的</h4><ol>
<li>Spark Streaming 消费 Kafka 数据推荐使用 Direct 模式。我们早期使用的是 High Level 或者叫 Receiver 模式并使用了 checkpoint 功能，这种方式在更新程序逻辑时需要删除 checkpoint 否则新的程序逻辑就无法生效。另外，由于使用了 checkpoint 功能，Streaming 任务会保持和 Hdfs 通信，可能会因为 NameNode 的抖动导致 Streaming 任务抖动。因此，推荐使用 Direct 模式，关于这种模式和 Receiver 模式的详细对比，可以参考官方文档。</li>
<li>保证 Spark Streaming 任务的资源稳定。以 Yarn 为例，运行 Streaming 任务的队列能够分配到的最小资源小于了任务所需要的资源，任务会出现频繁丢失 Executor 的情况，这会导致 Streaming 任务变慢，因为丢失的 Executor 所对应的数据需要重新计算，同时还需要重新分配 Executor。</li>
<li>Spark Streaming 消费 Kafka 时需要做数据流限速。默认情况下 Spark Streaming 以尽可能大的速度读取消息队列，当 Streaming 任务挂了很久之后再次被启动时，由于拉取的数据量过大可能会导致上游的 Kafka 集群 IO 被打爆进而出现 Kafka 集群长时间阻塞。可以使用 Streaming Conf 参数做限速，限定每秒拉取的最大速度。</li>
<li>Spark Streaming 任务失败后需要自动拉起。长时间运行发现，Spark Streaming 并不能 7 * 24h 稳定运行，我们用 Supervisor 管理 Driver 进程，当任务挂掉后 Driver 进程将不复存在，此时 Supervisor 将重新拉起 Streaming 任务。</li>
</ol>
<h3 id="Batch-ETL"><a href="#Batch-ETL" class="headerlink" title="Batch ETL"></a>Batch ETL</h3><p>接下来要介绍的是 Lambda 架构的第二个部分：Batch ETL，此部分我们需要解决数据落地、离线 ETL、数据批量导入 Druid 等问题。针对数据落地我们自研了 map reduce 任务 Batch Loader，针对数据修复我们自研了离线任务 Repair ETL，离线修复逻辑和实时逻辑共用一套 ETL Lib，针对批量导入 ProtoParquet 数据到 Druid，我们扩展了 Druid 的导入插件。</p>
<h3 id="Repair-ETL"><a href="#Repair-ETL" class="headerlink" title="Repair ETL"></a>Repair ETL</h3><p>数据架构图中有两个 Kafka，第一个 Kafka 存放的是原始日志，第二个 Kafka 存放的是实时 ETL 后的日志，我们将两个 Kafka 的数据全部落地，这样做的目的是为了保证数据链路的稳定性。因为实时 ETL 中有大量的业务逻辑，未知需求的逻辑也许会给整个流量数据带来安全隐患，而上游的 Log Collect Server 不存在任何业务逻辑只负责收发日志，相比之下第一个 Kafka 的数据要安全和稳定的多。Repair ETL 并不是经常启用，只有当实时 ETL 丢失数据或者出现逻辑错误时，才会启用该程序用于修复日志。</p>
<h3 id="Batch-Load-2-HDFS"><a href="#Batch-Load-2-HDFS" class="headerlink" title="Batch Load 2 HDFS"></a>Batch Load 2 HDFS</h3><p>前面已经介绍过，我们所有的埋点共用同一套 Proto Buffer Schema，数据传输格式全部为二进制。我们自研了落地 Kafka PB 数据到 Hdfs 的 Map Reduce 任务 BatchLoader，该任务除了落地数据外，还负责对数据去重。在 Streaming ETL 阶段我们做到了 At-least-once，通过此处的BatchLoader 去重我们实现了全局 Exactly-once。BatchLoader 除了支持落地数据、对数据去重外，还支持多目录分区（p_date/p_hour/p_plaform/p_logtype）、数据回放、自依赖管理（早期没有统一的调度器）等。截止到目前，BatchLoader 落地了 40+ 的 Kakfa Topic 数据。</p>
<h3 id="Batch-Load-2-Druid"><a href="#Batch-Load-2-Druid" class="headerlink" title="Batch Load 2 Druid"></a>Batch Load 2 Druid</h3><p>采用 Tranquility 实时导入 Druid，这种方式强制需要一个时间窗口，当上游数据延迟超过窗值后会丢弃窗口之外的数据，这种情况会导致实时报表出现指标错误。为了修复这种错误，我们通过 Druid 发起一个离线 Map Reduce 任务定期重导上一个时间段的数据。通过这里的 Batch 导入和前面的实时导入，实现了实时数仓的 Lambda 架构。</p>
<h3 id="实时数仓-1-0-的不足之处"><a href="#实时数仓-1-0-的不足之处" class="headerlink" title="实时数仓 1.0 的不足之处"></a>实时数仓 1.0 的不足之处</h3><p>到目前为止我们已经介绍完 Lambda 架构实时数仓的几个模块，1.0 版本的实时数仓有以下几个不足</p>
<ol>
<li>所有的流量数据存放在同一个 Kafka Topic 中，如果下游每个业务线都要消费，这会导致全量数据被消费多次，Kafka 出流量太高无法满足该需求。</li>
<li>所有的指标计算全部由 Druid 承担，Druid 同时兼顾实时数据源和离线数据源的查询，随着数据量的暴涨 Druid 稳定性急剧下降，这导致各个业务的核心报表不能稳定产出。</li>
<li>由于每个业务使用同一个流量数据源配置报表，导致查询效率低下，同时无法对业务做数据隔离和成本计算。</li>
</ol>
<h3 id="实时数仓-2-0-版本"><a href="#实时数仓-2-0-版本" class="headerlink" title="实时数仓 2.0 版本"></a>实时数仓 2.0 版本</h3><p>随着数据量的暴涨，Druid 中的流量数据源经常查询超时同时各业务消费实时数据的需求也开始增多，如果继续沿用实时数仓 1.0 架构，需要付出大量的额外成本。于是，在实时数仓 1.0 的基础上，我们建立起了实时数仓 2.0，梳理出了新的架构设计并开始着手建立实时数仓体系，新的架构如下图所示。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-8fa67c34aaa02587dc97db2c95f6b240_1440w.jpg" alt="img"></p>
<h3 id="原始层"><a href="#原始层" class="headerlink" title="原始层"></a>原始层</h3><p>实时数仓 1.0 我们只对流量数据做 ETL 处理，在 2.0 版本中我们加入了对业务库的变更日志 Binlog 的处理，Binlog 日志在原始层为库级别或者 Mysql 实例级别，即：一个库或者实例的变更日志存放在同一个 Kafka Topic 中。同时随着公司业务的发展不断有新 App 产生，在原始层不仅采集「知乎」日志，像知乎极速版以及内部孵化项目的埋点数据也需要采集，不同 App 的埋点数据仍然使用同一套 PB Schema。</p>
<h3 id="明细层"><a href="#明细层" class="headerlink" title="明细层"></a>明细层</h3><p>明细层是我们的 ETL 层，这一层数据是由原始层经过 Streaming ETL 后得到。其中对 Binlog 日志的处理主要是完成库或者实例日志到表日志的拆分，对流量日志主要是做一些通用 ETL 处理，由于我们使用的是同一套 PB 结构，对不同 App 数据处理的逻辑代码可以完全复用，这大大降低了我们的开发成本。</p>
<h3 id="汇总层之明细汇总"><a href="#汇总层之明细汇总" class="headerlink" title="汇总层之明细汇总"></a>汇总层之明细汇总</h3><p>明细汇总层是由明细层通过 ETL 得到，主要以宽表形式存在。业务明细汇总是由业务事实明细表和维度表 Join 得到，流量明细汇总是由流量日志按业务线拆分和流量维度 Join 得到。流量按业务拆分后可以满足各业务实时消费的需求，我们在流量拆分这一块做到了自动化，下图演示了流量数据自动切分的过程。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-6deaffa4c25cf1dbb356931626b6f444_1440w.jpg" alt="img"></p>
<p>Streaming Proxy 是流量分发模块，它消费上游 ETL 后的全量数据并定期读取埋点元信息，通过将流量数据与元信息数据进行「Join」完成按业务进行流量拆分的逻辑，同时也会对切分后的流量按业务做 ETL 处理。 只要埋点元信息中新增一个埋点，那么这个埋点对应的数据就会自动切分到该业务的 Kafka 中，最终业务 Kafka 中的数据是独属于当前业务的且已经被通用 ETL 和业务 ETL 处理过，这大大降低了各个业务使用数据的成本。</p>
<h3 id="汇总层之指标汇总"><a href="#汇总层之指标汇总" class="headerlink" title="汇总层之指标汇总"></a>汇总层之指标汇总</h3><p>指标汇总层是由明细层或者明细汇总层通过聚合计算得到，这一层产出了绝大部分的实时数仓指标，这也是与实时数仓 1.0 最大的区别。知乎是一个生产内容的平台，对业务指标的汇总我们可以从内容角度和用户角度进行汇总，从内容角度我们可以实时统计内容（内容可以是答案、问题、文章、视频、想法）的被点赞数、被关注数、被收藏数等指标，从用户角度我可以实时统计用户的粉丝数、回答数、提问数等指标。对流量指标的汇总我们分为各业务指标汇总和全局指标汇总。对各业务指标汇总，我们可以实时统计首页、搜索、视频、想法等业务的卡片曝光数、卡片点击数、CTR 等，对全局指标汇总我们主要以实时会话为主，实时统计一个会话内的 PV 数、卡片曝光数、点击数、浏览深度、会话时长等指标。</p>
<h3 id="指标汇总层的存储选型"><a href="#指标汇总层的存储选型" class="headerlink" title="指标汇总层的存储选型"></a>指标汇总层的存储选型</h3><p>不同于明细层和明细汇总层，指标汇总层需要将实时计算好的指标存储起来以供应用层使用。我们根据不同的场景选用了 HBase 和 Redis 作为实时指标的存储引擎。Redis 的场景主要是满足带 Update 操作且 OPS 较高的需求，例如：实时统计全站所有内容（问题、答案、文章等）的累计 PV 数，由于浏览内容产生大量的 PV 日志，可能高达几万或者几十万每秒，需要对每一条内容的 PV 进行实时累加，这种场景下选用 Redis 更为合适。HBase 的场景主要是满足高频 Append 操作、低频随机读取且指标列较多的需求，例如：每分钟统计一次所有内容的被点赞数、被关注数、被收藏数等指标，将每分钟聚合后的结果行 Append 到 HBase 并不会带来性能和存储量的问题，但这种情况下 Redis 在存储量上可能会出现瓶颈。</p>
<h3 id="指标计算打通"><a href="#指标计算打通" class="headerlink" title="指标计算打通"></a>指标计算打通</h3><h4 id="指标系统和可视化系统"><a href="#指标系统和可视化系统" class="headerlink" title="指标系统和可视化系统"></a>指标系统和可视化系统</h4><p>指标口径管理依赖指标系统，指标可视化依赖可视化系统，我们通过下图的需求开发过程来讲解如何将三者联系起来。</p>
<p><img src="/2020/08/12/%E5%AD%A6%E4%B9%A0%E7%9F%A5%E4%B9%8E%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%AE%9E%E8%B7%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/v2-6ea544ac5b0473365f142d15f0ea1948_1440w.jpg" alt="img"></p>
<ol>
<li><p>需求方整理好需求文档后向数仓工程师提出需求并约会议评审需求，需求文档中必须包含指标的计算口径和指标对应的维度。</p>
</li>
<li><p>数仓工程师根据需求文档对需求进行评审，评审不通过则返回需求方进一步整理需求并重新提需。</p>
</li>
<li><p>在需求评审通过后，数仓工程师开始排期开发</p>
</li>
<li><ol>
<li>首先在可视化系统中创建一个数据源，这个数据源是后期配置实时报表的数据源，创建数据源也即在 HBase 中创建一张 HBase 表。</li>
<li>针对该数据源创建指标列，创建指标列也即在 HBase 列族中创建列，创建指标列的同时会将该指标信息录入指标管理系统。</li>
<li>针对该数据源绑定维表，这个维表是后期配置多维报表时选用维度值要用的，如果要绑定的维表已经存在，则直接绑定，否则需要导入维表。</li>
<li>一个完整的数据源创建后，数仓工程师才能开发实时应用程序，通过应用程序将多维指标实时写入已创建的数据源中。</li>
</ol>
</li>
<li><p>需求方根据已创建的数据源直接配置实时报表。</p>
</li>
</ol>
<h3 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h3><p>应用层主要是使用汇总层数据以满足业务需求。应用层主要分三块：1.通过直接读取指标汇总数据做实时可视化，满足固化的实时报表需求，这部分由实时大盘服务承担；2.推荐算法等业务直接消费明细汇总数据做实时推荐；3.通过 Tranquility 程序实时摄入明细汇总数据到 Druid，满足实时多维即席分析需求。</p>
<h3 id="实时数仓-2-0-中的技术实现"><a href="#实时数仓-2-0-中的技术实现" class="headerlink" title="实时数仓 2.0 中的技术实现"></a>实时数仓 2.0 中的技术实现</h3><p>相比实时数仓 1.0 以 Spark Streaming 作为主要实现技术，在实时数仓 2.0 中，我们将 Flink 作为指标汇总层的主要计算框架。Flink 相比 Spark Streaming 有更明显的优势，主要体现在：低延迟、Exactly-once 语义支持、Streaming SQL 支持、状态管理、丰富的时间类型和窗口计算、CEP 支持等。</p>
<p>我们在实时数仓 2.0 中主要以 Flink 的 Streaming SQL 作为实现方案。使用 Streaming SQL 有以下优点：易于平台化、开发效率高、维度成本低等。目前 Streaming SQL 使用起来也有一些缺陷：1.语法和 Hive SQL 有一定区别，初使用时需要适应；2.UDF 不如 Hive 丰富，写 UDF 的频率高于 Hive。</p>
<h3 id="实时数仓-2-0-取得的进展"><a href="#实时数仓-2-0-取得的进展" class="headerlink" title="实时数仓 2.0 取得的进展"></a>实时数仓 2.0 取得的进展</h3><ol>
<li>在明细汇总层通过流量切分满足了各个业务实时消费日志的需求。目前完成流量切分的业务达到 14+，由于各业务消费的是切分后的流量，Kafka 出流量下降了一个数量级。</li>
<li>各业务核心实时报表可以稳定产出。由于核心报表的计算直接由数仓负责，可视化系统直接读取实时结果，保证了实时报表的稳定性，目前多个业务拥有实时大盘，实时报表达 40+。</li>
<li>提升了即席查询的稳定性。核心报表的指标计算转移到数仓，Druid 只负责即席查询，多维分析类的需求得到了满足。</li>
<li>成本计算需求得到了解决。由于各业务拥有了独立的数据源且各核心大盘由不同的实时程序负责，可以方便的统计各业务使用的存储资源和计算资源。</li>
</ol>
<h3 id="实时数仓未来展望"><a href="#实时数仓未来展望" class="headerlink" title="实时数仓未来展望"></a>实时数仓未来展望</h3><p>从实时数仓 1.0 到 2.0，不管是数据架构还是技术方案，我们在深度和广度上都有了更多的积累。随着公司业务的快速发展以及新技术的诞生，实时数仓也会不断的迭代优化。短期可预见的我们会从以下方面进一步提升实时数仓的服务能力。</p>
<ol>
<li>Streaming SQL 平台化。目前 Streaming SQL 任务是以代码开发 maven 打包的方式提交任务，开发成本高，后期随着 Streaming SQL 平台的上线，实时数仓的开发方式也会由 Jar 包转变为 SQL 文件。</li>
<li>实时数据元信息管理系统化。对数仓元信息的管理可以大幅度降低使用数据的成本，离线数仓的元信息管理已经基本完善，实时数仓的元信息管理才刚刚开始。</li>
<li>实时数仓结果验收自动化。对实时结果的验收只能借助与离线数据指标对比的方式，以 Hive 和 Kafka 数据源为例，分别执行 Hive SQL 和 Flink SQL，统计结果并对比是否一致实现实时结果验收的自动化。</li>
</ol>
]]></content>
      <categories>
        <category>实时数仓</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>实时计算</tag>
        <tag>实时数仓</tag>
      </tags>
  </entry>
  <entry>
    <title>spark-streaming学习1</title>
    <url>/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一、流处理"><a href="#一、流处理" class="headerlink" title="一、流处理"></a>一、流处理</h2><h3 id="1-1-静态数据处理"><a href="#1-1-静态数据处理" class="headerlink" title="1.1 静态数据处理"></a>1.1 静态数据处理</h3><p>在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171353524.png" alt="image-20200817171353524"></p>
<a id="more"></a>

<h3 id="1-2-流处理"><a href="#1-2-流处理" class="headerlink" title="1.2 流处理"></a>1.2 流处理</h3><p>而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。</p>
<p>大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。</p>
<p>接收和发送数据流并执行应用程序或分析逻辑的系统称为<strong>流处理器</strong>。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171405470.png" alt="image-20200817171405470"></p>
<p>流处理带来了静态数据处理所不具备的众多优点：</p>
<ul>
<li><strong>应用程序立即对数据做出反应</strong>：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期；</li>
<li><strong>流处理可以处理更大的数据量</strong>：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量；</li>
<li><strong>流处理更贴近现实的数据模型</strong>：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求；</li>
<li><strong>流处理分散和分离基础设施</strong>：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。</li>
</ul>
<h2 id="二、Spark-Streaming"><a href="#二、Spark-Streaming" class="headerlink" title="二、Spark Streaming"></a>二、Spark Streaming</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p>Spark Streaming 是 Spark 的一个子模块，用于快速构建可扩展，高吞吐量，高容错的流处理程序。具有以下特点：</p>
<ul>
<li>通过高级 API 构建应用程序，简单易用；</li>
<li>支持多种语言，如 Java，Scala 和 Python；</li>
<li>良好的容错性，Spark Streaming 支持快速从失败中恢复丢失的操作状态；</li>
<li>能够和 Spark 其他模块无缝集成，将流处理与批处理完美结合；</li>
<li>Spark Streaming 可以从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，也支持自定义数据源。</li>
</ul>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171416026.png" alt="image-20200817171416026"></p>
<h3 id="2-2-DStream"><a href="#2-2-DStream" class="headerlink" title="2.2 DStream"></a>2.2 DStream</h3><p>Spark Streaming 提供称为离散流 (DStream) 的高级抽象，用于表示连续的数据流。 DStream 可以从来自 Kafka，Flume 和 Kinesis 等数据源的输入数据流创建，也可以由其他 DStream 转化而来。<strong>在内部，DStream 表示为一系列 RDD</strong>。</p>
<p><img src="/2020/08/17/spark-streaming%E5%AD%A6%E4%B9%A01/image-20200817171426304.png" alt="image-20200817171426304"></p>
<h3 id="2-3-Spark-amp-Storm-amp-Flink"><a href="#2-3-Spark-amp-Storm-amp-Flink" class="headerlink" title="2.3 Spark &amp; Storm &amp; Flink"></a>2.3 Spark &amp; Storm &amp; Flink</h3><p>storm 和 Flink 都是真正意义上的流计算框架，但 Spark Streaming 只是将数据流进行极小粒度的拆分，拆分为多个批处理，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。</p>
]]></content>
      <categories>
        <category>spark streaming</category>
      </categories>
      <tags>
        <tag>流处理</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase学习2-系统架构与数据结构</title>
    <url>/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>一个典型的 Hbase Table 表如下：</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172516439.png" alt="image-20200817172516439"></p>
<a id="more"></a>

<h3 id="1-1-Row-Key-行键"><a href="#1-1-Row-Key-行键" class="headerlink" title="1.1 Row Key (行键)"></a>1.1 Row Key (行键)</h3><p><code>Row Key</code> 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：</p>
<ul>
<li>通过指定的 <code>Row Key</code> 进行访问；</li>
<li>通过 Row Key 的 range 进行访问，即访问指定范围内的行；</li>
<li>进行全表扫描。</li>
</ul>
<p><code>Row Key</code> 可以是任意字符串，存储时数据按照 <code>Row Key</code> 的字典序进行排序。这里需要注意以下两点：</p>
<ul>
<li>因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。</li>
<li>行的一次读写操作时原子性的 (不论一次读写多少列)。</li>
</ul>
<h3 id="1-2-Column-Family（列族）"><a href="#1-2-Column-Family（列族）" class="headerlink" title="1.2 Column Family（列族）"></a>1.2 Column Family（列族）</h3><p>HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族。</p>
<h3 id="1-3-Column-Qualifier-列限定符"><a href="#1-3-Column-Qualifier-列限定符" class="headerlink" title="1.3 Column Qualifier (列限定符)"></a>1.3 Column Qualifier (列限定符)</h3><p>列限定符，你可以理解为是具体的列名，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族，它们的列限定符分别是 <code>history</code> 和 <code>math</code>。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。</p>
<h3 id="1-4-Column-列"><a href="#1-4-Column-列" class="headerlink" title="1.4 Column(列)"></a>1.4 Column(列)</h3><p>HBase 中的列由列族和列限定符组成，它们由 <code>:</code>(冒号) 进行分隔，即一个完整的列名应该表述为 <code>列族名 ：列限定符</code>。</p>
<h3 id="1-5-Cell"><a href="#1-5-Cell" class="headerlink" title="1.5 Cell"></a>1.5 Cell</h3><p><code>Cell</code> 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。</p>
<h3 id="1-6-Timestamp-时间戳"><a href="#1-6-Timestamp-时间戳" class="headerlink" title="1.6 Timestamp(时间戳)"></a>1.6 Timestamp(时间戳)</h3><p>HBase 中通过 <code>row key</code> 和 <code>column</code> 确定的为一个存储单元称为 <code>Cell</code>。每个 <code>Cell</code> 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 <code>Cell</code> 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。</p>
<h2 id="二、存储结构"><a href="#二、存储结构" class="headerlink" title="二、存储结构"></a>二、存储结构</h2><h3 id="2-1-Regions"><a href="#2-1-Regions" class="headerlink" title="2.1 Regions"></a>2.1 Regions</h3><p>HBase Table 中的所有行按照 <code>Row Key</code> 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 <code>Region</code>, 一个 <code>Region</code> 包含了在 start key 和 end key 之间的所有行。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172532668.png" alt="image-20200817172532668"></p>
<p>每个表一开始只有一个 <code>Region</code>，随着数据不断增加，<code>Region</code> 会不断增大，当增大到一个阀值的时候，<code>Region</code> 就会等分为两个新的 <code>Region</code>。当 Table 中的行不断增多，就会有越来越多的 <code>Region</code>。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172542401.png" alt="image-20200817172542401"></p>
<p><code>Region</code> 是 HBase 中<strong>分布式存储和负载均衡的最小单元</strong>。这意味着不同的 <code>Region</code> 可以分布在不同的 <code>Region Server</code> 上。但一个 <code>Region</code> 是不会拆分到多个 Server 上的。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172550839.png" alt="image-20200817172550839"></p>
<h3 id="2-2-Region-Server"><a href="#2-2-Region-Server" class="headerlink" title="2.2 Region Server"></a>2.2 Region Server</h3><p><code>Region Server</code> 运行在 HDFS 的 DataNode 上。它具有以下组件：</p>
<ul>
<li><strong>WAL(Write Ahead Log，预写日志)</strong>：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。</li>
<li><strong>BlockCache</strong>：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 <code>最近最少使用原则</code> 清除多余的数据。</li>
<li><strong>MemStore</strong>：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。</li>
<li><strong>HFile</strong> ：将行数据按照 Key\Values 的形式存储在文件系统上。</li>
</ul>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172601304.png" alt="image-20200817172601304"></p>
<p>Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 <code>Store</code> 实例，每个 <code>Store</code> 会有 0 个或多个 <code>StoreFile</code> 与之对应，每个 <code>StoreFile</code> 则对应一个 <code>HFile</code>，HFile 就是实际存储在 HDFS 上的文件。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172611931.png" alt="image-20200817172611931"></p>
<h2 id="三、Hbase系统架构"><a href="#三、Hbase系统架构" class="headerlink" title="三、Hbase系统架构"></a>三、Hbase系统架构</h2><h3 id="3-1-系统架构"><a href="#3-1-系统架构" class="headerlink" title="3.1 系统架构"></a>3.1 系统架构</h3><p>HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：</p>
<p><strong>Zookeeper</strong></p>
<ol>
<li>保证任何时候，集群中只有一个 Master；</li>
<li>存贮所有 Region 的寻址入口；</li>
<li>实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；</li>
<li>存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。</li>
</ol>
<p><strong>Master</strong></p>
<ol>
<li>为 Region Server 分配 Region ；</li>
<li>负责 Region Server 的负载均衡 ；</li>
<li>发现失效的 Region Server 并重新分配其上的 Region；</li>
<li>GFS 上的垃圾文件回收；</li>
<li>处理 Schema 的更新请求。</li>
</ol>
<p><strong>Region Server</strong></p>
<ol>
<li>Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求；</li>
<li>Region Server 负责切分在运行过程中变得过大的 Region。</li>
</ol>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172621158.png" alt="image-20200817172621158"></p>
<h3 id="3-2-组件间的协作"><a href="#3-2-组件间的协作" class="headerlink" title="3.2 组件间的协作"></a>3.2 组件间的协作</h3><p>HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：</p>
<ul>
<li>每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；</li>
<li>所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；</li>
<li>如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。</li>
</ul>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172629352.png" alt="image-20200817172629352"></p>
<h2 id="四、数据的读写流程简述"><a href="#四、数据的读写流程简述" class="headerlink" title="四、数据的读写流程简述"></a>四、数据的读写流程简述</h2><h3 id="4-1-写入数据的流程"><a href="#4-1-写入数据的流程" class="headerlink" title="4.1 写入数据的流程"></a>4.1 写入数据的流程</h3><ol>
<li>Client 向 Region Server 提交写请求；</li>
<li>Region Server 找到目标 Region；</li>
<li>Region 检查数据是否与 Schema 一致；</li>
<li>如果客户端没有指定版本，则获取当前系统时间作为数据版本；</li>
<li>将更新写入 WAL Log；</li>
<li>将更新写入 Memstore；</li>
<li>判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。</li>
</ol>
<blockquote>
<p>  更为详细写入流程可以参考：<a href="http://hbasefly.com/2016/03/23/hbase_writer/" target="_blank" rel="noopener">HBase － 数据写入流程解析</a></p>
</blockquote>
<h3 id="4-2-读取数据的流程"><a href="#4-2-读取数据的流程" class="headerlink" title="4.2 读取数据的流程"></a>4.2 读取数据的流程</h3><p>以下是客户端首次读写 HBase 上数据的流程：</p>
<ol>
<li>客户端从 Zookeeper 获取 <code>META</code> 表所在的 Region Server；</li>
<li>客户端访问 <code>META</code> 表所在的 Region Server，从 <code>META</code> 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 <code>META</code> 表的位置；</li>
<li>客户端从行键所在的 Region Server 上获取数据。</li>
</ol>
<p>如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 <code>META</code> 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。</p>
<p>注：<code>META</code> 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。</p>
<p><img src="/2020/08/17/Hbase%E5%AD%A6%E4%B9%A0-2-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/image-20200817172639070.png" alt="image-20200817172639070"></p>
]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>How to Install ClickHouse with RPM packages from Altinity&#39;s repo(s)</title>
    <url>/2020/08/19/How-to-Install-ClickHouse-with-RPM-packages-from-Altinity-s-repo-s/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="What-is-this"><a href="#What-is-this" class="headerlink" title="What is this"></a>What is this</h2><p>This is a detailed explanation on how to install ready-to-use ClickHouse RPMs from Altinity’s repos (either <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">general repo</a> or <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">stable repo</a>) located on <a href="https://packagecloud.io/Altinity" target="_blank" rel="noopener">packagecloud.io</a>. This is <strong>not</strong> an instructions on how to build your own hand-made RPMs. However, if you need to build your own RPMs, there is a <a href="https://github.com/Altinity/clickhouse-rpm" target="_blank" rel="noopener">detailed explanation</a> on how to build ClickHouse RPMs from sources with the help of Altinity’s <a href="https://github.com/Altinity/clickhouse-rpm" target="_blank" rel="noopener">RPM builder</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="general-and-stable-repos"><a href="#general-and-stable-repos" class="headerlink" title="general and stable repos"></a><code>general</code> and <code>stable</code> repos</h3><p>Altinity provides two repos:</p>
<ul>
<li><p><code>general</code> <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">repo</a> with general ClickHouse releases.</p>
</li>
<li><p><code>stable</code> <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">repo</a> with Altinity Stable ClickHouse releases.</p>
<a id="more"></a>

</li>
</ul>
<h3 id="Supported-OSes"><a href="#Supported-OSes" class="headerlink" title="Supported OSes"></a>Supported OSes</h3><p>All instructions in this manual were tested on Centos 6.10, CentOS 7.5 and Amazon Linux 2.</p>
<p><strong>IMPORTANT for Amazon Linux users</strong> Amazon Linux is being detected as CentOS 6, while RPMs built for CentOS 7 are the best choice. So we need to explicitly install CentOS 7 RPMs More details further in the doc.</p>
<h3 id="Register-repo"><a href="#Register-repo" class="headerlink" title="Register repo"></a>Register repo</h3><p>In order to install ClickHouse RPM packages from Altinity’s repo, we need to register it (repo) with our <code>yum</code>, making <code>yum</code> aware of additional packages installable from external source.</p>
<p>In general, repositories are listed in <code>/etc/yum.repos.d</code> folder, so we need to add Altinity’s repo description in there.</p>
<p>This can be done either <a href="https://github.com/Altinity/clickhouse-rpm-install#manual-installation" target="_blank" rel="noopener">manually</a> or via <a href="https://github.com/Altinity/clickhouse-rpm-install#script-based-installation" target="_blank" rel="noopener">script</a>, provided by <code>packagecloud.io</code>. In any case, as a result, we’ll have ClickHouse packages available for installation via <code>yum</code>.</p>
<p><strong>IMPORTANT for Amazon Linux users</strong> Amazon Linux is being detected as CentOS 6 by the script, so we need to explicitly instruct it to use CentOS 7 repo.</p>
<ul>
<li>In case of <a href="https://github.com/Altinity/clickhouse-rpm-install#manual-installation" target="_blank" rel="noopener">manual installation</a>, just use <a href="https://github.com/Altinity/clickhouse-rpm-install#el7-repo-file" target="_blank" rel="noopener">EL7 repo file</a>. It is compatible with Amazon Linux</li>
<li>In case of <a href="https://github.com/Altinity/clickhouse-rpm-install#script-based-installation" target="_blank" rel="noopener">script-based installation</a>, script provided by packagecloud should be explicitly instructed to use CentOS 7 repo, instead of CentOS 6 repo, which is being used by default for Amazon Linux. More details further in the doc.</li>
</ul>
<p>Let’s start with script-based installation, since this approach looks like more user-friendly.</p>
<h2 id="Script-based-installation"><a href="#Script-based-installation" class="headerlink" title="Script-based installation"></a>Script-based installation</h2><p>For our convenience,<code>packagecloud.io</code> provides nice and user-friendly way to add repos with their <code>shell script</code>. We’ll need to download and run <strong>packagecloud</strong>‘s <code>shell script</code>, which will do all required steps.</p>
<h3 id="Install-dependencies"><a href="#Install-dependencies" class="headerlink" title="Install dependencies"></a>Install dependencies</h3><p>Installation process requires <code>curl</code> in order to download packages. ClickHouse test package has some dependencies in EPEL, so <code>epel-release</code> has to be installed as well, in case you’d like to install ClickHouse test package. Some installations do not have <code>sudo</code> installed, so we need to ensure it is availbale also.</p>
<p>Ensure <code>sudo</code> is available:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y sudo</span><br></pre></td></tr></table></figure>

<p>Ensure <code>curl</code> is available:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y curl</span><br><span class="line"># in case test package would be installed, add epel-release</span><br><span class="line">sudo yum install -y epel-release</span><br></pre></td></tr></table></figure>

<p>Let’s download and run installation <code>shell script</code>, provided by <code>packagecloud.io</code>. First of all, we need to point what script (from <code>general</code> or <code>stable</code> repo) we’ll be using:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># For &#39;general&#39; repo use this URL:</span><br><span class="line">SCRIPT_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;altinity&#x2F;clickhouse&#x2F;script.rpm.sh&quot;</span><br><span class="line"></span><br><span class="line"># For &#39;stable&#39; repo use this URL:</span><br><span class="line">SCRIPT_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;install&#x2F;repositories&#x2F;altinity&#x2F;clickhouse-altinity-stable&#x2F;script.rpm.sh&quot;</span><br></pre></td></tr></table></figure>

<p>Now we can register Altiniry’s repo in the system by running appropriate script.</p>
<p><strong>for CentOS 6 and 7</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s &quot;$&#123;SCRIPT_URL&#125;&quot; | sudo bash</span><br></pre></td></tr></table></figure>

<p><strong>for Amazon Linux</strong>:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -s &quot;$&#123;SCRIPT_URL&#125;&quot; | sudo os&#x3D;centos dist&#x3D;7 bash</span><br></pre></td></tr></table></figure>

<p>pay attention to <code>os=centos dist=7</code> explicitly specified.</p>
<p>At this point we have <code>yum</code> aware of additional RPM packages available.</p>
<p>We are ready to install ClickHouse.</p>
<h3 id="Install-packages"><a href="#Install-packages" class="headerlink" title="Install packages"></a>Install packages</h3><p>First of all, ensure we have ClickHouse packages available for installation</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*&#39;</span><br></pre></td></tr></table></figure>

<p>ClickHouse packages should be listed as available, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64            19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-common-static.x86_64     19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-compressor.x86_64        1.1.54336-3.el7       Altinity_clickhouse</span><br><span class="line">clickhouse-debuginfo.x86_64         19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-odbc.x86_64              1.0.0.20190611-1      Altinity_clickhouse</span><br><span class="line">clickhouse-server.x86_64            19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-server-common.x86_64     19.13.3.26-1.el7      Altinity_clickhouse</span><br><span class="line">clickhouse-test.x86_64              19.13.3.26-1.el7      Altinity_clickhouse</span><br></pre></td></tr></table></figure>

<p>There are multiple packages available (new versions and old tools as well), some of them are deprecated already, so there is no need to install all available RPMs.</p>
<p>Now let’s install ClickHouse main parts - server and client applications.</p>
<h4 id="Install-latest-ClickHouse-version"><a href="#Install-latest-ClickHouse-version" class="headerlink" title="Install latest ClickHouse version"></a>Install latest ClickHouse version</h4><p>In case we’d like to just install latest ClickHouse (it is so in most cases), we can simply install <code>clickhouse-server</code> and <code>clickhouse-client</code> as following:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<p>However, sometimes we’d like to install specific version of ClickHouse.</p>
<h4 id="Install-specific-ClickHouse-version"><a href="#Install-specific-ClickHouse-version" class="headerlink" title="Install specific ClickHouse version"></a>Install specific ClickHouse version</h4><p>We can either just want to install latest version from specific branch, or we may know what ClickHouse version we’d like to install exactly, or we can look over availbale (older) versions available for installation. These cases are little bit different, let’s take a look on both of them.</p>
<p><strong>Select latest version from specific branch</strong></p>
<p>In case we’d like to install latest version of <code>19.11.X.Y</code> family, we can list available latest <code>19.11.*</code> packages</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*19.11.*&#39;</span><br></pre></td></tr></table></figure>

<p>We’ll see packages of one proposed version (latest) only:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.9.52-1.el7</span><br><span class="line">clickhouse-debuginfo.x86_64      19.11.9.52-1.el7</span><br><span class="line">clickhouse-server.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-server-common.x86_64  19.11.9.52-1.el7</span><br><span class="line">clickhouse-test.x86_64           19.11.9.52-1.el7</span><br></pre></td></tr></table></figure>

<p><strong>Select specific version from specific branch</strong></p>
<p>In case we’d like to see all available versions of <code>19.11.X.Y</code> family, then select preferred version out of availbale for installation:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list &#39;clickhouse*19.11.*&#39; --showduplicates</span><br></pre></td></tr></table></figure>

<p>We’ll see all available package versions from <code>19.11.X.Y</code> family:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Available Packages</span><br><span class="line">clickhouse-client.x86_64         19.11.2.7-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.3.11-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.4.24-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.6.31-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.7.40-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.8.46-1.el7</span><br><span class="line">clickhouse-client.x86_64         19.11.9.52-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.2.7-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.3.11-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.4.24-1.el7</span><br><span class="line">clickhouse-common-static.x86_64  19.11.6.31-1.el7</span><br><span class="line">...</span><br><span class="line">and more</span><br></pre></td></tr></table></figure>

<p><strong>Install specific version</strong></p>
<p>By now, we have picked up specific version (out of available) - let’s install it:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y clickhouse-server-19.11.7.40 clickhouse-client-19.11.7.40</span><br></pre></td></tr></table></figure>

<p>and verify it is listed as installed</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum list installed &#39;clickhouse*&#39;</span><br></pre></td></tr></table></figure>

<p>ClickHouse packages should be listed as installed, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Installed Packages</span><br><span class="line">clickhouse-client.x86_64                 19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-common-static.x86_64          19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-server.x86_64                 19.11.7.40-1.el7           @Altinity_clickhouse</span><br><span class="line">clickhouse-server-common.x86_64          19.11.7.40-1.el7           @Altinity_clickhouse</span><br></pre></td></tr></table></figure>

<p>Ensure ClickHouse server is running</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;etc&#x2F;init.d&#x2F;clickhouse-server restart</span><br></pre></td></tr></table></figure>

<p>And connect to it with <code>clickhouse-client</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clickhouse-client</span><br></pre></td></tr></table></figure>

<p>ClickHouse server should respond, something like this:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ClickHouse client version 19.11.7.40.</span><br><span class="line">Connecting to localhost:9000 as user default.</span><br><span class="line">Connected to ClickHouse server version 19.11.7 revision 54423.</span><br><span class="line"></span><br><span class="line">ip-172-31-37-226.ec2.internal :)</span><br></pre></td></tr></table></figure>

<p>Well, all looks fine and ClickHouse installed from <strong>RPM</strong> packages!</p>
<p>We are all done!</p>
<h2 id="Manual-installation"><a href="#Manual-installation" class="headerlink" title="Manual installation"></a>Manual installation</h2><p>Let’s add any of Altinity’s repos - <a href="https://packagecloud.io/Altinity/clickhouse" target="_blank" rel="noopener">general</a> or <a href="https://packagecloud.io/Altinity/clickhouse-altinity-stable" target="_blank" rel="noopener">stable</a> manually</p>
<h3 id="Install-required-packages"><a href="#Install-required-packages" class="headerlink" title="Install required packages"></a>Install required packages</h3><p>We’ll need the following packages installed beforehands:</p>
<ul>
<li><code>pygpgme</code> - helps handling gpg-signatures</li>
<li><code>yum-utils</code> - contains tools for handling source RPMs</li>
<li><code>coreutils</code> - contains core utils and we’ll need <code>tee</code> command later</li>
<li><code>epel-release</code> - contains ClickHouse test package dependencies</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum install -y pygpgme yum-utils coreutils epel-release</span><br></pre></td></tr></table></figure>

<h3 id="Create-required-files"><a href="#Create-required-files" class="headerlink" title="Create required files"></a>Create required files</h3><p>Now let’s create <code>yum</code>‘s repository configuration file: <code>/etc/yum.repos.d/altinity_clickhouse.repo</code>. Depending on what CentOS version you are running you may need files for EL 6 or 7 version.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># For &#39;general&#39; repo use this URL</span><br><span class="line">BASE_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;altinity&#x2F;clickhouse&quot;</span><br><span class="line"></span><br><span class="line"># For &#39;stable&#39; repo use this URL</span><br><span class="line">BASE_URL&#x3D;&quot;https:&#x2F;&#x2F;packagecloud.io&#x2F;altinity&#x2F;clickhouse-altinity-stable&quot;</span><br></pre></td></tr></table></figure>

<h4 id="EL6-repo-file"><a href="#EL6-repo-file" class="headerlink" title="EL6 repo file"></a>EL6 repo file</h4><p>EL6 (<strong>do NOT use with Amazon Linux</strong>) ready-to-copy+paste command to create <code>yum</code>‘s repo config file.<br>It writes <code>/etc/yum.repos.d/altinity_clickhouse.repo</code> file:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;altinity_clickhouse.repo</span><br><span class="line">[altinity_clickhouse]</span><br><span class="line">name&#x3D;altinity_clickhouse</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;6&#x2F;\$basearch</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line"></span><br><span class="line">[altinity_clickhouse-source]</span><br><span class="line">name&#x3D;altinity_clickhouse-source</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;6&#x2F;SRPMS</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h4 id="EL7-repo-file"><a href="#EL7-repo-file" class="headerlink" title="EL7 repo file"></a>EL7 repo file</h4><p>EL7 <strong>and Amazon Linux</strong> ready-to-copy+paste command to create <code>yum</code>‘s repo config file.<br>It writes <code>/etc/yum.repos.d/altinity_clickhouse.repo</code> file:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF | sudo tee &#x2F;etc&#x2F;yum.repos.d&#x2F;altinity_clickhouse.repo</span><br><span class="line">[altinity_clickhouse]</span><br><span class="line">name&#x3D;altinity_clickhouse</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;7&#x2F;\$basearch</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line"></span><br><span class="line">[altinity_clickhouse-source]</span><br><span class="line">name&#x3D;altinity_clickhouse-source</span><br><span class="line">baseurl&#x3D;$&#123;BASE_URL&#125;&#x2F;el&#x2F;7&#x2F;SRPMS</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgkey&#x3D;$&#123;BASE_URL&#125;&#x2F;gpgkey</span><br><span class="line">sslverify&#x3D;1</span><br><span class="line">sslcacert&#x3D;&#x2F;etc&#x2F;pki&#x2F;tls&#x2F;certs&#x2F;ca-bundle.crt</span><br><span class="line">metadata_expire&#x3D;300</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>

<h3 id="Update-cache"><a href="#Update-cache" class="headerlink" title="Update cache"></a>Update cache</h3><p>After repo files created, let’s update <code>yum</code>‘s cache with packges from newly added repo</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo yum -q makecache -y --disablerepo&#x3D;&#39;*&#39; --enablerepo&#x3D;&#39;altinity*&#39;</span><br></pre></td></tr></table></figure>

<h3 id="Install-packages-manually"><a href="#Install-packages-manually" class="headerlink" title="Install packages manually"></a>Install packages manually</h3><p>Packages can be installed the same way as in section <a href="https://github.com/Altinity/clickhouse-rpm-install#install-packages" target="_blank" rel="noopener">Install packages</a> after script-based installation.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Now we have ClickHouse <strong>RPM</strong> packages available for easy installation.</p>
]]></content>
      <categories>
        <category>ClickHouse</category>
      </categories>
      <tags>
        <tag>ClickHouse</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark结构化流-窗口操作与水印</title>
    <url>/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="事件时间窗口操作"><a href="#事件时间窗口操作" class="headerlink" title="事件时间窗口操作"></a>事件时间窗口操作</h3><p>滑动事件时间窗口上的聚合对于结构化流而言非常简单，并且与分组聚合非常相似。在分组聚合中，在用户指定的分组列中为每个唯一值维护聚合值（例如，计数）。在基于窗口的聚合的情况下，行事件时间所属的每个窗口都会维护聚合值。让我们通过插图来了解这一点。</p>
<p>想象一下我们的<a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#quick-example" target="_blank" rel="noopener">快速示例</a>已被修改，并且流现在包含行以及生成行的时间。而不是运行字数统计，我们希望在10分钟的窗口内对字数进行计数，每5分钟更新一次。也就是说，在10分钟窗口12:00-12：10、12：05-12：15、12：10-12:20等之间接收的单词中的单词计数。请注意，12：00-12:10表示数据12:00之后但12:10之前到达。现在，考虑在12:07收到的单词。此字应增加对应于两个窗口12:00-12:10和12:05-12:15的计数。因此，计数将通过分组键（即单词）和窗口（可以从事件时间计算）来索引。</p>
<p>结果表如下所示。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-window.png" alt="窗口操作"></p>
<a id="more"></a>

<p>由于此窗口化类似于分组，因此在代码中，您可以使用<code>groupBy()</code>和<code>window()</code>操作来表示窗口化聚合。您可以在<a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala" target="_blank" rel="noopener">Scala</a> / <a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java" target="_blank" rel="noopener">Java</a> / <a href="https://github.com/apache/spark/blob/v2.2.0/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py" target="_blank" rel="noopener">Python中</a>看到以下示例的完整代码 。</p>
<ul>
<li>scala</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words = ... <span class="comment">// streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Group the data by window and word and compute the count of each group</span></span><br><span class="line"><span class="keyword">val</span> windowedCounts = words.groupBy(</span><br><span class="line">  window($<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>, <span class="string">"5 minutes"</span>),</span><br><span class="line">  $<span class="string">"word"</span></span><br><span class="line">).count()</span><br></pre></td></tr></table></figure>

<h3 id="处理后期数据和加水印"><a href="#处理后期数据和加水印" class="headerlink" title="处理后期数据和加水印"></a>处理后期数据和加水印</h3><p>现在考虑如果事件之一迟到应用程序会发生什么。例如，例如，应用程序可以在12:11接收在12:04（即事件时间）生成的单词。应用程序应使用12:04而不是12:11来更新窗口的旧计数<code>12:00 - 12:10</code>。这在我们基于窗口的分组中很自然地发生–结构化流可以长时间保持部分聚合的中间状态，以便后期数据可以正确更新旧窗口的聚合，如下所示。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-late-data.png" alt="处理后期数据"></p>
<p>但是，要连续几天运行此查询，系统必须限制其累积的中间内存状态量。这意味着系统需要知道何时可以从内存中状态删除旧聚合，因为应用程序将不再接收该聚合的最新数据。为此，我们在Spark 2.1中引入了 <strong>水印功能</strong>，该功能使引擎自动跟踪数据中的当前事件时间，并尝试相应地清除旧状态。您可以通过指定事件时间列和有关事件时间期望数据延迟的阈值来定义查询的水印。对于在时间开始的特定窗口<code>T</code>，引擎将维持状态并允许以后的数据更新状态，直到<code>(max event time seen by the engine - late threshold &gt; T)</code>。换句话说，阈值内的较晚数据将被汇总，但阈值后的数据将被丢弃。让我们通过一个例子来理解这一点。我们可以使用<code>withWatermark()</code>以下示例在上一个示例中轻松定义水印。</p>
<ul>
<li>scala</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> words = ... <span class="comment">// streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Group the data by window and word and compute the count of each group</span></span><br><span class="line"><span class="keyword">val</span> windowedCounts = words</span><br><span class="line">    .withWatermark(<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>)</span><br><span class="line">    .groupBy(</span><br><span class="line">        window($<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>, <span class="string">"5 minutes"</span>),</span><br><span class="line">        $<span class="string">"word"</span>)</span><br><span class="line">    .count()</span><br></pre></td></tr></table></figure>

<p>在此示例中，我们将在“时间戳”列的值上定义查询的水印，还将“ 10分钟”定义为允许数据晚到的阈值。如果此查询在“更新输出”模式下运行（稍后在“ <a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes" target="_blank" rel="noopener">输出模式”</a>部分中讨论），则引擎将在“结果表”中保持窗口的更新计数，直到该窗口早于水印为止，该时间滞后于“列”中的当前事件时间。时间戳”的时间缩短10分钟。这是一个例子。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-watermark-update-mode.png" alt="更新模式中的水印"></p>
<p>如图所示，引擎跟踪的最大事件时间是 <em>蓝色虚线</em>，<code>(max event time - &#39;10 mins&#39;)</code> 在每次触发的开始时设置的水印是红线。例如，当引擎观察到数据时 <code>(12:14, dog)</code>，它将为下一个触发器为<code>12:04</code>。此水印可让引擎再保持10分钟的中间状态，以便对较晚的数据进行计数。例如，数据<code>(12:09, cat)</code>不正确且延迟，并且落在windows <code>12:05 - 12:15</code>和中<code>12:10 - 12:20</code>。由于它仍在<code>12:04</code>触发器中的水印之前，因此引擎仍将中间计数保持为状态并正确更新相关窗口的计数。但是，当水印更新为<code>12:11</code>，<code>(12:00 - 12:10)</code>清除了窗口的中间状态，并且所有后续数据（例如<code>(12:04, donkey)</code>）都被认为“为时已晚”，因此被忽略。请注意，在每次触发后，更新的计数（即紫色行）都会写入更新，作为更新输出指示的触发输出。</p>
<p>某些接收器（例如文件）可能不支持更新模式所需的细粒度更新。为了与他们合作，我们还支持追加模式，其中仅将<em>最终计数</em>写入接收器。如下所示。</p>
<p>请注意，<code>withWatermark</code>在非流数据集上使用是no-op。由于水印不应以任何方式影响任何批量查询，因此我们将直接忽略它。</p>
<p><img src="/2020/08/20/Spark%E7%BB%93%E6%9E%84%E5%8C%96%E6%B5%81-%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%B0%B4%E5%8D%B0/structured-streaming-watermark-append-mode.png" alt="追加模式中的水印"></p>
<p>与之前的更新模式类似，引擎为每个窗口维护中间计数。但是，部分计数不会更新到结果表，也不会写入接收器。引擎等待“ 10分钟”来计算延迟日期，然后丢弃窗口的中间状态&lt;水印，并将最终计数附加到结果表/接收器。例如，<code>12:00 - 12:10</code>仅在将水印更新为之后，窗口的最终计数才添加到结果表中<code>12:11</code>。</p>
<p><strong>用于加水印以清除聚合状态</strong> 的条件重要的是要注意，对于加水印以清除聚合查询中的状态必须满足以下条件<em>（从Spark 2.1.1开始，将来可能会更改）</em>。</p>
<ul>
<li><strong>输出模式必须为追加或更新。</strong>完整模式要求保留所有聚合数据，因此不能使用水印删除中间状态。有关 每种输出模式的语义的详细说明，请参见“ <a href="https://spark.apache.org/docs/2.2.0/structured-streaming-programming-guide.html#output-modes" target="_blank" rel="noopener">输出模式”</a>部分。</li>
<li>聚合必须在“事件时间”列上或<code>window</code>“事件时间”列上有一个。</li>
<li><code>withWatermark</code>必须在与汇总中使用的时间戳列相同的列上调用。例如， <code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()</code>在附加输出模式下无效，因为水印是在与聚合列不同的列上定义的。</li>
<li><code>withWatermark</code>必须在使用水印详细信息的聚合之前调用。例如，<code>df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;)</code>在追加输出模式下无效。</li>
</ul>
]]></content>
      <categories>
        <category>spark-structured-streaming</category>
      </categories>
      <tags>
        <tag>流处理</tag>
        <tag>spark</tag>
        <tag>窗口操作</tag>
        <tag>水印操作</tag>
      </tags>
  </entry>
</search>
