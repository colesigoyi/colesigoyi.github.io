<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hive2的启动</title>
    <url>/2020/06/26/hive2%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<ol>
<li>启动hive的服务<br>hive ‐‐service hiveserver2 &amp;</li>
<li>使用beeline，去连接thrift的服务<br>beeline -u jdbc:hive2://CentOS-01:10000</li>
</ol>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>服务</tag>
        <tag>beeline</tag>
      </tags>
  </entry>
  <entry>
    <title>数据质量检查</title>
    <url>/2020/06/26/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E6%A3%80%E6%9F%A5/</url>
    <content><![CDATA[<h2 id="数据质量检查"><a href="#数据质量检查" class="headerlink" title="数据质量检查"></a>数据质量检查</h2><p>是在完成宽表数据开发后进行的，主要包括四个方面：</p>
<ol>
<li><p>重复值检查</p>
</li>
<li><p>缺失值检查</p>
</li>
<li><p>数据倾斜问题</p>
</li>
<li><p>异常值检查</p>
<a id="more"></a>

</li>
</ol>
<h2 id="1-重复值检查"><a href="#1-重复值检查" class="headerlink" title="1. 重复值检查"></a>1. 重复值检查</h2><h3 id="1-1-什么是重复值"><a href="#1-1-什么是重复值" class="headerlink" title="1.1 什么是重复值"></a>1.1 什么是重复值</h3><p>重复值的检查首先要明确一点，即重复值的定义。对于一份二维表形式的数据集来说，什么是重复值？主要有两个层次：<br>① 关键字段出现重复记录，比如主索引字段出现重复；<br>② 所有字段出现重复记录。<br>第一个层次是否是重复，必须从这份数据的业务含义进行确定。比如一张表，从业务上讲，一个用户应该只会有一条记录，那么如果某个用户出现了超过一条的记录，那么这就是重复值。第二个层次，就一定是重复值了。</p>
<h3 id="1-2-重复值产生的原因"><a href="#1-2-重复值产生的原因" class="headerlink" title="1.2 重复值产生的原因"></a>1.2 重复值产生的原因</h3><p>重复值的产生主要有两个原因，一是上游源数据造成的，二是数据准备脚本中的数据关联造成的。从数据准备角度来看，首先检查数据准备的脚本，判断使用的源表是否有重复记录，同时检查关联语句的正确性和严谨性，比如关联条件是否合理、是否有限定数据周期等等。<br>比如：检查源表数据是否重复的SQL：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> MON_ID,<span class="keyword">COUNT</span>(*),<span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> USER_ID)</span><br><span class="line"><span class="keyword">FROM</span> TABLE_NAME</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> MON_ID;</span><br></pre></td></tr></table></figure>

<p>如果是上游源数据出现重复，那么应该及时反映给上游进行修正；如果是脚本关联造成的，修改脚本，重新生成数据即可。<br>还有一份情况，这份数据集是一份单独的数据集，并不是在数据仓库中开发得到的数据，既没有上游源数据，也不存在生成数据的脚本，比如公开数据集，那么如何处理其中的重复值？一般的处理方式就是直接删除重复值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">dataset = pd.read_excel(<span class="string">"/labcenter/python/dataset.xlsx"</span>)</span><br><span class="line"><span class="comment">#判断重复数据</span></span><br><span class="line">dataset.duplicated()    <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.duplicated([<span class="string">'col2'</span>])    <span class="comment">#col2字段重复是重复数据</span></span><br><span class="line"><span class="comment">#删除重复数据</span></span><br><span class="line">dataset.drop_duplicates()     <span class="comment">#全部字段重复是重复数据</span></span><br><span class="line">dataset.drop_duplicates([<span class="string">'col2'</span>])   <span class="comment">#col2字段重复是重复数据</span></span><br></pre></td></tr></table></figure>

<h2 id="2-缺失值检查"><a href="#2-缺失值检查" class="headerlink" title="2. 缺失值检查"></a>2. 缺失值检查</h2><p>缺失值主要是指数据集中部分记录存在部分字段的信息缺失。</p>
<h3 id="2-1-缺失值出现的原因"><a href="#2-1-缺失值出现的原因" class="headerlink" title="2.1 缺失值出现的原因"></a>2.1 缺失值出现的原因</h3><p>出现趋势值主要有三种原因：<br>① 上游源系统因为技术或者成本原因无法完全获取到这一信息，比如对用户手机APP上网记录的解析；<br>② 从业务上讲，这一信息本来就不存在，比如一个学生的收入，一个未婚者的配偶姓名；<br>③ 数据准备脚本开发中的错误造成的。<br>第一种原因，短期内无法解决；第二种原因，数据的缺失并不是错误，无法避免；第三种原因，则只需通过查证修改脚本即可。<br>缺失值的存在既代表了某一部分信息的丢失，也影响了挖掘分析结论的可靠性与稳定性，因此，必须对缺失值进行处理。<br>如果缺失值记录数超过了全部记录数的50%，则应该从数据集中直接剔除掉该字段，尝试从业务上寻找替代字段；<br>如果缺失值记录数没有超过50%，则应该首先看这个字段在业务上是否有替代字段，如果有，则直接剔除掉该字段，如果没有，则必须对其进行处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#查看哪些字段有缺失值   </span></span><br><span class="line">dataset.isnull().any()    <span class="comment">#获取含有NaN的字段</span></span><br><span class="line"><span class="comment">#统计各字段的缺失值个数</span></span><br><span class="line">dataset.isnull().apply(pd.value_counts)</span><br><span class="line"><span class="comment">#删除含有缺失值的字段</span></span><br><span class="line">nan_col = dataset.isnull().any()</span><br><span class="line">dataset.drop(nan_col[nan_col].index,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-缺失值的处理"><a href="#2-2-缺失值的处理" class="headerlink" title="2.2 缺失值的处理"></a>2.2 缺失值的处理</h3><p>缺失值的处理主要有两种方式：过滤和填充。</p>
<h4 id="（1）缺失值的过滤"><a href="#（1）缺失值的过滤" class="headerlink" title="（1）缺失值的过滤"></a>（1）缺失值的过滤</h4><p>直接删除含有缺失值的记录，总体上会影响样本个数，如果删除样本过多或者数据集本来就是小数据集时，这种方式并不建议采用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#删除含有缺失值的记录</span></span><br><span class="line">dataset.dropna()</span><br></pre></td></tr></table></figure>

<h4 id="2）缺失值的填充"><a href="#2）缺失值的填充" class="headerlink" title="2）缺失值的填充"></a>2）缺失值的填充</h4><p>缺失值的填充主要三种方法：<br>① 方法一：使用特定值填充<br>使用缺失值字段的平均值、中位数、众数等统计量填充。<br>优点：简单、快速<br>缺点：容易产生数据倾斜<br>② 方法二：使用算法预测填充<br>将缺失值字段作为因变量，将没有缺失值字段作为自变量，使用决策树、随机森林、KNN、回归等预测算法进行缺失值的预测，用预测结果进行填充。<br>优点：相对精确<br>缺点：效率低，如果缺失值字段与其他字段相关性不大，预测效果差<br>③ 方法三：将缺失值单独作为一个分组，指定值进行填充<br>从业务上选择一个单独的值进行填充，使缺失值区别于其他值而作为一个分组，从而不影响算法计算。<br>优点：简单，实用<br>缺点：效率低</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用Pandas进行特定值填充</span></span><br><span class="line">dataset.fillna(<span class="number">0</span>)   <span class="comment">#不同字段的缺失值都用0填充</span></span><br><span class="line">dataset.fillna(&#123;<span class="string">'col2'</span>:<span class="number">20</span>,<span class="string">'col5'</span>:<span class="number">0</span>&#125;)    <span class="comment">#不同字段使用不同的填充值</span></span><br><span class="line">dataset.fillna(dataset.mean())   <span class="comment">#分别使用各字段的平均值填充</span></span><br><span class="line">dataset.fillna(dataset.median())     <span class="comment">#分别使用个字段的中位数填充</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#使用sklearn中的预处理方法进行缺失值填充(只适用于连续型字段)</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">dataset2 = dataset.drop([<span class="string">'col4'</span>],axis=<span class="number">1</span>)</span><br><span class="line">colsets = dataset2.columns</span><br><span class="line">nan_rule1 = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)    <span class="comment">#创建填充规则(平均值填充)</span></span><br><span class="line">pd.DataFrame(nan_rule1.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule2 = Imputer(missing_values=<span class="string">'median'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>) <span class="comment">#创建填充规则(中位数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule2.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br><span class="line">nan_rule3 = Imputer(missing_values=<span class="string">'most_frequent'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)  <span class="comment">#创建填充规则(众数填充)</span></span><br><span class="line">pd.DataFrame(nan_rule3.fit_transform(dataset2),columns=colsets)    <span class="comment">#应用规则</span></span><br></pre></td></tr></table></figure>

<h2 id="3-数据倾斜问题"><a href="#3-数据倾斜问题" class="headerlink" title="3. 数据倾斜问题"></a>3. 数据倾斜问题</h2><p>数据倾斜是指字段的取值分布主要集中在某个特定类别或者特定区间。</p>
<h3 id="3-1-数据倾斜问题的原因"><a href="#3-1-数据倾斜问题的原因" class="headerlink" title="3.1 数据倾斜问题的原因"></a>3.1 数据倾斜问题的原因</h3><p>出现这一问题的原因主要有三种：<br>① 上游源数据存在问题；<br>② 数据准备脚本的问题；<br>③ 数据本身的分布就是如此。<br>如果某个字段出现数据倾斜问题，必须首先排查上述第一、二种原因，如果都没有问题或者无法检查（如：单独的数据集），那么就要考虑这个字段对后续的分析建模是否有价值。一般来说，有严重的数据倾斜的字段对目标变量的区分能力很弱，对分析建模的价值不大，应该直接剔除掉。</p>
<h3 id="3-2-如何衡量数据的倾斜程度"><a href="#3-2-如何衡量数据的倾斜程度" class="headerlink" title="3.2 如何衡量数据的倾斜程度"></a>3.2 如何衡量数据的倾斜程度</h3><p>衡量数据的倾斜程度，主要采用频数分析方法，但因数据类别的不同而有所差异：<br>① 针对连续型字段，需要首先采用等宽分箱方式进行离散化，然后计算各分箱的记录数分布；<br>② 针对离散型字段，直接计算各类别的记录数分布。<br>一般来说，如果某个字段90%以上的记录数，主要集中在某个特定类别或者特定区间，那么这个字段就存在严重的数据倾斜问题。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#对于连续型变量进行等宽分箱</span></span><br><span class="line">pd.value_counts(pd.cut(dataset[<span class="string">'col3'</span>],<span class="number">5</span>))  <span class="comment">#分成5箱</span></span><br><span class="line"><span class="comment">#对于离散型变量进行频数统计</span></span><br><span class="line">pd.value_counts(dataset[<span class="string">'col4'</span>])</span><br></pre></td></tr></table></figure>

<h2 id="4-异常值检查"><a href="#4-异常值检查" class="headerlink" title="4. 异常值检查"></a>4. 异常值检查</h2><p>异常值是指数据中出现了处于特定分布、范围或者趋势之外的数据，这些数据一般会被成为异常值、离群点、噪音等。</p>
<h3 id="4-1-异常值产生的原因"><a href="#4-1-异常值产生的原因" class="headerlink" title="4.1 异常值产生的原因"></a>4.1 异常值产生的原因</h3><p>异常值的产生主要有两类原因：<br>① 数据采集、生成或者传递过程中发生的错误；<br>② 业务运营过程出现的一些特殊情况。<br>将第一种原因产生的异常值称为统计上的异常，这是错误带来的数据问题，需要解决；将第二种原因产生的异常值称为业务上的异常，反映了业务运营过程的某种特殊结果，它不是错误，但需要深究，在数据挖掘中的一种典型应用就是异常检测模型，比如信用卡欺诈，网络入侵检测、客户异动行为识别等等。</p>
<h3 id="4-2-异常值的识别方法"><a href="#4-2-异常值的识别方法" class="headerlink" title="4.2 异常值的识别方法"></a>4.2 异常值的识别方法</h3><p>异常值的识别方法主要有以下几种：</p>
<h4 id="（1）极值检查"><a href="#（1）极值检查" class="headerlink" title="（1）极值检查"></a>（1）极值检查</h4><p>主要检查字段的取值是否超出了合理的值域范围。<br>① 方法一：最大值最小值<br>使用最大值、最小值进行判断。比如客户年龄的最大值为199岁，客户账单的最小费用为-20，这些都明显存在异常。<br>② 方法二：3σ原则<br>如果数据服从正态分布，在3σ原则下，异常值被定义为与平均值的偏差超过了3倍标准差的值。这是因为，在正态分布的假设下，具体平均值3倍标准差之外的值出现的概率低于0.003，属于极个别的小概率事件。<br>③ 方法三：箱线图分析<br>箱线图提供了识别异常的标准：异常值被定义为小于下四分位-1.5倍的四分位间距，或者大于上四分位+1.5倍的四分位间距的值。<br>箱线图分析不要求数据服从任何分布，因此对异常值的识别比较客观。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#计算相关统计指标</span></span><br><span class="line">statDF = dataset2.describe()  <span class="comment">#获取描述性统计量</span></span><br><span class="line">statDF.loc[<span class="string">'mean+3std'</span>] = statDF.loc[<span class="string">'mean'</span>] + <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值+3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'mean-3std'</span>] = statDF.loc[<span class="string">'mean'</span>] - <span class="number">3</span> * statDF.loc[<span class="string">'std'</span>]  <span class="comment">#计算平均值-3倍标准差</span></span><br><span class="line">statDF.loc[<span class="string">'75%+1.5dist'</span>] = statDF.loc[<span class="string">'75%'</span>] + <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算上四分位+1.5倍的四分位间距</span></span><br><span class="line">statDF.loc[<span class="string">'25%-1.5dist'</span>] = statDF.loc[<span class="string">'25%'</span>] - <span class="number">1.5</span> * (statDF.loc[<span class="string">'75%'</span>] - statDF.loc[<span class="string">'25%'</span>])  <span class="comment">#计算下四分位-1.5倍的四分位间距</span></span><br><span class="line"><span class="comment">#获取各字段最大值、最小值</span></span><br><span class="line">statDF.loc[[<span class="string">'max'</span>,<span class="string">'min'</span>]]</span><br><span class="line"><span class="comment">#判断取值是否大于平均值+3倍标准差</span></span><br><span class="line">dataset3 = dataset2 - statDF.loc[<span class="string">'mean+3std'</span>]</span><br><span class="line">dataset3[dataset3&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于平均值-3倍标准差</span></span><br><span class="line">dataset4 = dataset2 - statDF.loc[<span class="string">'mean-3std'</span>]</span><br><span class="line">dataset4[dataset4&lt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否大于上四分位+1.5倍的四分位间距</span></span><br><span class="line">dataset5 = dataset2 - statDF.loc[<span class="string">'75%+1.5dist'</span>]</span><br><span class="line">dataset5[dataset5&gt;<span class="number">0</span>]</span><br><span class="line"><span class="comment">#判断取值是否小于下四分位-1.5倍的四分位间距</span></span><br><span class="line">dataset6 = dataset2 - statDF.loc[<span class="string">'25%-1.5dist'</span>]</span><br><span class="line">dataset6[dataset6&lt;<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<h4 id="（2）记录数分布检查"><a href="#（2）记录数分布检查" class="headerlink" title="（2）记录数分布检查"></a>（2）记录数分布检查</h4><p>主要检查字段的记录数分布是否超出合理的分布范围，包括三个指标：零值记录数、正值记录数、负值记录数。</p>
<h4 id="（3）波动检查"><a href="#（3）波动检查" class="headerlink" title="（3）波动检查"></a>（3）波动检查</h4><p>波动检查主要适用于有监督的数据，用于检查随着自变量的变化，因变量是否发生明显的波动情况。<br>以上异常值的识别方法主要针对连续型的字段，而对于离散型的字段的异常识别主要通过检查类别出现是否出现了合理阈值外的数据，比如苹果终端型号字段，出现了“P20”的取值。</p>
<h3 id="4-3-异常值的处理"><a href="#4-3-异常值的处理" class="headerlink" title="4.3 异常值的处理"></a>4.3 异常值的处理</h3><p>对于统计上的异常值的处理，主要采取两种方式：剔除或者替换。剔除是指直接将被标记为异常值的记录从数据集中删除掉，而替换是指将异常值用一个非异常值进行替换，比如边界值，或者有监督情况下的目标变量表征相似的某个值。<br>对于业务上的异常值的处理，原则就是进行深入探索分析，查找出现这一特殊情况的根本原因。</p>
]]></content>
      <categories>
        <category>数据质量</category>
      </categories>
      <tags>
        <tag>质量检查</tag>
        <tag>数仓</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title>Sqoop从mysql导入数据到hive</title>
    <url>/2020/07/22/sqoop%E4%BB%8Emysql%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0hive/</url>
    <content><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。Sqoop专为大数据批量传输设计，能够分割数据集并创建Hadoop任务来处理每个区块。</p>
<pre><code>把MySQL、Oracle等数据库中的数据导入到HDFS、Hive、HBase中。
把HDFS、Hive、HBase中的数据导出到MySQL、Oracle等数据库中。
1.4 为sqoop1, 1.9 为sqoop2 ，sqoop1与sqoop2是不兼容的。</code></pre><a id="more"></a>

<h3 id="实现需要"><a href="#实现需要" class="headerlink" title="实现需要"></a>实现需要</h3><p>数据库:</p>
<ul>
<li>driver</li>
<li>URL、username、password</li>
<li>database、table</li>
</ul>
<p>hadoop:</p>
<ul>
<li>type (hdfp、hive、hbase)</li>
<li>path 存储到哪里？</li>
<li>数据分隔符</li>
<li>mappers 数量，也就是使用多少线程。</li>
</ul>
<h2 id="二、命令"><a href="#二、命令" class="headerlink" title="二、命令"></a>二、命令</h2><h3 id="查看-sqoop-支持的命令"><a href="#查看-sqoop-支持的命令" class="headerlink" title="查看 sqoop 支持的命令"></a>查看 sqoop 支持的命令</h3><p>  sqoop help</p>
<h3 id="显示所有库名"><a href="#显示所有库名" class="headerlink" title="显示所有库名"></a>显示所有库名</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306 \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="显示某个数据库里所有表"><a href="#显示某个数据库里所有表" class="headerlink" title="显示某个数据库里所有表"></a>显示某个数据库里所有表</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234</span></span><br></pre></td></tr></table></figure>

<h3 id="MYSQL-导入数据到-HIVE"><a href="#MYSQL-导入数据到-HIVE" class="headerlink" title="MYSQL 导入数据到 HIVE"></a>MYSQL 导入数据到 HIVE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">sqoop import</span><br><span class="line"><span class="comment">--connect jdbc:mysql://192.168.66.4:3306/networkmanagement \</span></span><br><span class="line"><span class="comment">--username sendi \</span></span><br><span class="line"><span class="comment">--password 1234 \</span></span><br><span class="line"><span class="comment">--table people</span></span><br><span class="line"><span class="comment">--hive-import </span></span><br><span class="line"><span class="comment">--create-hive-table </span></span><br><span class="line"><span class="comment">--fields-terminated-by "\t"</span></span><br><span class="line">-m 5</span><br></pre></td></tr></table></figure>

<h3 id="hive-参数"><a href="#hive-参数" class="headerlink" title="hive 参数"></a>hive 参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–hive-import 必须参数，指定导入hive</span><br><span class="line">–hive-database default hive库名</span><br><span class="line">–hive-table people hive表名</span><br><span class="line">–fields-terminated-by hive的分隔符</span><br><span class="line">–hive-overwrite 重写重复字段</span><br><span class="line">–create-hive-table 帮创建好 hive 表，但是表存在会出错。不建议使用这个参数，因为到导入的时候，会与我们的字段类型有出入。</span><br><span class="line">–hive-partition-key “dt” 指定分区表的字段</span><br><span class="line">–hive-partition-value “2018-08-08” 指定分区表的值</span><br></pre></td></tr></table></figure>

<h3 id="导出没有主键的表"><a href="#导出没有主键的表" class="headerlink" title="导出没有主键的表"></a>导出没有主键的表</h3><p>可以使用两种方式：</p>
<ul>
<li>–split-by 指定切分的字段</li>
<li>-m 1 : 设置只使用一个map进行数据迁移</li>
</ul>
<h3 id="过滤条件"><a href="#过滤条件" class="headerlink" title="过滤条件"></a>过滤条件</h3><p>–where “age&gt;18” 匹配条件<br>       –columns “name,age” 选择要导入的指定列<br>       –query ‘select * from people where age&gt;18 and $CONDITIONS’: sql语句查询的结果集<br>      不能 –table 一起使用<br>      需要指定 –target-dir 路径 </p>
<h3 id="当数据库中字符为空时的处理"><a href="#当数据库中字符为空时的处理" class="headerlink" title="当数据库中字符为空时的处理"></a>当数据库中字符为空时的处理</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–null-non-string ‘0’ 当不是字符串的数据为空的时候，用 0 替换</span><br><span class="line">–null-string ‘string’ 当字符串为空的时候，使用string 字符替换</span><br></pre></td></tr></table></figure>

<h3 id="提高传输速度"><a href="#提高传输速度" class="headerlink" title="提高传输速度"></a>提高传输速度</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">–direct 提高数据库到hadoop的传输速度</span><br></pre></td></tr></table></figure>

<p>支持的数据库类型与版本：</p>
<ul>
<li>myslq 5.0 以上</li>
<li>oracle 10.2.0 以上</li>
</ul>
<h3 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h3><p>增量导入对应，首先需要知监控那一列，这列要从哪个值开始增量</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">* check-column id 用来指定一些列</span><br><span class="line">* 这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的,常用的是指定主键id.</span><br><span class="line">* –check-column 可以去指定多个列</span><br></pre></td></tr></table></figure>

<ul>
<li>last-value 10 从哪个值开始增量</li>
<li>incremental 增量的模式<ul>
<li>append id 是获取大于某一列的某个值。</li>
<li>lastmodified “2016-12-15 15:47:30” 获取某个时间后修改的所有数据<ul>
<li>–append 附加模式</li>
<li>–merge-key id 合并模式</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>注意：增量导入不能与 –delete-target-dir 一起使用，还有必须指定增量的模式</p>
]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Sqoop</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Python时间操作</title>
    <url>/2020/07/22/python%E6%97%B6%E9%97%B4/</url>
    <content><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">print</span> time.time()	<span class="comment">#输出的结果是:1279578704.6725271</span></span><br></pre></td></tr></table></figure>

<a id="more"></a>

<p>但是这样是一连串的数字不是我们想要的结果，我们可以利用time模块的格式化时间的方法来处理：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.localtime(time.time())</span><br></pre></td></tr></table></figure>

<p>用time.localtime()方法，作用是格式化时间戳为本地的时间。<br>输出的结果是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time.struct_time(tm_year=<span class="number">2010</span>, tm_mon=<span class="number">7</span>, tm_mday=<span class="number">19</span>, tm_hour=<span class="number">22</span>, tm_min=<span class="number">33</span>, tm_sec=<span class="number">39</span>, tm_wday=<span class="number">0</span>, tm_yday=<span class="number">200</span>, tm_isdst=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>现在看起来更有希望格式成我们想要的时间了。<br>time.strftime(‘%Y-%m-%d’,time.localtime(time.time()))</p>
<p>最后用time.strftime()方法，把刚才的一大串信息格式化成我们想要的东西，现在的结果是：<br>2010-07-19</p>
<p>time.strftime里面有很多参数，可以让你能够更随意的输出自己想要的东西：<br>下面是time.strftime的参数：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">strftime(format[, tuple]) -&gt; string</span><br></pre></td></tr></table></figure>

<p>将指定的struct_time(默认为当前时间)，根据指定的格式化字符串输出<br>python中时间日期格式化符号：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%y 两位数的年份表示（<span class="number">00</span><span class="number">-99</span>）</span><br><span class="line">%Y 四位数的年份表示（<span class="number">000</span><span class="number">-9999</span>）</span><br><span class="line">%m 月份（<span class="number">01</span><span class="number">-12</span>）</span><br><span class="line">%d 月内中的一天（<span class="number">0</span><span class="number">-31</span>）</span><br><span class="line">%H <span class="number">24</span>小时制小时数（<span class="number">0</span><span class="number">-23</span>）</span><br><span class="line">%I <span class="number">12</span>小时制小时数（<span class="number">01</span><span class="number">-12</span>） </span><br><span class="line">%M 分钟数（<span class="number">00</span>=<span class="number">59</span>）</span><br><span class="line">%S 秒（<span class="number">00</span><span class="number">-59</span>）</span><br><span class="line">%a 本地简化星期名称</span><br><span class="line">%A 本地完整星期名称</span><br><span class="line">%b 本地简化的月份名称</span><br><span class="line">%B 本地完整的月份名称</span><br><span class="line">%c 本地相应的日期表示和时间表示</span><br><span class="line">%j 年内的一天（<span class="number">001</span><span class="number">-366</span>）</span><br><span class="line">%p 本地A.M.或P.M.的等价符</span><br><span class="line">%U 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期天为星期的开始</span><br><span class="line">%w 星期（<span class="number">0</span><span class="number">-6</span>），星期天为星期的开始</span><br><span class="line">%W 一年中的星期数（<span class="number">00</span><span class="number">-53</span>）星期一为星期的开始</span><br><span class="line">%x 本地相应的日期表示</span><br><span class="line">%X 本地相应的时间表示</span><br><span class="line">%Z 当前时区的名称</span><br><span class="line">%% %号本身</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive中Parquet格式的使用</title>
    <url>/2020/07/22/Hive%E4%B8%ADParquet%E6%A0%BC%E5%BC%8F%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Hive中Parquet格式的使用"><a href="#Hive中Parquet格式的使用" class="headerlink" title="Hive中Parquet格式的使用"></a>Hive中Parquet格式的使用</h1><p><strong>#Hive建外部External表（外部表external table）：</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> <span class="string">`table_name`</span>(</span><br><span class="line">  <span class="string">`column1`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column2`</span> <span class="keyword">string</span>,</span><br><span class="line">  <span class="string">`column3`</span> <span class="keyword">string</span>)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (</span><br><span class="line">  <span class="string">`proc_date`</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> SERDE</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span></span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> INPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span></span><br><span class="line">OUTPUTFORMAT</span><br><span class="line">  <span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span></span><br><span class="line">LOCATION</span><br><span class="line">  <span class="string">'hdfs://hdfscluster/...'</span></span><br><span class="line">TBLPROPERTIES ( <span class="string">'orc.compress'</span>=<span class="string">'snappy'</span>);</span><br></pre></td></tr></table></figure>
<a id="more"></a>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">#添加分区并加载分区数据：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'...'</span>（不改变源数据存储位置）</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">add</span> <span class="keyword">if</span> <span class="keyword">not</span> exsit <span class="keyword">partition</span> (proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>) location <span class="string">'hdfs://hdfscluster/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'...'</span> <span class="keyword">into</span> <span class="keyword">table</span> table_name <span class="keyword">partition</span>(proc_date=<span class="string">'$&#123;hivevar:pdate&#125;'</span>);（会将源数据切到hive表指定的路径下）</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除分区：alter table table_name drop if exists partition(proc_date='$&#123;hivevar:pdate&#125;');</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>Parquet列式存储格式</title>
    <url>/2020/07/22/parquet%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F/</url>
    <content><![CDATA[<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目，最新的版本是1.8.0。</p>
<a id="more"></a>

<h2 id="列式存储"><a href="#列式存储" class="headerlink" title="列式存储"></a>列式存储</h2><p>列式存储和行式存储相比有哪些优势呢？</p>
<ol>
<li>可以跳过不符合条件的数据，只读取需要的数据，降低IO数据量。</li>
<li>压缩编码可以降低磁盘存储空间。由于同一列的数据类型是一样的，可以使用更高效的压缩编码（例如Run Length Encoding和Delta Encoding）进一步节约存储空间。</li>
<li>只读取需要的列，支持向量运算，能够获取更好的扫描性能。</li>
</ol>
]]></content>
      <categories>
        <category>Parquet</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>Parquet</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑主键,业务主键和复合主键</title>
    <url>/2020/07/21/%E9%80%BB%E8%BE%91%E4%B8%BB%E9%94%AE,%E4%B8%9A%E5%8A%A1%E4%B8%BB%E9%94%AE%E5%92%8C%E5%A4%8D%E5%90%88%E4%B8%BB%E9%94%AE/</url>
    <content><![CDATA[<p><strong>1.概念定义</strong></p>
<ul>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms191236(v=SQL.100).aspx" target="_blank" rel="noopener">主键(PRIMARY KEY)</a>：表通常具有包含唯一标识表中每一行的值的一列或一组列。这样的一列或多列称为表的主键 (PK)，用于强制表的实体完整性。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms175464(v=SQL.100).aspx" target="_blank" rel="noopener">外键(FOREIGN KEY)</a>：外键 (FK) 是用于建立和加强两个表数据之间的链接的一列或多列。在外键引用中，当一个表的列被引用作为另一个表的主键值的列时，就在两表之间创建了链接。这个列就成为第二个表的外键。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms190639(v=SQL.100).aspx" target="_blank" rel="noopener">聚集索引</a>：聚集索引基于数据行的键值在表内排序和存储这些数据行。每个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179325(v=SQL.100).aspx" target="_blank" rel="noopener">非聚集索引</a>：非聚集索引包含索引键值和指向表数据存储位置的行定位器。可以对表或索引视图创建多个非聚集索引。通常，设计非聚集索引是为改善经常使用的、没有建立聚集索引的查询的性能。</p>
</li>
<li><p><a href="http://msdn.microsoft.com/zh-cn/library/ms179413(v=SQL.100).aspx" target="_blank" rel="noopener">自动编号列和标识符列</a>：对于每个表，均可创建一个包含系统生成的序号值的标识符列，该序号值以唯一方式标识表中的每一行。</p>
</li>
<li><p>业务主键（自然主键）：在数据库表中把具有业务逻辑含义的字段作为主键，称为“自然主键(Natural Key)”。</p>
</li>
<li><p>逻辑主键（代理主键）：在数据库表中采用一个与当前表中逻辑信息无关的字段作为其主键，称为“代理主键”。</p>
</li>
<li><p>复合主键（联合主键）：通过两个或者多个字段的组合作为主键。</p>
<p><strong>2.原理分析</strong></p>
<p>​    使用逻辑主键的主要原因是，业务主键一旦改变则系统中关联该主键的部分的修改将会是不可避免的，并且引用越多改动越大。而使用逻辑主键则只需要修改相应的业务主键相关的业务逻辑即可，减少了因为业务主键相关改变对系统的影响范围。业务逻辑的改变是不可避免的，因为“永远不变的是变化”，没有任何一个公司是一成不变的，没有任何一个业务是永远不变的。最典型的例子就是身份证升位和驾驶执照号换用身份证号的业务变更。而且现实中也确实出现了<a href="http://zhidao.baidu.com/question/22152449" target="_blank" rel="noopener">身份证号码重复</a>的情况，这样如果用身份证号码作为主键也带来了难以处理的情况。当然应对改变，可以有很多解决方案，方案之一是做一新系统与时俱进，这对软件公司来说确实是件好事。</p>
<p>​    使用逻辑主键的另外一个原因是，业务主键过大，不利于传输、处理和存储。我认为一般如果业务主键超过8字节就应该考虑使用逻辑主键了，因为int是4字节的，bigint是8字节的，而业务主键一般是字符串，同样是 8 字节的 bigint 和 8 字节的字符串在传输和处理上自然是 bigint 效率更高一些。想象一下 code == “12345678” 和 id == 12345678 的汇编码的不同就知道了。当然逻辑主键不一定是 int 或者 bigint  ，而业务主键也不一定是字符串也可以是 int 或 datetime  等类型，同时传输的也不一定就是主键，这个就要具体分析了，但是原理类似，这里只是讨论通常情况。同时如果其他表需要引用该主键的话，也需要存储该主键，那么这个存储空间的开销也是不一样的。而且这些表的这个引用字段通常就是外键，或者通常也会建索引方便查找，这样也会造成存储空间的开销的不同，这也是需要具体分析的。</p>
<p>​    使用逻辑主键的再一个原因是，使用 int 或者 bigint 作为外键进行联接查询，性能会比以字符串作为外键进行联接查询快。原理和上面的类似，这里不再重复。</p>
<p>​    使用逻辑主键的再一个原因是，存在用户或维护人员误录入数据到业务主键中的问题。例如错把 RMB 录入为 RXB  ，相关的引用都是引用了错误的数据，一旦需要修改则非常麻烦。如果使用逻辑主键则问题很好解决，如果使用业务主键则会影响到其他表的外键数据，当然也可以通过级联更新方式解决，但是不是所有都能级联得了的。</p>
<p>​    使用业务主键的主要原因是，增加逻辑主键就是增加了一个业务无关的字段，而用户通常都是对于业务相关的字段进行查找（比如员工的工号，书本的 ISBN No.  ），这样我们除了为逻辑主键加索引，还必须为这些业务字段加索引，这样数据库的性能就会下降，而且也增加了存储空间的开销。所以对于业务上确实不常改变的基础数据而言，使用业务主键不失是一个比较好的选择。另一方面，对于基础数据而言，一般的增、删、改都比较少，所以这部分的开销也不会太多，而如果这时候对于业务逻辑的改变有担忧的话，也是可以考虑使用逻辑主键的，这就需要具体问题具体分析了。</p>
<p>​    使用业务主键的另外一个原因是，对于用户操作而言，都是通过业务字段进行的，所以在这些情况下，如果使用逻辑主键的话，必须要多做一次映射转换的动作。我认为这种担心是多余的，直接使用业务主键查询就能得到结果，根本不用管逻辑主键，除非业务主键本身就不唯一。另外，如果在设计的时候就考虑使用逻辑主键的话，编码的时候也是会以主键为主进行处理的，在系统内部传输、处理和存储都是相同的主键，不存在转换问题。除非现有系统是使用业务主键，要把现有系统改成使用逻辑主键，这种情况才会存在转换问题。暂时没有想到还有什么场景是存在这样的转换的。</p>
<p>​    使用业务主键的再一个原因是，对于银行系统而言安全性比性能更加重要，这时候就会考虑使用业务主键，既可以作为主键也可以作为冗余数据，避免因为使用逻辑主键带来的关联丢失问题。如果由于某种原因导致主表和子表关联关系丢失的话，银行可是会面临无法挽回的损失的。为了杜绝这种情况的发生，业务主键需要在重要的表中有冗余存在，这种情况最好的处理方式就是直接使用业务主键了。例如身份证号、存折号、卡号等。所以通常银行系统都要求使用业务主键，这个需求并不是出于性能的考虑而是出于安全性的考虑。</p>
<p>​    使用复合主键的主要原因和使用业务主键是相关的，通常业务主键只使用一个字段不能解决问题，那就只能使用多个字段了。例如使用姓名字段不够用了，再加个生日字段。这种使用复合主键方式效率非常低，主要原因和上面对于较大的业务主键的情况类似。另外如果其他表要与该表关联则需要引用复合主键的所有字段，这就不单纯是性能问题了，还有存储空间的问题了，当然你也可以认为这是合理的数据冗余，方便查询，但是感觉有点得不偿失。</p>
<p>​    使用复合主键的另外一个原因是，对于关系表来说必须关联两个实体表的主键，才能表示它们之间的关系，那么可以把这两个主键联合组成复合主键即可。如果两个实体存在多个关系，可以再加一个顺序字段联合组成复合主键，但是这样就会引入业务主键的弊端。当然也可以另外对这个关系表添加一个逻辑主键，避免了业务主键的弊端，同时也方便其他表对它的引用。</p>
<p>综合来说，网上大多数人是倾向于用逻辑主键的，而对于实体表用复合主键方式的应该没有多少人认同。支持业务主键的人通常有种误解，认为逻辑主键必须对用户来说有意义，其实逻辑主键只是系统内部使用的，对用户来说是无需知道的。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>数仓</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>主键</tag>
      </tags>
  </entry>
</search>
